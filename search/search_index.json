{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ml-networks","text":"<p>\u6751\u7530\u7814\u5171\u901a\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u5316\u8a08\u753b\u306e\u4e00\u74b0\u3068\u3057\u3066\u3001\u57fa\u672c\u7684\u306a\u6df1\u5c64\u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u63d0\u4f9b\u3059\u308bPython\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u3059\u3002</p>"},{"location":"#_1","title":"\u6982\u8981","text":"<p><code>ml-networks</code>\u306f\u3001PyTorch\u304a\u3088\u3073JAX\uff08Flax NNX\uff09\u30d9\u30fc\u30b9\u306e\u6df1\u5c64\u5b66\u7fd2\u30e2\u30c7\u30eb\u69cb\u7bc9\u3092\u652f\u63f4\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002\u4ee5\u4e0b\u306e\u6a5f\u80fd\u3092\u63d0\u4f9b\u3057\u307e\u3059\uff1a</p> <ul> <li>\u57fa\u672c\u7684\u306a\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3: MLP\u3001Encoder\u3001Decoder\u3001UNet\u3001Vision Transformer\uff08ViT\uff09\u306a\u3069</li> <li>\u5206\u5e03\u306e\u30b5\u30dd\u30fc\u30c8: \u6b63\u898f\u5206\u5e03\u3001\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5206\u5e03\u3001\u30d9\u30eb\u30cc\u30fc\u30a4\u5206\u5e03\u3001BSQ\u30b3\u30fc\u30c9\u30d6\u30c3\u30af</li> <li>\u640d\u5931\u95a2\u6570: Focal Loss\u3001Charbonnier Loss\u3001Focal Frequency Loss\u3001KL\u30c0\u30a4\u30d0\u30fc\u30b8\u30a7\u30f3\u30b9\u306a\u3069</li> <li>\u4fbf\u5229\u306a\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3: \u6d3b\u6027\u5316\u95a2\u6570\u3001\u6700\u9069\u5316\u624b\u6cd5\u3001\u30c7\u30fc\u30bf\u4fdd\u5b58\u30fb\u8aad\u307f\u8fbc\u307f\u6a5f\u80fd</li> <li>\u9ad8\u5ea6\u306a\u6a5f\u80fd: HyperNetwork\u3001\u5bfe\u7167\u5b66\u7fd2\uff08Contrastive Learning\uff09\u3001\u6761\u4ef6\u4ed8\u304dUNet</li> </ul>"},{"location":"#_2","title":"\u7279\u5fb4","text":"<ul> <li>\u30de\u30eb\u30c1\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u5bfe\u5fdc: PyTorch\u3068JAX\uff08Flax NNX\uff09\u306e\u4e21\u65b9\u3092\u30b5\u30dd\u30fc\u30c8\u3002\u540c\u4e00\u306eConfig\u4f53\u7cfb\u3067\u5207\u308a\u66ff\u3048\u53ef\u80fd</li> <li>\u4f7f\u3044\u3084\u3059\u3044: \u76f4\u611f\u7684\u306aAPI\u8a2d\u8a08\u3002YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u307f\u3001<code>hydra.utils.instantiate</code>\u3067\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316</li> <li>\u67d4\u8edf\u6027: \u8c4a\u5bcc\u306a\u8a2d\u5b9a\u30aa\u30d7\u30b7\u30e7\u30f3\u3002\u30d0\u30c3\u30af\u30dc\u30fc\u30f3\u3001\u6b63\u898f\u5316\u3001\u6d3b\u6027\u5316\u95a2\u6570\u3092\u81ea\u7531\u306b\u7d44\u307f\u5408\u308f\u305b</li> <li>\u5305\u62ec\u7684: \u6df1\u5c64\u5b66\u7fd2\u306b\u5fc5\u8981\u306a\u4e3b\u8981\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3092\u7db2\u7f85</li> <li>\u5b9f\u7528\u7684: \u578b\u30c1\u30a7\u30c3\u30af\uff08mypy\uff09\u5bfe\u5fdc\u3001CI/CD\u6574\u5099\u6e08\u307f</li> </ul>"},{"location":"#_3","title":"\u30d1\u30c3\u30b1\u30fc\u30b8\u69cb\u6210","text":"<pre><code>ml_networks/\n\u251c\u2500\u2500 config.py          # \u5171\u901a\u8a2d\u5b9a\u30af\u30e9\u30b9\uff08PyTorch/JAX\u5171\u901a\uff09\n\u251c\u2500\u2500 utils.py           # \u5171\u901a\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\n\u251c\u2500\u2500 callbacks.py       # PyTorch Lightning \u30b3\u30fc\u30eb\u30d0\u30c3\u30af\n\u251c\u2500\u2500 torch/             # PyTorch\u5b9f\u88c5\n\u2502   \u251c\u2500\u2500 layers.py      # MLP, Conv, Attention, Transformer\u306a\u3069\n\u2502   \u251c\u2500\u2500 vision.py      # Encoder, Decoder, ConvNet, ResNet, ViT\n\u2502   \u251c\u2500\u2500 unet.py        # ConditionalUnet1d, ConditionalUnet2d\n\u2502   \u251c\u2500\u2500 distributions.py  # \u78ba\u7387\u5206\u5e03\n\u2502   \u251c\u2500\u2500 loss.py        # \u640d\u5931\u95a2\u6570\n\u2502   \u251c\u2500\u2500 activations.py # \u30ab\u30b9\u30bf\u30e0\u6d3b\u6027\u5316\u95a2\u6570\n\u2502   \u251c\u2500\u2500 hypernetworks.py  # HyperNetwork\n\u2502   \u251c\u2500\u2500 contrastive.py # \u5bfe\u7167\u5b66\u7fd2\n\u2502   \u2514\u2500\u2500 torch_utils.py # PyTorch\u56fa\u6709\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\n\u2514\u2500\u2500 jax/               # JAX (Flax NNX) \u5b9f\u88c5\n    \u251c\u2500\u2500 layers.py      # MLP, Conv, Attention, Transformer\u306a\u3069\n    \u251c\u2500\u2500 vision.py      # Encoder, Decoder, ConvNet, ResNet, ViT\n    \u251c\u2500\u2500 unet.py        # ConditionalUnet1d, ConditionalUnet2d\n    \u251c\u2500\u2500 distributions.py  # \u78ba\u7387\u5206\u5e03\n    \u251c\u2500\u2500 loss.py        # \u640d\u5931\u95a2\u6570\n    \u251c\u2500\u2500 activations.py # \u30ab\u30b9\u30bf\u30e0\u6d3b\u6027\u5316\u95a2\u6570\n    \u251c\u2500\u2500 hypernetworks.py  # HyperNetwork\n    \u251c\u2500\u2500 contrastive.py # \u5bfe\u7167\u5b66\u7fd2\n    \u2514\u2500\u2500 jax_utils.py   # JAX\u56fa\u6709\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\n</code></pre>"},{"location":"#_4","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":""},{"location":"#_5","title":"\u8981\u4ef6","text":"<ul> <li>Python &gt;= 3.10</li> <li>PyTorch &gt;= 2.0\uff08PyTorch\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\uff09</li> <li>JAX &gt;= 0.4.30 + Flax &gt;= 0.12.0\uff08JAX\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\uff09</li> </ul>"},{"location":"#_6","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u65b9\u6cd5","text":"<p>\u4ee5\u4e0b\u306e\u3044\u305a\u308c\u304b\u306e\u65b9\u6cd5\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3067\u304d\u307e\u3059\uff1a</p>"},{"location":"#pip","title":"pip\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408","text":"<pre><code>pip install https://github.com/keio-crl/ml-networks.git\n</code></pre>"},{"location":"#rye","title":"rye\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408","text":"<pre><code>rye add ml-networks --git https://github.com/keio-crl/ml-networks.git\n</code></pre>"},{"location":"#uv","title":"uv\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408","text":"<pre><code>uv add https://github.com/keio-crl/ml-networks.git\n</code></pre>"},{"location":"#jax","title":"JAX\u30b5\u30dd\u30fc\u30c8\u3092\u8ffd\u52a0\u3059\u308b\u5834\u5408","text":"<pre><code>pip install \"ml-networks[jax] @ https://github.com/keio-crl/ml-networks.git\"\n</code></pre> <p>\u6ce8\u610f: uv\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\u306f\u3001<code>&lt;access token&gt;</code>\u3092GitHub\u306ePersonal Access Token\u306b\u7f6e\u304d\u63db\u3048\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"#_7","title":"\u30af\u30a4\u30c3\u30af\u30b9\u30bf\u30fc\u30c8","text":"PyTorchJAX (Flax NNX) <pre><code>from ml_networks.torch import MLPLayer\nfrom ml_networks import MLPConfig, LinearConfig\nimport torch\n\n# MLP\u306e\u8a2d\u5b9a\nmlp_config = MLPConfig(\n    hidden_dim=128,\n    n_layers=2,\n    output_activation=\"Tanh\",\n    linear_cfg=LinearConfig(activation=\"ReLU\", bias=True)\n)\n\n# MLP\u306e\u4f5c\u6210\u3068\u63a8\u8ad6\nmlp = MLPLayer(input_dim=16, output_dim=8, mlp_config=mlp_config)\nx = torch.randn(32, 16)\ny = mlp(x)\nprint(y.shape)  # torch.Size([32, 8])\n</code></pre> <pre><code>from ml_networks.jax import MLPLayer\nfrom ml_networks import MLPConfig, LinearConfig\nimport jax\nimport jax.numpy as jnp\n\n# MLP\u306e\u8a2d\u5b9a\uff08PyTorch\u3068\u540c\u3058Config\u3092\u4f7f\u7528\uff09\nmlp_config = MLPConfig(\n    hidden_dim=128,\n    n_layers=2,\n    output_activation=\"Tanh\",\n    linear_cfg=LinearConfig(activation=\"ReLU\", bias=True)\n)\n\n# MLP\u306e\u4f5c\u6210\u3068\u63a8\u8ad6\nmlp = MLPLayer(input_dim=16, output_dim=8, mlp_config=mlp_config, rngs=jax.random.PRNGKey(0))\nx = jnp.ones((32, 16))\ny = mlp(x)\nprint(y.shape)  # (32, 8)\n</code></pre> <p>\u8a73\u7d30\u306f\u30af\u30a4\u30c3\u30af\u30b9\u30bf\u30fc\u30c8\u30ac\u30a4\u30c9\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"#_8","title":"\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8","text":"<ul> <li>\u30af\u30a4\u30c3\u30af\u30b9\u30bf\u30fc\u30c8 - \u57fa\u672c\u7684\u306a\u4f7f\u7528\u65b9\u6cd5</li> <li>\u8a2d\u5b9a\u7ba1\u7406\u30ac\u30a4\u30c9 - YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080\u65b9\u6cd5\uff08\u63a8\u5968\uff09</li> <li>\u30ac\u30a4\u30c9:<ul> <li>MLP - \u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3</li> <li>Encoder - \u753b\u50cf\u30a8\u30f3\u30b3\u30fc\u30c0</li> <li>Decoder - \u753b\u50cf\u30c7\u30b3\u30fc\u30c0</li> <li>UNet - \u6761\u4ef6\u4ed8\u304dUNet</li> <li>Distributions - \u78ba\u7387\u5206\u5e03</li> <li>\u640d\u5931\u95a2\u6570 - \u5404\u7a2e\u640d\u5931\u95a2\u6570</li> <li>\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u3068\u8aad\u307f\u8fbc\u307f - blosc2\u5f62\u5f0f\u306e\u30c7\u30fc\u30bfI/O</li> <li>\u305d\u306e\u4ed6\u306e\u4fbf\u5229\u306a\u6a5f\u80fd - \u6d3b\u6027\u5316\u95a2\u6570\u3001\u6700\u9069\u5316\u3001seed\u56fa\u5b9a\u306a\u3069</li> <li>\u9ad8\u5ea6\u306a\u6a5f\u80fd - HyperNetwork\u3001\u5bfe\u7167\u5b66\u7fd2\u3001Attention</li> <li>JAX\u30d0\u30c3\u30af\u30a8\u30f3\u30c9 - JAX (Flax NNX) \u3067\u306e\u4f7f\u7528\u65b9\u6cd5</li> </ul> </li> <li>API \u30ea\u30d5\u30a1\u30ec\u30f3\u30b9 - \u5b8c\u5168\u306aAPI\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8</li> </ul>"},{"location":"#_9","title":"\u30e9\u30a4\u30bb\u30f3\u30b9","text":"<p>\u3053\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u30e9\u30a4\u30bb\u30f3\u30b9\u60c5\u5831\u306b\u3064\u3044\u3066\u306f\u3001\u30ea\u30dd\u30b8\u30c8\u30ea\u306eLICENSE\u30d5\u30a1\u30a4\u30eb\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"#_10","title":"\u4f5c\u8005","text":"<ul> <li>oakwood-fujiken (oakwood.n14.4sp@keio.jp)</li> <li>nomutin (nomura0508@icloud.com)</li> </ul>"},{"location":"development/","title":"\u958b\u767a","text":"<p>\u958b\u767a\u306b\u95a2\u3059\u308b\u60c5\u5831\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"development/#_2","title":"\u958b\u767a\u74b0\u5883\u306e\u30bb\u30c3\u30c8\u30a2\u30c3\u30d7","text":"<pre><code># \u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u30af\u30ed\u30fc\u30f3\ngit clone https://github.com/keio-crl/ml-networks.git\ncd ml-networks\n\n# \u958b\u767a\u7528\u4f9d\u5b58\u95a2\u4fc2\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\npip install -e \".[dev]\"\n\n# JAX\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u306e\u958b\u767a\u3082\u884c\u3046\u5834\u5408\npip install -e \".[jax]\"\n</code></pre>"},{"location":"development/#_3","title":"\u30b3\u30fc\u30c9\u54c1\u8cea\u30c1\u30a7\u30c3\u30af","text":"<pre><code># \u30ea\u30f3\u30bf\u30fc\u306e\u5b9f\u884c\nruff check .\n\n# \u578b\u30c1\u30a7\u30c3\u30af\nmypy src/\n\n# \u30d5\u30a9\u30fc\u30de\u30c3\u30c8\nruff format .\n</code></pre>"},{"location":"development/#_4","title":"\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u30d3\u30eb\u30c9","text":"<pre><code># \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u7528\u4f9d\u5b58\u95a2\u4fc2\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\npip install -e \".[docs]\"\n\n# \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u30d3\u30eb\u30c9\nmkdocs build\n\n# \u30ed\u30fc\u30ab\u30eb\u30b5\u30fc\u30d0\u30fc\u3067\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u78ba\u8a8d\nmkdocs serve\n</code></pre>"},{"location":"development/#_5","title":"\u30d0\u30fc\u30b8\u30e7\u30f3\u7ba1\u7406","text":"<p>\u3053\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u306f\u3001\u30bb\u30de\u30f3\u30c6\u30a3\u30c3\u30af\u30d0\u30fc\u30b8\u30e7\u30cb\u30f3\u30b0\uff08Semantic Versioning\uff09\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"development/#_6","title":"\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u5f62\u5f0f","text":"<p>\u30d0\u30fc\u30b8\u30e7\u30f3\u306f <code>MAJOR.MINOR.PATCH</code> \u306e\u5f62\u5f0f\u3067\u7ba1\u7406\u3055\u308c\u3066\u3044\u307e\u3059\uff1a - MAJOR: \u4e92\u63db\u6027\u306e\u306a\u3044\u5909\u66f4\u304c\u3042\u308b\u5834\u5408 - MINOR: \u5f8c\u65b9\u4e92\u63db\u6027\u3092\u4fdd\u3063\u305f\u6a5f\u80fd\u8ffd\u52a0\u306e\u5834\u5408 - PATCH: \u5f8c\u65b9\u4e92\u63db\u6027\u3092\u4fdd\u3063\u305f\u30d0\u30b0\u4fee\u6b63\u306e\u5834\u5408</p>"},{"location":"development/#_7","title":"\u30d0\u30fc\u30b8\u30e7\u30f3\u306e\u66f4\u65b0\u65b9\u6cd5","text":""},{"location":"development/#1","title":"\u65b9\u6cd51: \u30b9\u30af\u30ea\u30d7\u30c8\u3092\u4f7f\u7528\uff08\u63a8\u5968\uff09","text":"<pre><code># \u30d1\u30c3\u30c1\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u4e0a\u3052\u308b (0.1.0 -&gt; 0.1.1)\npython scripts/bump_version.py patch\n\n# \u30de\u30a4\u30ca\u30fc\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u4e0a\u3052\u308b (0.1.0 -&gt; 0.2.0)\npython scripts/bump_version.py minor\n\n# \u30e1\u30b8\u30e3\u30fc\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u4e0a\u3052\u308b (0.1.0 -&gt; 1.0.0)\npython scripts/bump_version.py major\n</code></pre> <p>\u30b9\u30af\u30ea\u30d7\u30c8\u5b9f\u884c\u5f8c\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u30ea\u30ea\u30fc\u30b9\u3057\u307e\u3059\uff1a</p> <pre><code># \u5909\u66f4\u3092\u78ba\u8a8d\ngit diff pyproject.toml\n\n# \u30b3\u30df\u30c3\u30c8\ngit add pyproject.toml\ngit commit -m \"Bump version to X.Y.Z\"\n\n# \u30bf\u30b0\u3092\u4f5c\u6210\ngit tag -a vX.Y.Z -m \"Release vX.Y.Z\"\n\n# \u30d7\u30c3\u30b7\u30e5\ngit push origin main\ngit push origin vX.Y.Z\n</code></pre>"},{"location":"development/#2-github-actions","title":"\u65b9\u6cd52: GitHub Actions\u3092\u4f7f\u7528","text":"<ol> <li>GitHub\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u300cActions\u300d\u30bf\u30d6\u306b\u79fb\u52d5</li> <li>\u300cVersion Bump\u300d\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u9078\u629e</li> <li>\u300cRun workflow\u300d\u3092\u30af\u30ea\u30c3\u30af</li> <li>\u30d0\u30fc\u30b8\u30e7\u30f3\u30bf\u30a4\u30d7\uff08patch/minor/major\uff09\u3092\u9078\u629e</li> <li>\u30bf\u30b0\u3092\u4f5c\u6210\u3059\u308b\u304b\u3069\u3046\u304b\u3092\u9078\u629e</li> <li>\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u5b9f\u884c</li> </ol>"},{"location":"development/#_8","title":"\u30ea\u30ea\u30fc\u30b9\u30d7\u30ed\u30bb\u30b9","text":"<ol> <li>\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u66f4\u65b0: \u4e0a\u8a18\u306e\u65b9\u6cd5\u3067\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u66f4\u65b0</li> <li>\u30bf\u30b0\u3092\u4f5c\u6210: <code>vX.Y.Z</code> \u306e\u5f62\u5f0f\u3067\u30bf\u30b0\u3092\u4f5c\u6210</li> <li>\u81ea\u52d5\u30ea\u30ea\u30fc\u30b9: \u30bf\u30b0\u3092\u30d7\u30c3\u30b7\u30e5\u3059\u308b\u3068\u3001GitHub Actions\u304c\u81ea\u52d5\u7684\u306b\uff1a</li> <li>\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u30d3\u30eb\u30c9</li> <li>\u30ea\u30ea\u30fc\u30b9\u30ce\u30fc\u30c8\u3092\u751f\u6210</li> <li>GitHub Release\u3092\u4f5c\u6210</li> </ol>"},{"location":"development/#cicd","title":"CI/CD","text":"<p>\u3053\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3067\u306f\u3001\u4ee5\u4e0b\u306eGitHub Actions\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u307e\u3059\uff1a</p> <ul> <li>CI: \u30d7\u30c3\u30b7\u30e5/\u30d7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u6642\u306b\u81ea\u52d5\u5b9f\u884c</li> <li>\u30ea\u30f3\u30c8\u30c1\u30a7\u30c3\u30af\uff08ruff\uff09</li> <li>\u578b\u30c1\u30a7\u30c3\u30af\uff08mypy\uff09</li> <li>\u30c6\u30b9\u30c8\u5b9f\u884c\uff08pytest\uff09</li> <li> <p>\u30d1\u30c3\u30b1\u30fc\u30b8\u30d3\u30eb\u30c9\u78ba\u8a8d</p> </li> <li> <p>Release: \u30bf\u30b0\u304c\u30d7\u30c3\u30b7\u30e5\u3055\u308c\u305f\u3068\u304d\u306b\u81ea\u52d5\u5b9f\u884c</p> </li> <li>\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30d3\u30eb\u30c9</li> <li>GitHub Release\u306e\u4f5c\u6210</li> </ul>"},{"location":"development/#_9","title":"\u30b3\u30f3\u30c8\u30ea\u30d3\u30e5\u30fc\u30b7\u30e7\u30f3","text":"<p>\u30b3\u30f3\u30c8\u30ea\u30d3\u30e5\u30fc\u30b7\u30e7\u30f3\u3092\u6b53\u8fce\u3057\u307e\u3059\u3002\u30d7\u30eb\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u9001\u4fe1\u3059\u308b\u524d\u306b\u3001\u30b3\u30fc\u30c9\u54c1\u8cea\u30c1\u30a7\u30c3\u30af\u3092\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"getting-started/","title":"\u30af\u30a4\u30c3\u30af\u30b9\u30bf\u30fc\u30c8","text":"<p>\u3053\u306e\u30ac\u30a4\u30c9\u3067\u306f\u3001<code>ml-networks</code>\u306e\u57fa\u672c\u7684\u306a\u4f7f\u7528\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"getting-started/#_2","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<p>\u307e\u305a\u3001<code>ml-networks</code>\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\uff1a</p> <pre><code>pip install https://github.com/keio-crl/ml-networks.git\n</code></pre> <p>JAX\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3082\u4f7f\u7528\u3059\u308b\u5834\u5408\u306f\u3001\u8ffd\u52a0\u306e\u4f9d\u5b58\u95a2\u4fc2\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u307e\u3059\uff1a</p> <pre><code>pip install jax flax optax distrax\n</code></pre>"},{"location":"getting-started/#_3","title":"\u57fa\u672c\u7684\u306a\u4f7f\u7528\u4f8b","text":"<p>\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9: YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080</p> <p>\u8a2d\u5b9a\u3092Python\u30b3\u30fc\u30c9\u306b\u30d9\u30bf\u66f8\u304d\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u3001YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8aad\u307f\u8fbc\u3080\u3053\u3068\u3092\u63a8\u5968\u3057\u307e\u3059\u3002 \u8a73\u7d30\u306f\u8a2d\u5b9a\u7ba1\u7406\u30ac\u30a4\u30c9\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <p>PyTorch/JAX\u306e\u9078\u629e</p> <p><code>ml-networks</code>\u306fPyTorch\u3068JAX\u306e\u4e21\u65b9\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059\u3002 PyTorch\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\u306f<code>from ml_networks.torch import ...</code>\u3001 JAX\u3092\u4f7f\u7528\u3059\u308b\u5834\u5408\u306f<code>from ml_networks.jax import ...</code>\u3067\u30a4\u30f3\u30dd\u30fc\u30c8\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u8a2d\u5b9a\u30af\u30e9\u30b9\uff08<code>Config</code>\uff09\u306f\u5171\u901a\u3067<code>from ml_networks import ...</code>\u3067\u30a4\u30f3\u30dd\u30fc\u30c8\u3057\u307e\u3059\u3002</p>"},{"location":"getting-started/#mlp","title":"MLP\u306e\u4f7f\u7528","text":"<p>\u6700\u3082\u30b7\u30f3\u30d7\u30eb\u306a\u4f8b\u3068\u3057\u3066\u3001MLP\uff08\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\uff09\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002</p>"},{"location":"getting-started/#1-yaml","title":"\u65b9\u6cd51: YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8aad\u307f\u8fbc\u3080\uff08\u63a8\u5968\uff09","text":"<p>\u307e\u305a\u3001\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb <code>configs/mlp_config.yaml</code> \u3092\u4f5c\u6210\u3057\u307e\u3059\uff1a</p> <pre><code>_target_: ml_networks.layers.MLPLayer\ninput_dim: 16\noutput_dim: 8\nmlp_config:\n  _target_: ml_networks.config.MLPConfig\n  hidden_dim: 128\n  n_layers: 2\n  output_activation: Tanh\n  linear_cfg:\n    _target_: ml_networks.config.LinearConfig\n    activation: ReLU\n    bias: true\n    norm: none\n</code></pre> <p>Python\u30b3\u30fc\u30c9\uff1a</p> <pre><code>from hydra.utils import instantiate\nfrom omegaconf import OmegaConf\nimport torch\n\n# YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080\ncfg = OmegaConf.load(\"configs/mlp_config.yaml\")\n\n# instantiate\u3092\u4f7f\u7528\u3057\u3066\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\nmlp = instantiate(cfg)\n\n# \u63a8\u8ad6\nx = torch.randn(32, 16)\ny = mlp(x)\nprint(y.shape)  # torch.Size([32, 8])\n</code></pre>"},{"location":"getting-started/#2-python","title":"\u65b9\u6cd52: Python\u30b3\u30fc\u30c9\u3067\u76f4\u63a5\u8a2d\u5b9a\u3059\u308b","text":"<pre><code>from ml_networks import MLPLayer, MLPConfig, LinearConfig\nimport torch\n\n# MLP\u306e\u8a2d\u5b9a\nmlp_config = MLPConfig(\n    hidden_dim=128,  # \u96a0\u308c\u5c64\u306e\u6b21\u5143\n    n_layers=2,      # \u96a0\u308c\u5c64\u306e\u6570\n    output_activation=\"Tanh\",  # \u51fa\u529b\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\n    linear_cfg=LinearConfig(\n        activation=\"ReLU\",  # \u6d3b\u6027\u5316\u95a2\u6570\n        bias=True,          # \u30d0\u30a4\u30a2\u30b9\u3092\u4f7f\u3046\u304b\u3069\u3046\u304b\n        norm=\"none\",        # \u6b63\u898f\u5316\u3092\u884c\u3046\u304b\u3069\u3046\u304b\n    )\n)\n\n# MLP\u306e\u4f5c\u6210\ninput_dim = 16\noutput_dim = 8\nmlp = MLPLayer(input_dim, output_dim, mlp_config)\n\n# \u63a8\u8ad6\nx = torch.randn(32, input_dim)\ny = mlp(x)\nprint(y.shape)  # torch.Size([32, 8])\n</code></pre>"},{"location":"getting-started/#encoder","title":"Encoder\u306e\u4f7f\u7528","text":"<p>\u753b\u50cf\u3092\u7279\u5fb4\u91cf\u306b\u5909\u63db\u3059\u308bEncoder\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002</p>"},{"location":"getting-started/#1-yaml_1","title":"\u65b9\u6cd51: YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8aad\u307f\u8fbc\u3080\uff08\u63a8\u5968\uff09","text":"<p>\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb <code>configs/encoder_config.yaml</code> \u3092\u4f5c\u6210\u3057\u307e\u3059\uff1a</p> <pre><code>_target_: ml_networks.vision.Encoder\nfeature_dim: 64\nobs_shape: [3, 64, 64]\nencoder_cfg:\n  _target_: ml_networks.config.ConvNetConfig\n  channels: [16, 32, 64]\n  conv_cfgs:\n    - _target_: ml_networks.config.ConvConfig\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: ReLU\n    - _target_: ml_networks.config.ConvConfig\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: ReLU\n    - _target_: ml_networks.config.ConvConfig\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: ReLU\nfull_connection_cfg:\n  _target_: ml_networks.config.LinearConfig\n  activation: ReLU\n  bias: true\n</code></pre> <p>Python\u30b3\u30fc\u30c9\uff1a</p> <pre><code>from hydra.utils import instantiate\nfrom omegaconf import OmegaConf\nimport torch\n\n# YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080\ncfg = OmegaConf.load(\"configs/encoder_config.yaml\")\n\n# instantiate\u3092\u4f7f\u7528\u3057\u3066\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\nencoder = instantiate(cfg)\n\n# \u63a8\u8ad6\nobs = torch.randn(32, 3, 64, 64)\nz = encoder(obs)\nprint(z.shape)  # torch.Size([32, 64])\n</code></pre>"},{"location":"getting-started/#2-python_1","title":"\u65b9\u6cd52: Python\u30b3\u30fc\u30c9\u3067\u76f4\u63a5\u8a2d\u5b9a\u3059\u308b","text":"<pre><code>from ml_networks import Encoder, ConvNetConfig, ConvConfig, LinearConfig\nimport torch\n\n# Encoder\u306e\u8a2d\u5b9a\nencoder_cfg = ConvNetConfig(\n    channels=[16, 32, 64],\n    conv_cfgs=[\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n    ]\n)\n\n# \u5168\u7d50\u5408\u5c64\u306e\u8a2d\u5b9a\nfull_connection_cfg = LinearConfig(\n    activation=\"ReLU\",\n    bias=True,\n)\n\n# Encoder\u306e\u4f5c\u6210\nobs_shape = (3, 64, 64)  # (C, H, W)\nfeature_dim = 64\nencoder = Encoder(feature_dim, obs_shape, encoder_cfg, full_connection_cfg)\n\n# \u63a8\u8ad6\nobs = torch.randn(32, 3, 64, 64)\nz = encoder(obs)\nprint(z.shape)  # torch.Size([32, 64])\n</code></pre>"},{"location":"getting-started/#decoder","title":"Decoder\u306e\u4f7f\u7528","text":"<p>\u7279\u5fb4\u91cf\u304b\u3089\u753b\u50cf\u3092\u518d\u69cb\u6210\u3059\u308bDecoder\u3092\u4f7f\u7528\u3057\u307e\u3059\uff1a</p> <pre><code>from ml_networks import Decoder, ConvNetConfig, ConvConfig, LinearConfig\nimport torch\n\n# Decoder\u306e\u8a2d\u5b9a\ndecoder_cfg = ConvNetConfig(\n    channels=[64, 32, 16],\n    conv_cfgs=[\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"Tanh\"),\n    ]\n)\n\n# \u5168\u7d50\u5408\u5c64\u306e\u8a2d\u5b9a\nfull_connection_cfg = LinearConfig(\n    activation=\"ReLU\",\n    bias=True,\n)\n\n# Decoder\u306e\u4f5c\u6210\nobs_shape = (3, 64, 64)\nfeature_dim = 64\ndecoder = Decoder(feature_dim, obs_shape, decoder_cfg, full_connection_cfg)\n\n# \u63a8\u8ad6\nz = torch.randn(32, feature_dim)\npredicted_obs = decoder(z)\nprint(predicted_obs.shape)  # torch.Size([32, 3, 64, 64])\n</code></pre>"},{"location":"getting-started/#_4","title":"\u5206\u5e03\u306e\u4f7f\u7528","text":"<p>\u7279\u5fb4\u91cf\u3092\u5206\u5e03\u306b\u5909\u63db\u3057\u307e\u3059\uff1a</p> <pre><code>from ml_networks import Distribution, Encoder, ConvNetConfig, ConvConfig, MLPConfig, LinearConfig\nimport torch\n\n# Encoder\u306e\u8a2d\u5b9a\uff08\u5206\u5e03\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u51fa\u529b\u3059\u308b\u305f\u3081\u3001\u7279\u5fb4\u91cf\u6b21\u5143\u306e2\u500d\u304c\u5fc5\u8981\uff09\nencoder_cfg = ConvNetConfig(\n    channels=[16, 32, 64],\n    conv_cfgs=[\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n    ]\n)\n\nfull_connection_cfg = MLPConfig(\n    hidden_dim=128,\n    n_layers=2,\n    output_activation=\"Identity\",\n    linear_cfg=LinearConfig(activation=\"ReLU\", bias=True)\n)\n\nfeature_dim = 64\nobs_shape = (3, 64, 64)\n\n# \u6b63\u898f\u5206\u5e03\u306e\u5834\u5408\u3001\u5e73\u5747\u3068\u6a19\u6e96\u504f\u5dee\u3067\u7279\u5fb4\u91cf\u6b21\u5143\u306e2\u500d\u304c\u5fc5\u8981\nencoder = Encoder(feature_dim * 2, obs_shape, encoder_cfg, full_connection_cfg)\n\n# \u5206\u5e03\u306e\u8a2d\u5b9a\ndist = Distribution(\n    in_dim=feature_dim,  # \u5206\u5e03\u306e\u6b21\u5143\n    dist=\"normal\",       # \u5206\u5e03\u306e\u7a2e\u985e\n    n_groups=1,          # \u5206\u5e03\u306e\u30b0\u30eb\u30fc\u30d7\u6570\n)\n\n# \u63a8\u8ad6\nobs = torch.randn(32, 3, 64, 64)\nz = encoder(obs)\ndist_z = dist(z)\nprint(dist_z)  # NormalStoch(mean: torch.Size([32, 64]), std: torch.Size([32, 64]), stoch: torch.Size([32, 64]))\n</code></pre>"},{"location":"getting-started/#_5","title":"\u6b21\u306e\u30b9\u30c6\u30c3\u30d7","text":"<ul> <li>\u8a2d\u5b9a\u7ba1\u7406\u30ac\u30a4\u30c9 - YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080\u65b9\u6cd5\uff08\u63a8\u5968\uff09</li> <li>MLP\u30ac\u30a4\u30c9 - MLP\u306e\u8a73\u7d30\u306a\u4f7f\u7528\u65b9\u6cd5</li> <li>Encoder\u30ac\u30a4\u30c9 - Encoder\u306e\u8a73\u7d30\u306a\u4f7f\u7528\u65b9\u6cd5</li> <li>Decoder\u30ac\u30a4\u30c9 - Decoder\u306e\u8a73\u7d30\u306a\u4f7f\u7528\u65b9\u6cd5</li> <li>UNet\u30ac\u30a4\u30c9 - \u6761\u4ef6\u4ed8\u304dUNet\u306e\u4f7f\u7528\u65b9\u6cd5</li> <li>\u5206\u5e03\u30ac\u30a4\u30c9 - \u5206\u5e03\u306e\u8a73\u7d30\u306a\u4f7f\u7528\u65b9\u6cd5</li> <li>\u640d\u5931\u95a2\u6570\u30ac\u30a4\u30c9 - \u5404\u7a2e\u640d\u5931\u95a2\u6570\u306e\u4f7f\u7528\u65b9\u6cd5</li> <li>\u9ad8\u5ea6\u306a\u6a5f\u80fd - HyperNetwork\u3001\u5bfe\u7167\u5b66\u7fd2\u3001Attention\u6a5f\u69cb</li> <li>JAX\u30d0\u30c3\u30af\u30a8\u30f3\u30c9 - JAX (Flax NNX) \u3067\u306e\u4f7f\u7528\u65b9\u6cd5</li> <li>API \u30ea\u30d5\u30a1\u30ec\u30f3\u30b9 - \u5b8c\u5168\u306aAPI\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8</li> </ul>"},{"location":"api/","title":"API \u30ea\u30d5\u30a1\u30ec\u30f3\u30b9","text":"<p><code>ml-networks</code>\u306e\u5b8c\u5168\u306aAPI\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u3067\u3059\u3002</p>"},{"location":"api/#_1","title":"\u30e2\u30b8\u30e5\u30fc\u30eb\u4e00\u89a7","text":""},{"location":"api/#_2","title":"\u5171\u901a\u30e2\u30b8\u30e5\u30fc\u30eb","text":"<ul> <li>\u8a2d\u5b9a - \u8a2d\u5b9a\u30af\u30e9\u30b9\uff08PyTorch/JAX\u5171\u901a\uff09</li> <li>\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3 - \u5171\u901a\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\u95a2\u6570</li> </ul>"},{"location":"api/#pytorch-ml_networkstorch","title":"PyTorch (<code>ml_networks.torch</code>)","text":"<ul> <li>\u30ec\u30a4\u30e4\u30fc - \u57fa\u672c\u7684\u306a\u30ec\u30a4\u30e4\u30fc\uff08MLP\u3001Conv\u3001Attention\u3001Transformer\u306a\u3069\uff09</li> <li>\u30d3\u30b8\u30e7\u30f3 - \u30d3\u30b8\u30e7\u30f3\u95a2\u9023\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\uff08Encoder\u3001Decoder\u3001ConvNet\u3001ResNet\u3001ViT\u306a\u3069\uff09</li> <li>\u5206\u5e03 - \u5206\u5e03\u95a2\u9023\u306e\u30af\u30e9\u30b9\u3068\u95a2\u6570</li> <li>\u640d\u5931\u95a2\u6570 - \u640d\u5931\u95a2\u6570</li> <li>\u6d3b\u6027\u5316\u95a2\u6570 - \u30ab\u30b9\u30bf\u30e0\u6d3b\u6027\u5316\u95a2\u6570</li> <li>UNet - \u6761\u4ef6\u4ed8\u304dUNet\u30af\u30e9\u30b9</li> <li>\u305d\u306e\u4ed6 - HyperNet\u3001ContrastiveLearning\u3001BaseModule\u3001ProgressBarCallback</li> </ul>"},{"location":"api/#jax-ml_networksjax","title":"JAX (<code>ml_networks.jax</code>)","text":"<ul> <li>JAX API - JAX\uff08Flax NNX\uff09\u5b9f\u88c5\u306eAPI\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9</li> </ul>"},{"location":"api/#_3","title":"\u4e3b\u8981\u306a\u30af\u30e9\u30b9\u3068\u95a2\u6570","text":""},{"location":"api/#_4","title":"\u30ec\u30a4\u30e4\u30fc","text":""},{"location":"api/#ml_networks.torch.layers.MLPLayer","title":"MLPLayer","text":"<pre><code>MLPLayer(input_dim, output_dim, cfg)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>Multi-layer perceptron layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input dimension.</p> required <code>output_dim</code> <code>int</code> <p>Output dimension.</p> required <code>cfg</code> <code>MLPConfig</code> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = MLPConfig(\n...     hidden_dim=16,\n...     n_layers=3,\n...     output_activation=\"ReLU\",\n...     linear_cfg=LinearConfig(\n...         activation=\"ReLU\",\n...         norm=\"layer\",\n...         norm_cfg={\"eps\": 1e-05, \"elementwise_affine\": True, \"bias\": True},\n...         dropout=0.1,\n...         norm_first=False,\n...         bias=True\n...     )\n... )\n&gt;&gt;&gt; mlp = MLPLayer(32, 16, cfg)\n&gt;&gt;&gt; x = torch.randn(1, 32)\n&gt;&gt;&gt; output = mlp(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    output_dim: int,\n    cfg: MLPConfig,\n) -&gt; None:\n    super().__init__()\n    self.cfg = deepcopy(cfg)\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.hidden_dim = cfg.hidden_dim\n    self.n_layers = cfg.n_layers\n    self.dense = self._build_dense()\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.MLPLayer-attributes","title":"Attributes","text":""},{"location":"api/#ml_networks.torch.layers.MLPLayer.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = deepcopy(cfg)\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.MLPLayer.dense","title":"dense  <code>instance-attribute</code>","text":"<pre><code>dense = _build_dense()\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.MLPLayer.hidden_dim","title":"hidden_dim  <code>instance-attribute</code>","text":"<pre><code>hidden_dim = hidden_dim\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.MLPLayer.input_dim","title":"input_dim  <code>instance-attribute</code>","text":"<pre><code>input_dim = input_dim\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.MLPLayer.n_layers","title":"n_layers  <code>instance-attribute</code>","text":"<pre><code>n_layers = n_layers\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.MLPLayer.output_dim","title":"output_dim  <code>instance-attribute</code>","text":"<pre><code>output_dim = output_dim\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.MLPLayer-functions","title":"Functions","text":""},{"location":"api/#ml_networks.torch.layers.MLPLayer.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (*, input_dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (*, output_dim)</p> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (*, input_dim)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (*, output_dim)\n\n    \"\"\"\n    return self.dense(x)\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.LinearNormActivation","title":"LinearNormActivation","text":"<pre><code>LinearNormActivation(input_dim, output_dim, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Linear layer with normalization and activation, and dropouts.</p> References <p>LayerNorm: https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html RMSNorm: https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html Dropout: https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input dimension.</p> required <code>output_dim</code> <code>int</code> <p>Output dimension.</p> required <code>cfg</code> <code>LinearConfig</code> <p>Linear layer configuration.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = LinearConfig(\n...     activation=\"ReLU\",\n...     norm=\"layer\",\n...     norm_cfg={\"eps\": 1e-05, \"elementwise_affine\": True, \"bias\": True},\n...     dropout=0.1,\n...     norm_first=False,\n...     bias=True\n... )\n&gt;&gt;&gt; linear = LinearNormActivation(32, 16, cfg)\n&gt;&gt;&gt; linear\nLinearNormActivation(\n  (linear): Linear(in_features=32, out_features=16, bias=True)\n  (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n  (activation): Activation(\n    (activation): ReLU()\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n)\n&gt;&gt;&gt; x = torch.randn(1, 32)\n&gt;&gt;&gt; output = linear(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16])\n</code></pre> <pre><code>&gt;&gt;&gt; cfg = LinearConfig(\n...     activation=\"SiGLU\",\n...     norm=\"none\",\n...     norm_cfg={},\n...     dropout=0.0,\n...     norm_first=True,\n...     bias=True\n... )\n&gt;&gt;&gt; linear = LinearNormActivation(32, 16, cfg)\n&gt;&gt;&gt; # If activation includes \"glu\", linear output_dim is doubled to adjust actual output_dim.\n&gt;&gt;&gt; linear\nLinearNormActivation(\n  (linear): Linear(in_features=32, out_features=32, bias=True)\n  (norm): Identity()\n  (activation): Activation(\n    (activation): SiGLU()\n  )\n  (dropout): Identity()\n)\n&gt;&gt;&gt; x = torch.randn(1, 32)\n&gt;&gt;&gt; output = linear(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    output_dim: int,\n    cfg: LinearConfig,\n) -&gt; None:\n    super().__init__()\n    self.linear = nn.Linear(\n        input_dim,\n        output_dim * 2 if \"glu\" in cfg.activation.lower() else output_dim,\n        bias=cfg.bias,\n    )\n    if cfg.norm_first:\n        normalized_shape = input_dim\n    else:\n        normalized_shape = output_dim * 2 if \"glu\" in cfg.activation.lower() else output_dim\n\n    cfg.norm_cfg[\"normalized_shape\"] = normalized_shape\n    self.norm = get_norm(cfg.norm, **cfg.norm_cfg)\n    self.activation = Activation(cfg.activation)\n    self.dropout: nn.Module\n    if cfg.dropout &gt; 0:\n        self.dropout = nn.Dropout(cfg.dropout)\n    else:\n        self.dropout = nn.Identity()\n    self.norm_first = cfg.norm_first\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.LinearNormActivation-attributes","title":"Attributes","text":""},{"location":"api/#ml_networks.torch.layers.LinearNormActivation.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = Activation(activation)\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.LinearNormActivation.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.LinearNormActivation.linear","title":"linear  <code>instance-attribute</code>","text":"<pre><code>linear = Linear(input_dim, output_dim * 2 if 'glu' in lower() else output_dim, bias=bias)\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.LinearNormActivation.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = get_norm(norm, **(norm_cfg))\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.LinearNormActivation.norm_first","title":"norm_first  <code>instance-attribute</code>","text":"<pre><code>norm_first = norm_first\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.LinearNormActivation-functions","title":"Functions","text":""},{"location":"api/#ml_networks.torch.layers.LinearNormActivation.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (*, input_dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (*, output_dim)</p> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (*, input_dim)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (*, output_dim)\n    \"\"\"\n    if self.norm_first:\n        x = self.norm(x)\n        x = self.linear(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n    else:\n        x = self.linear(x)\n        x = self.norm(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n    return x\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.ConvNormActivation","title":"ConvNormActivation","text":"<pre><code>ConvNormActivation(in_channels, out_channels, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Convolutional layer with normalization and activation, and dropouts.</p> References <p>PixelShuffle: https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html PixelUnshuffle: https://pytorch.org/docs/stable/generated/torch.nn.PixelUnshuffle.html BatchNorm2d: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html GroupNorm: https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html LayerNorm: https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html InstanceNorm2d: https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html Conv2d: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html Dropout: https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Input channels.</p> required <code>out_channels</code> <code>int</code> <p>Output channels.</p> required <code>cfg</code> <code>ConvConfig</code> <p>Convolutional layer configuration.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = ConvConfig(\n...     activation=\"ReLU\",\n...     kernel_size=3,\n...     stride=1,\n...     padding=1,\n...     dilation=1,\n...     groups=1,\n...     bias=True,\n...     dropout=0.1,\n...     norm=\"batch\",\n...     norm_cfg={\"affine\": True, \"track_running_stats\": True},\n...     scale_factor=0\n... )\n&gt;&gt;&gt; conv = ConvNormActivation(3, 16, cfg)\n&gt;&gt;&gt; x = torch.randn(1, 3, 32, 32)\n&gt;&gt;&gt; output = conv(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16, 32, 32])\n</code></pre> <pre><code>&gt;&gt;&gt; cfg = ConvConfig(\n...     activation=\"SiGLU\",\n...     kernel_size=3,\n...     stride=1,\n...     padding=1,\n...     dilation=1,\n...     groups=1,\n...     bias=True,\n...     dropout=0.0,\n...     norm=\"none\",\n...     norm_cfg={},\n...     scale_factor=2\n... )\n&gt;&gt;&gt; conv = ConvNormActivation(3, 16, cfg)\n&gt;&gt;&gt; x = torch.randn(1, 3, 32, 32)\n&gt;&gt;&gt; output = conv(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16, 64, 64])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    cfg: ConvConfig,\n) -&gt; None:\n    super().__init__()\n\n    out_channels_ = out_channels\n    if \"glu\" in cfg.activation.lower():\n        out_channels_ *= 2\n    if cfg.scale_factor &gt; 0:\n        out_channels_ *= abs(cfg.scale_factor) ** 2\n    elif cfg.scale_factor &lt; 0:\n        out_channels_ //= abs(cfg.scale_factor) ** 2\n    self.conv = nn.Conv2d(\n        in_channels=in_channels,\n        out_channels=out_channels_,\n        kernel_size=cfg.kernel_size,\n        stride=cfg.stride,\n        padding=cfg.padding,\n        dilation=cfg.dilation,\n        groups=cfg.groups,\n        bias=cfg.bias,\n        padding_mode=cfg.padding_mode,\n    )\n    if cfg.norm != \"none\" and cfg.norm != \"group\":\n        cfg.norm_cfg[\"num_features\"] = out_channels_\n    elif cfg.norm == \"group\":\n        cfg.norm_cfg[\"num_channels\"] = in_channels if cfg.norm_first else out_channels_\n\n    norm_type: Literal[\"layer\", \"rms\", \"group\", \"batch2d\", \"batch1d\", \"none\"] = (\n        \"batch2d\" if cfg.norm == \"batch\" else cfg.norm\n    )  # type: ignore[assignment]\n    self.norm = get_norm(norm_type, **cfg.norm_cfg)\n    self.pixel_shuffle: nn.Module\n    if cfg.scale_factor &gt; 0:\n        self.pixel_shuffle = nn.PixelShuffle(cfg.scale_factor)\n    elif cfg.scale_factor &lt; 0:\n        self.pixel_shuffle = nn.PixelUnshuffle(abs(cfg.scale_factor))\n    else:\n        self.pixel_shuffle = nn.Identity()\n    self.activation = Activation(cfg.activation, dim=-3)\n    self.dropout: nn.Module = nn.Dropout(cfg.dropout) if cfg.dropout &gt; 0 else nn.Identity()\n    self.norm_first = cfg.norm_first\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.ConvNormActivation-attributes","title":"Attributes","text":""},{"location":"api/#ml_networks.torch.layers.ConvNormActivation.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = Activation(activation, dim=-3)\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.ConvNormActivation.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = Conv2d(in_channels=in_channels, out_channels=out_channels_, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias, padding_mode=padding_mode)\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.ConvNormActivation.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = Dropout(dropout) if dropout &gt; 0 else Identity()\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.ConvNormActivation.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = get_norm(norm_type, **(norm_cfg))\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.ConvNormActivation.norm_first","title":"norm_first  <code>instance-attribute</code>","text":"<pre><code>norm_first = norm_first\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.ConvNormActivation.pixel_shuffle","title":"pixel_shuffle  <code>instance-attribute</code>","text":"<pre><code>pixel_shuffle\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.ConvNormActivation-functions","title":"Functions","text":""},{"location":"api/#ml_networks.torch.layers.ConvNormActivation.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (B, in_channels, H, W) or (in_channels, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (B, out_channels, H', W') or (out_channels, H', W')</p> <code>H' and W' are calculated as follows:</code> <code>H' = (H + 2*padding - dilation * (kernel_size - 1) - 1) // stride + 1</code> <code>H' = H' * scale_factor if scale_factor &gt; 0 else H' // abs(scale_factor) if scale_factor &lt; 0 else H'</code> <code>W' = (W + 2*padding - dilation * (kernel_size - 1) - 1) // stride + 1</code> <code>W' = W' * scale_factor if scale_factor &gt; 0 else W' // abs(scale_factor) if scale_factor &lt; 0 else W'</code> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (B, in_channels, H, W) or (in_channels, H, W)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (B, out_channels, H', W') or (out_channels, H', W')\n    H' and W' are calculated as follows:\n    H' = (H + 2*padding - dilation * (kernel_size - 1) - 1) // stride + 1\n    H' = H' * scale_factor if scale_factor &gt; 0 else H' // abs(scale_factor) if scale_factor &lt; 0 else H'\n    W' = (W + 2*padding - dilation * (kernel_size - 1) - 1) // stride + 1\n    W' = W' * scale_factor if scale_factor &gt; 0 else W' // abs(scale_factor) if scale_factor &lt; 0 else W'\n\n    \"\"\"\n    if self.norm_first:\n        x = self.norm(x)\n        x = self.conv(x)\n        x = self.pixel_shuffle(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n    else:\n        x = self.conv(x)\n        x = self.norm(x)\n        x = self.pixel_shuffle(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n    return x\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.ConvTransposeNormActivation","title":"ConvTransposeNormActivation","text":"<pre><code>ConvTransposeNormActivation(in_channels, out_channels, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Transposed convolutional layer with normalization and activation, and dropouts.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Input channels.</p> required <code>out_channels</code> <code>int</code> <p>Output channels.</p> required <code>cfg</code> <code>ConvConfig</code> <p>Convolutional layer configuration.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = ConvConfig(\n...     activation=\"ReLU\",\n...     kernel_size=3,\n...     stride=1,\n...     padding=1,\n...     output_padding=0,\n...     dilation=1,\n...     groups=1,\n...     bias=True,\n...     dropout=0.1,\n...     norm=\"batch\",\n...     norm_cfg={\"affine\": True, \"track_running_stats\": True}\n... )\n&gt;&gt;&gt; conv = ConvTransposeNormActivation(3, 16, cfg)\n&gt;&gt;&gt; x = torch.randn(1, 3, 32, 32)\n&gt;&gt;&gt; output = conv(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16, 32, 32])\n</code></pre> <pre><code>&gt;&gt;&gt; cfg = ConvConfig(\n...     activation=\"SiGLU\",\n...     kernel_size=3,\n...     stride=1,\n...     padding=1,\n...     output_padding=0,\n...     dilation=1,\n...     groups=1,\n...     bias=True,\n...     dropout=0.0,\n...     norm=\"none\",\n...     norm_cfg={}\n... )\n&gt;&gt;&gt; conv = ConvTransposeNormActivation(3, 16, cfg)\n&gt;&gt;&gt; x = torch.randn(1, 3, 32, 32)\n&gt;&gt;&gt; output = conv(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16, 32, 32])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    cfg: ConvConfig,\n) -&gt; None:\n    super().__init__()\n\n    self.conv = nn.ConvTranspose2d(\n        in_channels,\n        out_channels * 2 if \"glu\" in cfg.activation.lower() else out_channels,\n        cfg.kernel_size,\n        cfg.stride,\n        cfg.padding,\n        cfg.output_padding,\n        cfg.groups,\n        bias=cfg.bias,\n        dilation=cfg.dilation,\n    )\n    if cfg.norm not in {\"none\", \"group\"}:\n        cfg.norm_cfg[\"num_features\"] = out_channels * 2 if \"glu\" in cfg.activation.lower() else out_channels\n    elif cfg.norm == \"group\":\n        cfg.norm_cfg[\"num_channels\"] = out_channels * 2 if \"glu\" in cfg.activation.lower() else out_channels\n    norm_type: Literal[\"layer\", \"rms\", \"group\", \"batch2d\", \"batch1d\", \"none\"] = (\n        \"batch2d\" if cfg.norm == \"batch\" else cfg.norm\n    )  # type: ignore[assignment]\n    self.norm = get_norm(norm_type, **cfg.norm_cfg)\n    self.activation = Activation(cfg.activation, dim=-3)\n    self.dropout: nn.Module = nn.Dropout(cfg.dropout) if cfg.dropout &gt; 0 else nn.Identity()\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.ConvTransposeNormActivation-attributes","title":"Attributes","text":""},{"location":"api/#ml_networks.torch.layers.ConvTransposeNormActivation.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = Activation(activation, dim=-3)\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.ConvTransposeNormActivation.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = ConvTranspose2d(in_channels, out_channels * 2 if 'glu' in lower() else out_channels, kernel_size, stride, padding, output_padding, groups, bias=bias, dilation=dilation)\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.ConvTransposeNormActivation.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = Dropout(dropout) if dropout &gt; 0 else Identity()\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.ConvTransposeNormActivation.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = get_norm(norm_type, **(norm_cfg))\n</code></pre>"},{"location":"api/#ml_networks.torch.layers.ConvTransposeNormActivation-functions","title":"Functions","text":""},{"location":"api/#ml_networks.torch.layers.ConvTransposeNormActivation.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (B, in_channels, H, W) or (in_channels, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (B, out_channels, H', W') or (out_channels, H', W')</p> <code>H' and W' are calculated as follows:</code> <code>H' = (H - 1) * stride - 2 * padding + kernel_size + output_padding</code> <code>W' = (W - 1) * stride - 2 * padding + kernel_size + output_padding</code> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (B, in_channels, H, W) or (in_channels, H, W)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (B, out_channels, H', W') or (out_channels, H', W')\n    H' and W' are calculated as follows:\n    H' = (H - 1) * stride - 2 * padding + kernel_size + output_padding\n    W' = (W - 1) * stride - 2 * padding + kernel_size + output_padding\n    \"\"\"\n    x = self.conv(x)\n    x = self.norm(x)\n    x = self.activation(x)\n    return self.dropout(x)\n</code></pre>"},{"location":"api/#_5","title":"\u30d3\u30b8\u30e7\u30f3","text":""},{"location":"api/#ml_networks.torch.vision.Encoder","title":"Encoder","text":"<pre><code>Encoder(feature_dim, obs_shape, backbone_cfg, fc_cfg=None)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Encoder with various architectures.</p> <p>Parameters:</p> Name Type Description Default <code>feature_dim</code> <code>int | tuple[int, int, int]</code> <p>Dimension of the feature tensor. If int, Encoder includes full connection layer to downsample the feature tensor. Otherwise, Encoder does not include full connection layer and directly process with backbone network.</p> required <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>shape of the input tensor</p> required <code>backbone_cfg</code> <code>ViTConfig | ConvNetConfig | ResNetConfig</code> <p>configuration of the network</p> required <code>fc_cfg</code> <code>MLPConfig | LinearConfig | SpatialSoftmaxConfig | None</code> <p>configuration of the full connection layer. If feature_dim is tuple, fc_cfg is ignored. If feature_dim is int, fc_cfg must be provided. Default is None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; feature_dim = 128\n&gt;&gt;&gt; obs_shape = (3, 64, 64)\n&gt;&gt;&gt; cfg = ConvNetConfig(\n...     channels=[16, 32, 64],\n...     conv_cfgs=[\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...     ]\n... )\n&gt;&gt;&gt; fc_cfg = LinearConfig(\n...     activation=\"ReLU\",\n...     bias=True\n... )\n&gt;&gt;&gt; encoder = Encoder(feature_dim, obs_shape, cfg, fc_cfg)\n&gt;&gt;&gt; x = torch.randn(2, *obs_shape)\n&gt;&gt;&gt; y = encoder(x)\n&gt;&gt;&gt; y.shape\ntorch.Size([2, 128])\n</code></pre> <pre><code>&gt;&gt;&gt; encoder\nEncoder(\n  (encoder): ConvNet(\n    (conv): Sequential(\n      (0): ConvNormActivation(\n        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (pixel_shuffle): Identity()\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n      (1): ConvNormActivation(\n        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (pixel_shuffle): Identity()\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n      (2): ConvNormActivation(\n        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (pixel_shuffle): Identity()\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n    )\n  )\n  (fc): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): LinearNormActivation(\n      (linear): Linear(in_features=4096, out_features=128, bias=True)\n      (norm): Identity()\n      (activation): Activation(\n        (activation): ReLU()\n      )\n      (dropout): Identity()\n    )\n  )\n)\n</code></pre> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def __init__(\n    self,\n    feature_dim: int | tuple[int, int, int],\n    obs_shape: tuple[int, int, int],\n    backbone_cfg: ViTConfig | ConvNetConfig | ResNetConfig,\n    fc_cfg: MLPConfig | LinearConfig | SpatialSoftmaxConfig | None = None,\n) -&gt; None:\n    super().__init__()\n\n    self.obs_shape = obs_shape\n\n    self.encoder: nn.Module\n    if isinstance(backbone_cfg, ViTConfig):\n        self.encoder = ViT(obs_shape, backbone_cfg)\n    elif isinstance(backbone_cfg, ConvNetConfig):\n        self.encoder = ConvNet(obs_shape, backbone_cfg)\n    elif isinstance(backbone_cfg, ResNetConfig):\n        self.encoder = ResNetPixUnshuffle(obs_shape, backbone_cfg)\n    else:\n        msg = f\"{type(backbone_cfg)} is not implemented\"\n        raise NotImplementedError(msg)\n\n    self.feature_dim = feature_dim\n    # \u578b\u60c5\u5831\u3092\u88dc\u3046\u305f\u3081\u306b\u660e\u793a\u7684\u306b\u30ad\u30e3\u30b9\u30c8\n    self.conved_size = cast(\"int\", self.encoder.conved_size)\n    self.conved_shape = cast(\"tuple[int, int]\", self.encoder.conved_shape)\n    self.last_channel = cast(\"int\", self.encoder.last_channel)\n\n    if isinstance(feature_dim, int):\n        assert fc_cfg is not None, \"fc_cfg must be provided if feature_dim is provided\"\n    else:\n        assert feature_dim == (self.last_channel, *self.conved_shape), (\n            f\"{feature_dim} != {(self.last_channel, *self.conved_shape)}\"\n        )\n    self.fc: nn.Module\n    if isinstance(fc_cfg, MLPConfig):\n        assert isinstance(feature_dim, int), \"feature_dim must be int when using MLPConfig\"\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            MLPLayer(self.conved_size, feature_dim, fc_cfg),\n        )\n    elif isinstance(fc_cfg, LinearConfig):\n        assert isinstance(feature_dim, int), \"feature_dim must be int when using LinearConfig\"\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            LinearNormActivation(self.conved_size, feature_dim, fc_cfg),\n        )\n    elif isinstance(fc_cfg, AdaptiveAveragePoolingConfig):\n        assert isinstance(feature_dim, int), \"feature_dim must be int when using AdaptiveAveragePoolingConfig\"\n        self.fc = nn.Sequential(\n            nn.AdaptiveAvgPool2d(fc_cfg.output_size),\n            nn.Flatten(),\n            LinearNormActivation(\n                int(self.last_channel * np.prod(fc_cfg.output_size)),\n                feature_dim,\n                fc_cfg.additional_layer,\n            )\n            if isinstance(\n                fc_cfg.additional_layer,\n                LinearConfig,\n            )\n            else MLPLayer(\n                int(self.last_channel * np.prod(fc_cfg.output_size)),\n                feature_dim,\n                fc_cfg.additional_layer,\n            )\n            if isinstance(\n                fc_cfg.additional_layer,\n                MLPConfig,\n            )\n            else nn.Identity(),\n        )\n        if fc_cfg.additional_layer is None:\n            self.feature_dim = (\n                self.last_channel * (fc_cfg.output_size**2)\n                if isinstance(\n                    fc_cfg.output_size,\n                    int,\n                )\n                else self.last_channel * np.prod(fc_cfg.output_size)\n            )\n\n    elif isinstance(fc_cfg, SpatialSoftmaxConfig):\n        assert isinstance(self.feature_dim, int), \"feature_dim must be int when using SpatialSoftmaxConfig\"\n        self.fc = nn.Sequential(\n            SpatialSoftmax(fc_cfg),\n            nn.Flatten(),\n            LinearNormActivation(\n                self.last_channel * 2,\n                self.feature_dim,\n                fc_cfg.additional_layer,\n            )\n            if isinstance(\n                fc_cfg.additional_layer,\n                LinearConfig,\n            )\n            else MLPLayer(\n                self.last_channel * 2,\n                self.feature_dim,\n                fc_cfg.additional_layer,\n            )\n            if isinstance(\n                fc_cfg.additional_layer,\n                MLPConfig,\n            )\n            else nn.Identity(),\n        )\n        if fc_cfg.additional_layer is None:\n            self.feature_dim = self.last_channel * 2\n    else:\n        self.fc = nn.Identity()\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Encoder-attributes","title":"Attributes","text":""},{"location":"api/#ml_networks.torch.vision.Encoder.conved_shape","title":"conved_shape  <code>instance-attribute</code>","text":"<pre><code>conved_shape = cast('tuple[int, int]', conved_shape)\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Encoder.conved_size","title":"conved_size  <code>instance-attribute</code>","text":"<pre><code>conved_size = cast('int', conved_size)\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Encoder.encoder","title":"encoder  <code>instance-attribute</code>","text":"<pre><code>encoder\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Encoder.fc","title":"fc  <code>instance-attribute</code>","text":"<pre><code>fc\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Encoder.feature_dim","title":"feature_dim  <code>instance-attribute</code>","text":"<pre><code>feature_dim = feature_dim\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Encoder.last_channel","title":"last_channel  <code>instance-attribute</code>","text":"<pre><code>last_channel = cast('int', last_channel)\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Encoder.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Encoder-functions","title":"Functions","text":""},{"location":"api/#ml_networks.torch.vision.Encoder.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor of shape (batch_size, *obs_shape)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor of shape (batch_size, *feature_dim)</p> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        input tensor of shape (batch_size, *obs_shape)\n\n    Returns\n    -------\n    torch.Tensor\n        output tensor of shape (batch_size, *feature_dim)\n    \"\"\"\n    batch_shape = x.shape[:-3]\n\n    x = x.reshape([-1, *self.obs_shape])\n    x = self.encoder(x)\n    x = x.view(-1, self.last_channel, *self.conved_shape)\n    x = self.fc(x)\n    return x.reshape([*batch_shape, *x.shape[1:]])\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Decoder","title":"Decoder","text":"<pre><code>Decoder(feature_dim, obs_shape, backbone_cfg, fc_cfg=None)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Decoder with various architectures.</p> <p>Parameters:</p> Name Type Description Default <code>feature_dim</code> <code>int | tuple[int, int, int]</code> <p>dimension of the feature tensor, if int, Decoder includes full connection layer to upsample the feature tensor. Otherwise, Decoder does not include full connection layer and directly process with backbone network.</p> required <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>shape of the output tensor</p> required <code>backbone_cfg</code> <code>ConvNetConfig | ViTConfig | ResNetConfig</code> <p>configuration of the network</p> required <code>fc_cfg</code> <code>MLPConfig | LinearConfig | None</code> <p>configuration of the full connection layer. If feature_dim is tuple, fc_cfg is ignored. If feature_dim is int, fc_cfg must be provided. Default is None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; feature_dim = 128\n&gt;&gt;&gt; obs_shape = (3, 64, 64)\n&gt;&gt;&gt; cfg = ConvNetConfig(\n...     channels=[64, 32, 16],\n...     conv_cfgs=[\n...         ConvConfig(kernel_size=4, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=4, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=4, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...     ]\n... )\n&gt;&gt;&gt; fc_cfg = MLPConfig(\n...     hidden_dim=256,\n...     n_layers=2,\n...     output_activation= \"ReLU\",\n...     linear_cfg= LinearConfig(\n...         activation= \"ReLU\",\n...         bias= True\n...     )\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; decoder = Decoder(feature_dim, obs_shape, cfg, fc_cfg)\n&gt;&gt;&gt; x = torch.randn(2, feature_dim)\n&gt;&gt;&gt; y = decoder(x)\n&gt;&gt;&gt; y.shape\ntorch.Size([2, 3, 64, 64])\n</code></pre> <pre><code>&gt;&gt;&gt; decoder\nDecoder(\n  (fc): MLPLayer(\n    (dense): Sequential(\n      (0): LinearNormActivation(\n        (linear): Linear(in_features=128, out_features=256, bias=True)\n        (norm): Identity()\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n      (1): LinearNormActivation(\n        (linear): Linear(in_features=256, out_features=256, bias=True)\n        (norm): Identity()\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n      (2): LinearNormActivation(\n        (linear): Linear(in_features=256, out_features=1024, bias=True)\n        (norm): Identity()\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n    )\n  )\n  (decoder): ConvTranspose(\n    (first_conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n    (conv): Sequential(\n      (0): ConvTransposeNormActivation(\n        (conv): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n        (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n      (1): ConvTransposeNormActivation(\n        (conv): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n        (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n      (2): ConvTransposeNormActivation(\n        (conv): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n        (norm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n    )\n  )\n)\n</code></pre> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def __init__(\n    self,\n    feature_dim: int | tuple[int, int, int],\n    obs_shape: tuple[int, int, int],\n    backbone_cfg: ConvNetConfig | ViTConfig | ResNetConfig,\n    fc_cfg: MLPConfig | LinearConfig | None = None,\n) -&gt; None:\n    super().__init__()\n\n    self.obs_shape = obs_shape\n    self.feature_dim = feature_dim\n\n    self.input_shape: tuple[int, int, int]\n    if isinstance(backbone_cfg, ViTConfig):\n        self.input_shape = ViT.get_input_shape(obs_shape, backbone_cfg)\n    elif isinstance(backbone_cfg, ConvNetConfig):\n        self.input_shape = cast(\n            \"tuple[int, int, int]\",\n            ConvTranspose.get_input_shape(obs_shape, backbone_cfg),\n        )\n    elif isinstance(backbone_cfg, ResNetConfig):\n        self.input_shape = cast(\n            \"tuple[int, int, int]\",\n            ResNetPixShuffle.get_input_shape(obs_shape, backbone_cfg),\n        )\n    else:\n        msg = f\"{type(backbone_cfg)} is not implemented\"\n        raise NotImplementedError(msg)\n    if isinstance(feature_dim, int):\n        assert fc_cfg is not None, \"fc_cfg must be provided if feature_dim is provided\"\n        self.has_fc = True\n    else:\n        assert feature_dim == self.input_shape, f\"{feature_dim} != {self.input_shape}\"\n        self.has_fc = False\n\n    if isinstance(fc_cfg, MLPConfig):\n        assert isinstance(feature_dim, int), \"feature_dim must be int when using MLPConfig\"\n        self.fc: nn.Module = MLPLayer(feature_dim, int(np.prod(self.input_shape)), fc_cfg)\n    elif isinstance(fc_cfg, LinearConfig):\n        assert isinstance(feature_dim, int), \"feature_dim must be int when using LinearConfig\"\n        self.fc = LinearNormActivation(feature_dim, int(np.prod(self.input_shape)), fc_cfg)\n    else:\n        self.fc = nn.Identity()\n\n    if isinstance(backbone_cfg, ViTConfig):\n        self.decoder: nn.Module = ViT(in_shape=self.input_shape, obs_shape=obs_shape, cfg=backbone_cfg)\n    elif isinstance(backbone_cfg, ConvNetConfig):\n        self.decoder = ConvTranspose(in_shape=self.input_shape, obs_shape=obs_shape, cfg=backbone_cfg)\n    elif isinstance(backbone_cfg, ResNetConfig):\n        self.decoder = ResNetPixShuffle(in_shape=self.input_shape, obs_shape=obs_shape, cfg=backbone_cfg)\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Decoder-attributes","title":"Attributes","text":""},{"location":"api/#ml_networks.torch.vision.Decoder.decoder","title":"decoder  <code>instance-attribute</code>","text":"<pre><code>decoder = ViT(in_shape=input_shape, obs_shape=obs_shape, cfg=backbone_cfg)\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Decoder.fc","title":"fc  <code>instance-attribute</code>","text":"<pre><code>fc = MLPLayer(feature_dim, int(prod(input_shape)), fc_cfg)\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Decoder.feature_dim","title":"feature_dim  <code>instance-attribute</code>","text":"<pre><code>feature_dim = feature_dim\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Decoder.has_fc","title":"has_fc  <code>instance-attribute</code>","text":"<pre><code>has_fc = True\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Decoder.input_shape","title":"input_shape  <code>instance-attribute</code>","text":"<pre><code>input_shape\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Decoder.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.Decoder-functions","title":"Functions","text":""},{"location":"api/#ml_networks.torch.vision.Decoder.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor of shape (batch_size, *feature_dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor of shape (batch_size, *obs_shape)</p> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        input tensor of shape (batch_size, *feature_dim)\n\n    Returns\n    -------\n    torch.Tensor\n        output tensor of shape (batch_size, *obs_shape)\n\n    \"\"\"\n    if self.has_fc:\n        batch_shape, data_shape = x.shape[:-1], x.shape[-1:]\n    else:\n        batch_shape, data_shape = x.shape[:-3], x.shape[-3:]\n    x = x.reshape([-1, *data_shape])\n    x = self.fc(x)\n    x = x.reshape([-1, *self.input_shape])\n    x = self.decoder(x)\n\n    return x.reshape([*batch_shape, *self.obs_shape])\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ConvNet","title":"ConvNet","text":"<pre><code>ConvNet(obs_shape, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Convolutional Neural Network for Encoder.</p> <p>Parameters:</p> Name Type Description Default <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>shape of input tensor</p> required <code>cfg</code> <code>ConvNetConfig</code> <p>configuration of the network</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; obs_shape = (3, 64, 64)\n&gt;&gt;&gt; cfg = ConvNetConfig(\n...     channels=[16, 32, 64],\n...     conv_cfgs=[\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...     ]\n... )\n&gt;&gt;&gt; encoder = ConvNet(obs_shape, cfg)\n&gt;&gt;&gt; encoder\nConvNet(\n  (conv): Sequential(\n    (0): ConvNormActivation(\n      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (pixel_shuffle): Identity()\n      (activation): Activation(\n        (activation): ReLU()\n      )\n      (dropout): Identity()\n    )\n    (1): ConvNormActivation(\n      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (pixel_shuffle): Identity()\n      (activation): Activation(\n        (activation): ReLU()\n      )\n      (dropout): Identity()\n    )\n    (2): ConvNormActivation(\n      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (pixel_shuffle): Identity()\n      (activation): Activation(\n        (activation): ReLU()\n      )\n      (dropout): Identity()\n    )\n  )\n)\n&gt;&gt;&gt; x = torch.randn(2, *obs_shape)\n&gt;&gt;&gt; y = encoder(x)\n&gt;&gt;&gt; y.shape\ntorch.Size([2, 64, 8, 8])\n</code></pre> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def __init__(\n    self,\n    obs_shape: tuple[int, int, int],\n    cfg: ConvNetConfig,\n) -&gt; None:\n    super().__init__()\n\n    self.obs_shape = obs_shape\n    self.channels = [obs_shape[0], *cfg.channels]\n    self.cfg = cfg\n\n    self.conv = self._build_conv()\n\n    self.last_channel = self.channels[-1]\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ConvNet-attributes","title":"Attributes","text":""},{"location":"api/#ml_networks.torch.vision.ConvNet.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ConvNet.channels","title":"channels  <code>instance-attribute</code>","text":"<pre><code>channels = [obs_shape[0], *(channels)]\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ConvNet.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = _build_conv()\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ConvNet.conved_shape","title":"conved_shape  <code>property</code>","text":"<pre><code>conved_shape\n</code></pre> <p>Get the shape of the output tensor after convolutional layers.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>shape of the output tensor</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; obs_shape = (3, 64, 64)\n&gt;&gt;&gt; cfg = ConvNetConfig(\n...     channels=[64, 32, 16],\n...     conv_cfgs=[\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...     ]\n... )\n&gt;&gt;&gt; encoder = ConvNet(obs_shape, cfg)\n&gt;&gt;&gt; encoder.conved_shape\n(8, 8)\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ConvNet.conved_size","title":"conved_size  <code>property</code>","text":"<pre><code>conved_size\n</code></pre> <p>Get the size of the output tensor after convolutional layers.</p> <p>Returns:</p> Type Description <code>int</code> <p>size of the output tensor</p>"},{"location":"api/#ml_networks.torch.vision.ConvNet.last_channel","title":"last_channel  <code>instance-attribute</code>","text":"<pre><code>last_channel = channels[-1]\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ConvNet.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ConvNet-functions","title":"Functions","text":""},{"location":"api/#ml_networks.torch.vision.ConvNet.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor of shape (batch_size, *obs_shape)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor of shape (batch_size, self.last_channel, *self.conved_shape)</p> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        input tensor of shape (batch_size, *obs_shape)\n\n    Returns\n    -------\n    torch.Tensor\n        output tensor of shape (batch_size, self.last_channel, *self.conved_shape)\n\n    \"\"\"\n    return self.conv(x)\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ResNetPixUnshuffle","title":"ResNetPixUnshuffle","text":"<pre><code>ResNetPixUnshuffle(obs_shape, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>ResNet with PixelUnshuffle for Encoder.</p> <p>Parameters:</p> Name Type Description Default <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>shape of input tensor</p> required <code>cfg</code> <code>ResNetConfig</code> <p>configuration of the network</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; obs_shape = (3, 64, 64)\n&gt;&gt;&gt; cfg = ResNetConfig(\n...     conv_channel=64,\n...     conv_kernel=3,\n...     f_kernel=3,\n...     conv_activation=\"ReLU\",\n...     out_activation=\"ReLU\",\n...     n_res_blocks=2,\n...     scale_factor=2,\n...     n_scaling=3,\n...     norm=\"batch\",\n...     norm_cfg={},\n...     dropout=0.0\n... )\n&gt;&gt;&gt; encoder = ResNetPixUnshuffle(obs_shape, cfg)\n&gt;&gt;&gt; x = torch.randn(2, *obs_shape)\n&gt;&gt;&gt; y = encoder(x)\n&gt;&gt;&gt; y.shape\ntorch.Size([2, 64, 8, 8])\n</code></pre> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def __init__(\n    self,\n    obs_shape: tuple[int, int, int],\n    cfg: ResNetConfig,\n) -&gt; None:\n    super().__init__()\n\n    self.obs_shape = obs_shape\n    self.cfg = cfg\n\n    first_cfg = ConvConfig(\n        activation=cfg.conv_activation,\n        kernel_size=cfg.f_kernel,\n        stride=1,\n        padding=cfg.f_kernel // 2,\n        dilation=1,\n        groups=1,\n        bias=True,\n        dropout=cfg.dropout,\n        norm=cfg.norm,\n        norm_cfg=cfg.norm_cfg,\n        padding_mode=cfg.padding_mode,\n    )\n    # First layer\n    self.conv1 = ConvNormActivation(self.obs_shape[0], cfg.conv_channel, first_cfg)\n\n    # downsampling\n    downsample: list[nn.Module] = []\n    downsample_cfg = first_cfg\n    downsample_cfg.kernel_size = cfg.conv_kernel\n    downsample_cfg.padding = cfg.conv_kernel // 2\n    downsample_cfg.scale_factor = -cfg.scale_factor\n    for _ in range(cfg.n_scaling):\n        downsample += [\n            ConvNormActivation(cfg.conv_channel, cfg.conv_channel, downsample_cfg),\n        ]\n    self.downsample = nn.Sequential(*downsample)\n\n    # Residual blocks\n    res_blocks: list[nn.Module] = []\n    for _ in range(cfg.n_res_blocks):\n        res_blocks += [\n            ResidualBlock(\n                cfg.conv_channel,\n                cfg.conv_kernel,\n                cfg.conv_activation,\n                cfg.norm,\n                cfg.norm_cfg,\n                cfg.dropout,\n                cfg.padding_mode,\n            ),\n        ]\n        if cfg.attention is not None:\n            res_blocks += [Attention2d(cfg.conv_channel, nhead=None, attn_cfg=cfg.attention)]\n\n    self.res_blocks = nn.Sequential(*res_blocks)\n\n    cov2_cfg = first_cfg\n    cov2_cfg.kernel_size = cfg.conv_kernel\n    cov2_cfg.padding = cfg.conv_kernel // 2\n    cov2_cfg.scale_factor = 0\n\n    # Second conv layer post residual blocks\n    self.conv2 = ConvNormActivation(cfg.conv_channel, cfg.conv_channel, cov2_cfg)\n\n    # Final output layer\n    final_cfg = first_cfg\n    final_cfg.kernel_size = cfg.conv_kernel\n    final_cfg.padding = cfg.conv_kernel // 2\n\n    self.conv3 = ConvNormActivation(cfg.conv_channel, cfg.conv_channel, final_cfg)\n    self.last_channel = cfg.conv_channel\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ResNetPixUnshuffle-attributes","title":"Attributes","text":""},{"location":"api/#ml_networks.torch.vision.ResNetPixUnshuffle.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ResNetPixUnshuffle.conv1","title":"conv1  <code>instance-attribute</code>","text":"<pre><code>conv1 = ConvNormActivation(obs_shape[0], conv_channel, first_cfg)\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ResNetPixUnshuffle.conv2","title":"conv2  <code>instance-attribute</code>","text":"<pre><code>conv2 = ConvNormActivation(conv_channel, conv_channel, cov2_cfg)\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ResNetPixUnshuffle.conv3","title":"conv3  <code>instance-attribute</code>","text":"<pre><code>conv3 = ConvNormActivation(conv_channel, conv_channel, final_cfg)\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ResNetPixUnshuffle.conved_shape","title":"conved_shape  <code>property</code>","text":"<pre><code>conved_shape\n</code></pre> <p>Get the shape of the output tensor after convolutional layers.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>shape of the output tensor</p>"},{"location":"api/#ml_networks.torch.vision.ResNetPixUnshuffle.conved_size","title":"conved_size  <code>property</code>","text":"<pre><code>conved_size\n</code></pre> <p>Get the size of the output tensor after convolutional layers.</p> <p>Returns:</p> Type Description <code>int</code> <p>size of the output tensor</p>"},{"location":"api/#ml_networks.torch.vision.ResNetPixUnshuffle.downsample","title":"downsample  <code>instance-attribute</code>","text":"<pre><code>downsample = Sequential(*downsample)\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ResNetPixUnshuffle.last_channel","title":"last_channel  <code>instance-attribute</code>","text":"<pre><code>last_channel = conv_channel\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ResNetPixUnshuffle.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ResNetPixUnshuffle.res_blocks","title":"res_blocks  <code>instance-attribute</code>","text":"<pre><code>res_blocks = Sequential(*res_blocks)\n</code></pre>"},{"location":"api/#ml_networks.torch.vision.ResNetPixUnshuffle-functions","title":"Functions","text":""},{"location":"api/#ml_networks.torch.vision.ResNetPixUnshuffle.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor of shape (batch_size, *obs_shape)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor of shape (batch_size, self.last_channel, *self.conved_shape)</p> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        input tensor of shape (batch_size, *obs_shape)\n\n    Returns\n    -------\n    torch.Tensor\n        output tensor of shape (batch_size, self.last_channel, *self.conved_shape)\n\n    \"\"\"\n    out = self.conv1(x)\n    out1 = self.downsample(out)\n    out_res = self.res_blocks(out1)\n    out2 = self.conv2(out_res)\n    out = torch.add(out1, out2)\n    return self.conv3(out)\n</code></pre>"},{"location":"api/#_6","title":"\u5206\u5e03","text":""},{"location":"api/#ml_networks.torch.distributions.Distribution","title":"Distribution","text":"<pre><code>Distribution(in_dim, dist, n_groups=1, spherical=False)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A distribution function.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input dimension.</p> required <code>dist</code> <code>Literal['normal', 'categorical', 'bernoulli']</code> <p>Distribution type.</p> required <code>n_groups</code> <code>int</code> <p>Number of groups. Default is 1. This is used for the categorical and Bernoulli distributions.</p> <code>1</code> <code>spherical</code> <code>bool</code> <p>Whether to project samples to the unit sphere. Default is False. This is used for the categorical and Bernoulli distributions. If True and dist==\"categorical\", the samples are projected from {0, 1} to {-1, 1}. If True and dist==\"bernoulli\", the samples are projected from {0, 1} to the unit sphere.</p> <p>refer to https://arxiv.org/abs/2406.07548</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dist = Distribution(10, \"normal\")\n&gt;&gt;&gt; data = torch.randn(2, 20)\n&gt;&gt;&gt; posterior = dist(data)\n&gt;&gt;&gt; posterior.__class__.__name__\n'NormalStoch'\n&gt;&gt;&gt; posterior.shape\nNormalShape(mean=torch.Size([2, 10]), std=torch.Size([2, 10]), stoch=torch.Size([2, 10]))\n</code></pre> <pre><code>&gt;&gt;&gt; dist = Distribution(10, \"categorical\", n_groups=2)\n&gt;&gt;&gt; data = torch.randn(2, 10)\n&gt;&gt;&gt; posterior = dist(data)\n&gt;&gt;&gt; posterior.__class__.__name__\n'CategoricalStoch'\n&gt;&gt;&gt; posterior.shape\nCategoricalShape(logits=torch.Size([2, 2, 5]), probs=torch.Size([2, 2, 5]), stoch=torch.Size([2, 10]))\n</code></pre> <pre><code>&gt;&gt;&gt; dist = Distribution(10, \"bernoulli\", n_groups=2)\n&gt;&gt;&gt; data = torch.randn(2, 10)\n&gt;&gt;&gt; posterior = dist(data)\n&gt;&gt;&gt; posterior.__class__.__name__\n'BernoulliStoch'\n&gt;&gt;&gt; posterior.shape\nBernoulliShape(logits=torch.Size([2, 2, 5]), probs=torch.Size([2, 2, 5]), stoch=torch.Size([2, 10]))\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __init__(\n    self,\n    in_dim: int,\n    dist: Literal[\"normal\", \"categorical\", \"bernoulli\"],\n    n_groups: int = 1,\n    spherical: bool = False,\n) -&gt; None:\n    super().__init__()\n\n    self.dist = dist\n    self.spherical = spherical\n    self.n_class = in_dim // n_groups\n    self.in_dim = in_dim\n    self.n_groups = n_groups\n\n    if dist == \"normal\":\n        self.posterior = self.normal  # type: ignore[assignment]\n    elif dist == \"categorical\":\n        self.posterior = self.categorical  # type: ignore[assignment]\n    elif dist == \"bernoulli\":\n        self.posterior = self.bernoulli  # type: ignore[assignment]\n    else:\n        raise NotImplementedError\n\n    if spherical:\n        self.codebook = BSQCodebook(self.n_class)\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.Distribution-attributes","title":"Attributes","text":""},{"location":"api/#ml_networks.torch.distributions.Distribution.codebook","title":"codebook  <code>instance-attribute</code>","text":"<pre><code>codebook = BSQCodebook(n_class)\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.Distribution.dist","title":"dist  <code>instance-attribute</code>","text":"<pre><code>dist = dist\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.Distribution.in_dim","title":"in_dim  <code>instance-attribute</code>","text":"<pre><code>in_dim = in_dim\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.Distribution.n_class","title":"n_class  <code>instance-attribute</code>","text":"<pre><code>n_class = in_dim // n_groups\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.Distribution.n_groups","title":"n_groups  <code>instance-attribute</code>","text":"<pre><code>n_groups = n_groups\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.Distribution.posterior","title":"posterior  <code>instance-attribute</code>","text":"<pre><code>posterior = normal\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.Distribution.spherical","title":"spherical  <code>instance-attribute</code>","text":"<pre><code>spherical = spherical\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.Distribution-functions","title":"Functions","text":""},{"location":"api/#ml_networks.torch.distributions.Distribution.bernoulli","title":"bernoulli","text":"<pre><code>bernoulli(logits, deterministic=False, inv_tmp=1.0)\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def bernoulli(self, logits: torch.Tensor, deterministic: bool = False, inv_tmp: float = 1.0) -&gt; BernoulliStoch:\n    batch_shape = logits.shape[:-1]\n    chunked_logits = torch.chunk(logits, self.n_groups, dim=-1)\n    logits = torch.stack(chunked_logits, dim=-2)\n    logits = logits * inv_tmp\n    probs = torch.sigmoid(logits)\n\n    dist = BernoulliStraightThrough(probs=probs)\n    posterior_dist = D.Independent(dist, 1)\n\n    sample = posterior_dist.rsample()\n\n    if self.spherical:\n        sample = self.codebook.bits_to_codes(sample)\n\n    if deterministic:\n        sample = (\n            torch.where(sample &gt; 0.5, torch.ones_like(sample), torch.zeros_like(sample)) + probs - probs.detach()\n        )\n\n    return BernoulliStoch(\n        logits,\n        probs,\n        sample.reshape([*batch_shape, -1]),\n    )\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.Distribution.categorical","title":"categorical","text":"<pre><code>categorical(logits, deterministic=False, inv_tmp=1.0)\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def categorical(self, logits: torch.Tensor, deterministic: bool = False, inv_tmp: float = 1.0) -&gt; CategoricalStoch:\n    batch_shape = logits.shape[:-1]\n    logits_chunk = torch.chunk(logits, self.n_groups, dim=-1)\n    logits = torch.stack(logits_chunk, dim=-2)\n    logits = logits\n    probs = softmax(logits, dim=-1, temperature=1 / inv_tmp)\n    dist = D.OneHotCategoricalStraightThrough(probs=probs)\n    posterior_dist = D.Independent(dist, 1)\n\n    sample = posterior_dist.rsample()\n\n    if self.spherical:\n        sample = sample * 2 - 1\n\n    return CategoricalStoch(\n        logits,\n        probs,\n        sample.reshape([*batch_shape, -1])\n        if not deterministic\n        else self.deterministic_onehot(probs).reshape([*batch_shape, -1]),\n    )\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.Distribution.deterministic_onehot","title":"deterministic_onehot","text":"<pre><code>deterministic_onehot(input)\n</code></pre> <p>Compute the one-hot vector by argmax.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>One-hot vector.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; input = torch.arange(6).reshape(2, 3) / 5.0\n&gt;&gt;&gt; dist = Distribution(3, \"categorical\")\n&gt;&gt;&gt; onehot = dist.deterministic_onehot(input)\n&gt;&gt;&gt; onehot\ntensor([[0., 0., 1.],\n        [0., 0., 1.]])\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def deterministic_onehot(self, input: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the one-hot vector by argmax.\n\n    Parameters\n    ----------\n    input : torch.Tensor\n        Input tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        One-hot vector.\n\n    Examples\n    --------\n    &gt;&gt;&gt; input = torch.arange(6).reshape(2, 3) / 5.0\n    &gt;&gt;&gt; dist = Distribution(3, \"categorical\")\n    &gt;&gt;&gt; onehot = dist.deterministic_onehot(input)\n    &gt;&gt;&gt; onehot\n    tensor([[0., 0., 1.],\n            [0., 0., 1.]])\n    \"\"\"\n    return F.one_hot(input.argmax(dim=-1), num_classes=self.n_class) + input - input.detach()\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.Distribution.forward","title":"forward","text":"<pre><code>forward(x, deterministic=False, inv_tmp=1.0)\n</code></pre> <p>Compute the posterior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>deterministic</code> <code>bool</code> <p>Whether to use the deterministic mode. Default is False. if True and dist==\"normal\", the mean is returned. if True and dist==\"categorical\", the one-hot vector computed by argmax is returned. if True and dist==\"bernoulli\", 1 is returned if x &gt; 0.5 or 0 is returned if x &lt;= 0.5.</p> <code>False</code> <code>inv_tmp</code> <code>float</code> <p>Inverse temperature. Default is 1.0. This is used for the categorical and Bernoulli distributions.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>StochState</code> <p>Posterior distribution.</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    deterministic: bool = False,\n    inv_tmp: float = 1.0,\n) -&gt; StochState:\n    \"\"\"\n    Compute the posterior distribution.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n    deterministic : bool, optional\n        Whether to use the deterministic mode. Default is False.\n        if True and dist==\"normal\", the mean is returned.\n        if True and dist==\"categorical\", the one-hot vector computed by argmax is returned.\n        if True and dist==\"bernoulli\", 1 is returned if x &gt; 0.5 or 0 is returned if x &lt;= 0.5.\n\n    inv_tmp : float, optional\n        Inverse temperature. Default is 1.0.\n        This is used for the categorical and Bernoulli distributions.\n\n    Returns\n    -------\n    StochState\n        Posterior distribution.\n\n\n    \"\"\"\n    return self.posterior(x, deterministic=deterministic, inv_tmp=inv_tmp)\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.Distribution.normal","title":"normal","text":"<pre><code>normal(mu_std, deterministic=False, inv_tmp=1.0)\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def normal(self, mu_std: torch.Tensor, deterministic: bool = False, inv_tmp: float = 1.0) -&gt; NormalStoch:\n    assert mu_std.shape[-1] == self.in_dim * 2, (\n        f\"mu_std.shape[-1] {mu_std.shape[-1]} and in_dim {self.in_dim} must be the same.\"\n    )\n\n    mu, std = torch.chunk(mu_std, 2, dim=-1)\n    std = F.softplus(std) + 1e-6\n\n    normal_dist = D.Normal(mu, std)\n    posterior_dist = D.Independent(normal_dist, 1)\n\n    sample = posterior_dist.rsample() if not deterministic else mu\n\n    return NormalStoch(mu, std, sample if not deterministic else mu)\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.NormalStoch","title":"NormalStoch  <code>dataclass</code>","text":"<pre><code>NormalStoch(mean, std, stoch)\n</code></pre> <p>Parameters of a normal distribution and its stochastic sample.</p> <p>Attributes:</p> Name Type Description <code>mean</code> <code>Tensor</code> <p>Mean of the normal distribution.</p> <code>std</code> <code>Tensor</code> <p>Standard deviation of the normal distribution.</p> <code>stoch</code> <code>Tensor</code> <p>sample from the normal distribution with reparametrization trick.</p>"},{"location":"api/#ml_networks.torch.distributions.NormalStoch-attributes","title":"Attributes","text":""},{"location":"api/#ml_networks.torch.distributions.NormalStoch.mean","title":"mean  <code>instance-attribute</code>","text":"<pre><code>mean\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.NormalStoch.shape","title":"shape  <code>property</code>","text":"<pre><code>shape\n</code></pre> <p>mean, std, stoch \u306e shape \u3092\u30bf\u30d7\u30eb\u3067\u8fd4\u3059.</p>"},{"location":"api/#ml_networks.torch.distributions.NormalStoch.std","title":"std  <code>instance-attribute</code>","text":"<pre><code>std\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.NormalStoch.stoch","title":"stoch  <code>instance-attribute</code>","text":"<pre><code>stoch\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.NormalStoch-functions","title":"Functions","text":""},{"location":"api/#ml_networks.torch.distributions.NormalStoch.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name)\n</code></pre> <p>torch.Tensor \u306b\u542b\u307e\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3092\u547c\u3073\u51fa\u3057\u305f\u3089\u3001\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b.</p> <p>\u4f8b: normal.flatten() \u2192 NormalStoch(mean.flatten(), std.flatten(), stoch.flatten()).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>\u30e1\u30bd\u30c3\u30c9\u540d\u3002</p> required <p>Returns:</p> Type Description <code>callable</code> <p>torch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3092\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b\u95a2\u6570\u3002</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>\u6307\u5b9a\u3055\u308c\u305f\u540d\u524d\u304ctorch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3067\u306a\u3044\u5834\u5408\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"torch.Tensor \u306b\u542b\u307e\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3092\u547c\u3073\u51fa\u3057\u305f\u3089\u3001\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b.\n\n    \u4f8b: normal.flatten() \u2192 NormalStoch(mean.flatten(), std.flatten(), stoch.flatten()).\n\n    Parameters\n    ----------\n    name : str\n        \u30e1\u30bd\u30c3\u30c9\u540d\u3002\n\n    Returns\n    -------\n    callable\n        torch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3092\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b\u95a2\u6570\u3002\n\n    Raises\n    ------\n    AttributeError\n        \u6307\u5b9a\u3055\u308c\u305f\u540d\u524d\u304ctorch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3067\u306a\u3044\u5834\u5408\u3002\n    \"\"\"\n    if hasattr(torch.Tensor, name):  # torch.Tensor \u306e\u30e1\u30bd\u30c3\u30c9\u304b\u78ba\u8a8d\n\n        def method(*args: Any, **kwargs: Any) -&gt; NormalStoch:\n            return NormalStoch(\n                getattr(self.mean, name)(*args, **kwargs),\n                getattr(self.std, name)(*args, **kwargs),\n                getattr(self.stoch, name)(*args, **kwargs),\n            )\n\n        return method\n    msg = f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    raise AttributeError(msg)\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.NormalStoch.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u30a2\u30af\u30bb\u30b9.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int or slice or tuple</code> <p>\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u6307\u5b9a\u3002</p> required <p>Returns:</p> Type Description <code>NormalStoch</code> <p>\u6307\u5b9a\u3055\u308c\u305f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5bfe\u5fdc\u3059\u308b<code>NormalStoch</code>\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __getitem__(self, idx: int | slice | tuple) -&gt; NormalStoch:\n    \"\"\"\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u30a2\u30af\u30bb\u30b9.\n\n    Parameters\n    ----------\n    idx : int or slice or tuple\n        \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u6307\u5b9a\u3002\n\n    Returns\n    -------\n    NormalStoch\n        \u6307\u5b9a\u3055\u308c\u305f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5bfe\u5fdc\u3059\u308b`NormalStoch`\u3002\n    \"\"\"\n    return NormalStoch(self.mean[idx], self.std[idx], self.stoch[idx])\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.NormalStoch.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>\u9577\u3055\u3092\u8fd4\u3059.</p> <p>Returns:</p> Type Description <code>int</code> <p>\u30d0\u30c3\u30c1\u6b21\u5143\u306e\u9577\u3055\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\u9577\u3055\u3092\u8fd4\u3059.\n\n    Returns\n    -------\n    int\n        \u30d0\u30c3\u30c1\u6b21\u5143\u306e\u9577\u3055\u3002\n    \"\"\"\n    return self.stoch.shape[0]\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.NormalStoch.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>\u521d\u671f\u5316\u5f8c\u306e\u51e6\u7406.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p><code>mean</code> \u3068 <code>std</code> \u306eshape\u304c\u7570\u306a\u308b\u5834\u5408\u3001\u307e\u305f\u306f<code>std</code>\u306b\u8ca0\u306e\u5024\u304c\u542b\u307e\u308c\u308b\u5834\u5408\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"\u521d\u671f\u5316\u5f8c\u306e\u51e6\u7406.\n\n    Raises\n    ------\n    ValueError\n        `mean` \u3068 `std` \u306eshape\u304c\u7570\u306a\u308b\u5834\u5408\u3001\u307e\u305f\u306f`std`\u306b\u8ca0\u306e\u5024\u304c\u542b\u307e\u308c\u308b\u5834\u5408\u3002\n    \"\"\"\n    if self.mean.shape != self.std.shape:\n        msg = f\"mean.shape {self.mean.shape} and std.shape {self.std.shape} must be the same.\"\n        raise ValueError(msg)\n    if (self.std &lt; 0).any():\n        msg = \"std must be non-negative.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.NormalStoch.get_distribution","title":"get_distribution","text":"<pre><code>get_distribution(independent=1)\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def get_distribution(self, independent: int = 1) -&gt; D.Independent:\n    return D.Independent(D.Normal(self.mean, self.std), independent)\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.NormalStoch.save","title":"save","text":"<pre><code>save(path)\n</code></pre> <p>Save the parameters of the normal distribution to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the parameters.</p> required Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"\n    Save the parameters of the normal distribution to the specified path.\n\n    Parameters\n    ----------\n    path : str\n        Path to save the parameters.\n\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n\n    save_blosc2(f\"{path}/mean.blosc2\", self.mean.detach().clone().cpu().numpy())\n    save_blosc2(f\"{path}/std.blosc2\", self.std.detach().clone().cpu().numpy())\n    save_blosc2(f\"{path}/stoch.blosc2\", self.stoch.detach().clone().cpu().numpy())\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.NormalStoch.squeeze","title":"squeeze","text":"<pre><code>squeeze(dim)\n</code></pre> <p>Squeeze the parameters of the normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension to squeeze.</p> required <p>Returns:</p> Type Description <code>NormalStoch</code> <p>Squeezed normal distribution.</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def squeeze(self, dim: int) -&gt; NormalStoch:\n    \"\"\"\n    Squeeze the parameters of the normal distribution.\n\n    Parameters\n    ----------\n    dim : int\n        Dimension to squeeze.\n\n    Returns\n    -------\n    NormalStoch\n        Squeezed normal distribution.\n\n    \"\"\"\n    return NormalStoch(\n        self.mean.squeeze(dim),\n        self.std.squeeze(dim),\n        self.stoch.squeeze(dim),\n    )\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.NormalStoch.unsqueeze","title":"unsqueeze","text":"<pre><code>unsqueeze(dim)\n</code></pre> <p>Unsqueeze the parameters of the normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension to unsqueeze.</p> required <p>Returns:</p> Type Description <code>NormalStoch</code> <p>Unsqueezed normal distribution.</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def unsqueeze(self, dim: int) -&gt; NormalStoch:\n    \"\"\"\n    Unsqueeze the parameters of the normal distribution.\n\n    Parameters\n    ----------\n    dim : int\n        Dimension to unsqueeze.\n\n    Returns\n    -------\n    NormalStoch\n        Unsqueezed normal distribution.\n\n    \"\"\"\n    return NormalStoch(\n        self.mean.unsqueeze(dim),\n        self.std.unsqueeze(dim),\n        self.stoch.unsqueeze(dim),\n    )\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.CategoricalStoch","title":"CategoricalStoch  <code>dataclass</code>","text":"<pre><code>CategoricalStoch(logits, probs, stoch)\n</code></pre> <p>Parameters of a categorical distribution and its stochastic sample.</p> <p>Attributes:</p> Name Type Description <code>logits</code> <code>Tensor</code> <p>Logits of the categorical distribution.</p> <code>probs</code> <code>Tensor</code> <p>Probabilities of the categorical distribution.</p> <code>stoch</code> <code>Tensor</code> <p>sample from the categorical distribution with Straight-Through Estimator.</p>"},{"location":"api/#ml_networks.torch.distributions.CategoricalStoch-attributes","title":"Attributes","text":""},{"location":"api/#ml_networks.torch.distributions.CategoricalStoch.logits","title":"logits  <code>instance-attribute</code>","text":"<pre><code>logits\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.CategoricalStoch.probs","title":"probs  <code>instance-attribute</code>","text":"<pre><code>probs\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.CategoricalStoch.shape","title":"shape  <code>property</code>","text":"<pre><code>shape\n</code></pre> <p>mean, std, stoch \u306e shape \u3092\u30bf\u30d7\u30eb\u3067\u8fd4\u3059.</p>"},{"location":"api/#ml_networks.torch.distributions.CategoricalStoch.stoch","title":"stoch  <code>instance-attribute</code>","text":"<pre><code>stoch\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.CategoricalStoch-functions","title":"Functions","text":""},{"location":"api/#ml_networks.torch.distributions.CategoricalStoch.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name)\n</code></pre> <p>torch.Tensor \u306b\u542b\u307e\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3092\u547c\u3073\u51fa\u3057\u305f\u3089\u3001\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b.</p> <p>\u4f8b: normal.flatten() \u2192 NormalStoch(mean.flatten(), std.flatten(), stoch.flatten()).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>\u30e1\u30bd\u30c3\u30c9\u540d\u3002</p> required <p>Returns:</p> Type Description <code>callable</code> <p>torch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3092\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b\u95a2\u6570\u3002</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>\u6307\u5b9a\u3055\u308c\u305f\u540d\u524d\u304ctorch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3067\u306a\u3044\u5834\u5408\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"torch.Tensor \u306b\u542b\u307e\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3092\u547c\u3073\u51fa\u3057\u305f\u3089\u3001\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b.\n\n    \u4f8b: normal.flatten() \u2192 NormalStoch(mean.flatten(), std.flatten(), stoch.flatten()).\n\n    Parameters\n    ----------\n    name : str\n        \u30e1\u30bd\u30c3\u30c9\u540d\u3002\n\n    Returns\n    -------\n    callable\n        torch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3092\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b\u95a2\u6570\u3002\n\n    Raises\n    ------\n    AttributeError\n        \u6307\u5b9a\u3055\u308c\u305f\u540d\u524d\u304ctorch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3067\u306a\u3044\u5834\u5408\u3002\n    \"\"\"\n    if hasattr(torch.Tensor, name):  # torch.Tensor \u306e\u30e1\u30bd\u30c3\u30c9\u304b\u78ba\u8a8d\n\n        def method(*args: Any, **kwargs: Any) -&gt; CategoricalStoch:\n            return CategoricalStoch(\n                getattr(self.logits, name)(*args, **kwargs),\n                getattr(self.probs, name)(*args, **kwargs),\n                getattr(self.stoch, name)(*args, **kwargs),\n            )\n\n        return method\n    msg = f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    raise AttributeError(msg)\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.CategoricalStoch.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u30a2\u30af\u30bb\u30b9.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int or slice or tuple</code> <p>\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u6307\u5b9a\u3002</p> required <p>Returns:</p> Type Description <code>CategoricalStoch</code> <p>\u6307\u5b9a\u3055\u308c\u305f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5bfe\u5fdc\u3059\u308b<code>CategoricalStoch</code>\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __getitem__(self, idx: int | slice | tuple) -&gt; CategoricalStoch:\n    \"\"\"\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u30a2\u30af\u30bb\u30b9.\n\n    Parameters\n    ----------\n    idx : int or slice or tuple\n        \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u6307\u5b9a\u3002\n\n    Returns\n    -------\n    CategoricalStoch\n        \u6307\u5b9a\u3055\u308c\u305f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5bfe\u5fdc\u3059\u308b`CategoricalStoch`\u3002\n    \"\"\"\n    return CategoricalStoch(self.logits[idx], self.probs[idx], self.stoch[idx])\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.CategoricalStoch.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>\u9577\u3055\u3092\u8fd4\u3059.</p> <p>Returns:</p> Type Description <code>int</code> <p>\u30d0\u30c3\u30c1\u6b21\u5143\u306e\u9577\u3055\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\u9577\u3055\u3092\u8fd4\u3059.\n\n    Returns\n    -------\n    int\n        \u30d0\u30c3\u30c1\u6b21\u5143\u306e\u9577\u3055\u3002\n    \"\"\"\n    return self.stoch.shape[0]\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.CategoricalStoch.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>\u521d\u671f\u5316\u5f8c\u306e\u51e6\u7406.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p><code>logits</code> \u3068 <code>probs</code> \u306eshape\u304c\u7570\u306a\u308b\u5834\u5408\u3001 \u3042\u308b\u3044\u306f<code>probs</code>\u304c[0, 1]\u306e\u7bc4\u56f2\u5916\u3001\u307e\u305f\u306f\u548c\u304c1\u304b\u3089\u5927\u304d\u304f\u305a\u308c\u3066\u3044\u308b\u5834\u5408\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"\u521d\u671f\u5316\u5f8c\u306e\u51e6\u7406.\n\n    Raises\n    ------\n    ValueError\n        `logits` \u3068 `probs` \u306eshape\u304c\u7570\u306a\u308b\u5834\u5408\u3001\n        \u3042\u308b\u3044\u306f`probs`\u304c[0, 1]\u306e\u7bc4\u56f2\u5916\u3001\u307e\u305f\u306f\u548c\u304c1\u304b\u3089\u5927\u304d\u304f\u305a\u308c\u3066\u3044\u308b\u5834\u5408\u3002\n    \"\"\"\n    if self.logits.shape != self.probs.shape:\n        msg = f\"logits.shape {self.logits.shape} and probs.shape {self.probs.shape} must be the same.\"\n        raise ValueError(msg)\n    if (self.probs &lt; 0).any() or (self.probs &gt; 1).any():\n        msg = \"probs must be in the range [0, 1].\"\n        raise ValueError(msg)\n    if (self.probs.sum(dim=-1) - 1).abs().max() &gt; 1e-6:\n        msg = \"probs must sum to 1.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.CategoricalStoch.get_distribution","title":"get_distribution","text":"<pre><code>get_distribution(independent=1)\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def get_distribution(self, independent: int = 1) -&gt; D.Independent:\n    return D.Independent(D.OneHotCategoricalStraightThrough(self.probs), independent)\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.CategoricalStoch.save","title":"save","text":"<pre><code>save(path)\n</code></pre> <p>Save the parameters of the categorical distribution to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the parameters.</p> required Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"\n    Save the parameters of the categorical distribution to the specified path.\n\n    Parameters\n    ----------\n    path : str\n        Path to save the parameters.\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n\n    save_blosc2(f\"{path}/logits.blosc2\", self.logits.detach().clone().cpu().numpy())\n    save_blosc2(f\"{path}/probs.blosc2\", self.probs.detach().clone().cpu().numpy())\n    save_blosc2(f\"{path}/stoch.blosc2\", self.stoch.detach().clone().cpu().numpy())\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.CategoricalStoch.squeeze","title":"squeeze","text":"<pre><code>squeeze(dim)\n</code></pre> <p>Squeeze the parameters of the categorical distribution.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension to squeeze.</p> required <p>Returns:</p> Type Description <code>CategoricalStoch</code> <p>Squeezed categorical distribution.</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def squeeze(self, dim: int) -&gt; CategoricalStoch:\n    \"\"\"\n    Squeeze the parameters of the categorical distribution.\n\n    Parameters\n    ----------\n    dim : int\n        Dimension to squeeze.\n\n    Returns\n    -------\n    CategoricalStoch\n        Squeezed categorical distribution.\n\n    \"\"\"\n    return CategoricalStoch(\n        self.logits.squeeze(dim),\n        self.probs.squeeze(dim),\n        self.stoch.squeeze(dim),\n    )\n</code></pre>"},{"location":"api/#ml_networks.torch.distributions.CategoricalStoch.unsqueeze","title":"unsqueeze","text":"<pre><code>unsqueeze(dim)\n</code></pre> <p>Unsqueeze the parameters of the categorical distribution.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension to unsqueeze.</p> required <p>Returns:</p> Type Description <code>CategoricalStoch</code> <p>Unsqueezed categorical distribution.</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def unsqueeze(self, dim: int) -&gt; CategoricalStoch:\n    \"\"\"\n    Unsqueeze the parameters of the categorical distribution.\n\n    Parameters\n    ----------\n    dim : int\n        Dimension to unsqueeze.\n\n    Returns\n    -------\n    CategoricalStoch\n        Unsqueezed categorical distribution.\n\n    \"\"\"\n    return CategoricalStoch(\n        self.logits.unsqueeze(dim),\n        self.probs.unsqueeze(dim),\n        self.stoch.unsqueeze(dim),\n    )\n</code></pre>"},{"location":"api/#_7","title":"\u640d\u5931\u95a2\u6570","text":""},{"location":"api/#ml_networks.torch.loss.focal_loss","title":"focal_loss","text":"<pre><code>focal_loss(prediction, target, gamma=2.0, sum_dim=-1)\n</code></pre> <p>Focal loss function. Mainly for multi-class classification.</p> Reference <p>Focal Loss for Dense Object Detection https://arxiv.org/abs/1708.02002</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted tensor. This should be before softmax.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor.</p> required <code>gamma</code> <code>float</code> <p>The gamma parameter. Default is 2.0.</p> <code>2.0</code> <code>sum_dim</code> <code>int</code> <p>The dimension to sum the loss. Default is -1.</p> <code>-1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The focal loss.</p> Source code in <code>src/ml_networks/torch/loss.py</code> <pre><code>def focal_loss(\n    prediction: torch.Tensor,\n    target: torch.Tensor,\n    gamma: float = 2.0,\n    sum_dim: int = -1,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Focal loss function. Mainly for multi-class classification.\n\n    Reference\n    ---------\n    Focal Loss for Dense Object Detection\n    https://arxiv.org/abs/1708.02002\n\n    Parameters\n    ----------\n    prediction : torch.Tensor\n        The predicted tensor. This should be before softmax.\n    target : torch.Tensor\n        The target tensor.\n    gamma : float\n        The gamma parameter. Default is 2.0.\n    sum_dim : int\n        The dimension to sum the loss. Default is -1.\n\n    Returns\n    -------\n    torch.Tensor\n        The focal loss.\n\n    \"\"\"\n    prediction = prediction.unsqueeze(1).transpose(sum_dim, 1).squeeze(-1)\n    if gamma:\n        log_prob = F.log_softmax(prediction, dim=1)\n        prob = torch.exp(log_prob)\n        loss = F.nll_loss((1 - prob) ** gamma * log_prob, target, reduction=\"none\")\n    else:\n        loss = F.cross_entropy(prediction, target, reduction=\"none\")\n    return loss.mean(0).sum()\n</code></pre>"},{"location":"api/#ml_networks.torch.loss.charbonnier","title":"charbonnier","text":"<pre><code>charbonnier(prediction, target, epsilon=0.001, alpha=1, sum_dim=None)\n</code></pre> <p>Charbonnier loss function.</p> Reference <p>A General and Adaptive Robust Loss Function http://arxiv.org/abs/1701.03077</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted tensor.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor.</p> required <code>epsilon</code> <code>float</code> <p>A small value to avoid division by zero. Default is 1e-3.</p> <code>0.001</code> <code>alpha</code> <code>float</code> <p>The alpha parameter. Default is 1.</p> <code>1</code> <code>sum_dim</code> <code>int | list[int] | tuple[int, ...] | None</code> <p>The dimension to sum the loss. Default is None (sums over [-1, -2, -3]).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The Charbonnier loss.</p> Source code in <code>src/ml_networks/torch/loss.py</code> <pre><code>def charbonnier(\n    prediction: torch.Tensor,\n    target: torch.Tensor,\n    epsilon: float = 1e-3,\n    alpha: float = 1,\n    sum_dim: int | list[int] | tuple[int, ...] | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Charbonnier loss function.\n\n    Reference\n    ---------\n    A General and Adaptive Robust Loss Function\n    http://arxiv.org/abs/1701.03077\n\n    Parameters\n    ----------\n    prediction : torch.Tensor\n        The predicted tensor.\n    target : torch.Tensor\n        The target tensor.\n    epsilon : float\n        A small value to avoid division by zero. Default is 1e-3.\n    alpha : float\n        The alpha parameter. Default is 1.\n    sum_dim : int | list[int] | tuple[int, ...] | None\n        The dimension to sum the loss. Default is None (sums over [-1, -2, -3]).\n\n    Returns\n    -------\n    torch.Tensor\n        The Charbonnier loss.\n\n    \"\"\"\n    if sum_dim is None:\n        sum_dim = [-1, -2, -3]\n    x = prediction - target\n    loss = (x**2 + epsilon**2) ** (alpha / 2)\n    return torch.sum(loss, dim=sum_dim)\n</code></pre>"},{"location":"api/#ml_networks.torch.loss.FocalFrequencyLoss","title":"FocalFrequencyLoss","text":"<pre><code>FocalFrequencyLoss(loss_weight=1.0, alpha=1.0, patch_factor=1, ave_spectrum=False, log_matrix=False, batch_matrix=False)\n</code></pre> <p>The torch.nn.Module class that implements focal frequency loss.</p> <p>A frequency domain loss function for optimizing generative models.</p> Reference <p>Focal Frequency Loss for Image Reconstruction and Synthesis. In ICCV 2021. https://arxiv.org/pdf/2012.12821.pdf</p> <p>Parameters:</p> Name Type Description Default <code>loss_weight</code> <code>float</code> <p>weight for focal frequency loss. Default: 1.0</p> <code>1.0</code> <code>alpha</code> <code>float</code> <p>the scaling factor alpha of the spectrum weight matrix for flexibility. Default: 1.0</p> <code>1.0</code> <code>patch_factor</code> <code>int</code> <p>the factor to crop image patches for patch-based focal frequency loss. Default: 1</p> <code>1</code> <code>ave_spectrum</code> <code>bool</code> <p>whether to use minibatch average spectrum. Default: False</p> <code>False</code> <code>log_matrix</code> <code>bool</code> <p>whether to adjust the spectrum weight matrix by logarithm. Default: False</p> <code>False</code> <code>batch_matrix</code> <code>bool</code> <p>whether to calculate the spectrum weight matrix using batch-based statistics. Default: False</p> <code>False</code> Source code in <code>src/ml_networks/torch/loss.py</code> <pre><code>def __init__(\n    self,\n    loss_weight: float = 1.0,\n    alpha: float = 1.0,\n    patch_factor: int = 1,\n    ave_spectrum: bool = False,\n    log_matrix: bool = False,\n    batch_matrix: bool = False,\n) -&gt; None:\n    self.loss_weight = loss_weight\n    self.alpha = alpha\n    self.patch_factor = patch_factor\n    self.ave_spectrum = ave_spectrum\n    self.log_matrix = log_matrix\n    self.batch_matrix = batch_matrix\n</code></pre>"},{"location":"api/#ml_networks.torch.loss.FocalFrequencyLoss-attributes","title":"Attributes","text":""},{"location":"api/#ml_networks.torch.loss.FocalFrequencyLoss.alpha","title":"alpha  <code>instance-attribute</code>","text":"<pre><code>alpha = alpha\n</code></pre>"},{"location":"api/#ml_networks.torch.loss.FocalFrequencyLoss.ave_spectrum","title":"ave_spectrum  <code>instance-attribute</code>","text":"<pre><code>ave_spectrum = ave_spectrum\n</code></pre>"},{"location":"api/#ml_networks.torch.loss.FocalFrequencyLoss.batch_matrix","title":"batch_matrix  <code>instance-attribute</code>","text":"<pre><code>batch_matrix = batch_matrix\n</code></pre>"},{"location":"api/#ml_networks.torch.loss.FocalFrequencyLoss.log_matrix","title":"log_matrix  <code>instance-attribute</code>","text":"<pre><code>log_matrix = log_matrix\n</code></pre>"},{"location":"api/#ml_networks.torch.loss.FocalFrequencyLoss.loss_weight","title":"loss_weight  <code>instance-attribute</code>","text":"<pre><code>loss_weight = loss_weight\n</code></pre>"},{"location":"api/#ml_networks.torch.loss.FocalFrequencyLoss.patch_factor","title":"patch_factor  <code>instance-attribute</code>","text":"<pre><code>patch_factor = patch_factor\n</code></pre>"},{"location":"api/#ml_networks.torch.loss.FocalFrequencyLoss-functions","title":"Functions","text":""},{"location":"api/#ml_networks.torch.loss.FocalFrequencyLoss.__call__","title":"__call__","text":"<pre><code>__call__(pred, target, matrix=None, mean_batch=True)\n</code></pre> <p>Forward function to calculate focal frequency loss.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>of shape (N, C, H, W). Predicted tensor.</p> required <code>target</code> <code>Tensor</code> <p>of shape (N, C, H, W). Target tensor.</p> required <code>matrix</code> <code>Tensor | None</code> <p>Default: None (If set to None: calculated online, dynamic).</p> <code>None</code> <code>mean_batch</code> <code>bool</code> <p>Whether to average over batch dimension.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The focal frequency loss.</p> Source code in <code>src/ml_networks/torch/loss.py</code> <pre><code>def __call__(\n    self,\n    pred: torch.Tensor,\n    target: torch.Tensor,\n    matrix: torch.Tensor | None = None,\n    mean_batch: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"Forward function to calculate focal frequency loss.\n\n    Parameters\n    ----------\n    pred: torch.Tensor\n        of shape (N, C, H, W). Predicted tensor.\n    target: torch.Tensor\n        of shape (N, C, H, W). Target tensor.\n    matrix: torch.Tensor | None\n        Default: None (If set to None: calculated online, dynamic).\n    mean_batch: bool\n        Whether to average over batch dimension.\n\n    Returns\n    -------\n    torch.Tensor\n        The focal frequency loss.\n    \"\"\"\n    if target.shape != pred.shape:\n        target = target.expand_as(pred)\n    if pred.ndim == 5:\n        batch_shape = pred.shape[:2]\n        pred = pred.flatten(0, 1)\n        target = target.flatten(0, 1)\n        flattened = True\n    else:\n        flattened = False\n\n    pred_freq = self.tensor2freq(pred)\n    target_freq = self.tensor2freq(target)\n\n    # whether to use minibatch average spectrum\n    if self.ave_spectrum:\n        pred_freq = torch.mean(pred_freq, 0, keepdim=True)\n        target_freq = torch.mean(target_freq, 0, keepdim=True)\n\n    # calculate focal frequency loss\n    loss = self.loss_formulation(pred_freq, target_freq, matrix, mean_batch) * self.loss_weight\n    if flattened and not mean_batch:\n        loss = loss.reshape(batch_shape)\n    return loss\n</code></pre>"},{"location":"api/#ml_networks.torch.loss.FocalFrequencyLoss.loss_formulation","title":"loss_formulation","text":"<pre><code>loss_formulation(recon_freq, real_freq, matrix=None, mean_batch=True)\n</code></pre> Source code in <code>src/ml_networks/torch/loss.py</code> <pre><code>def loss_formulation(\n    self,\n    recon_freq: torch.Tensor,\n    real_freq: torch.Tensor,\n    matrix: torch.Tensor | None = None,\n    mean_batch: bool = True,\n) -&gt; torch.Tensor:\n    # spectrum weight matrix\n    if matrix is not None:\n        # if the matrix is predefined\n        weight_matrix = matrix.detach()\n    else:\n        # if the matrix is calculated online: continuous, dynamic, based on current Euclidean distance\n        matrix_tmp = (recon_freq - real_freq) ** 2\n        matrix_tmp = torch.sqrt(matrix_tmp[..., 0] + matrix_tmp[..., 1]) ** self.alpha\n\n        # whether to adjust the spectrum weight matrix by logarithm\n        if self.log_matrix:\n            matrix_tmp = torch.log(matrix_tmp + 1.0)\n\n        # whether to calculate the spectrum weight matrix using batch-based statistics\n        if self.batch_matrix:\n            matrix_tmp = matrix_tmp / matrix_tmp.max()\n        else:\n            matrix_tmp = matrix_tmp / matrix_tmp.max(-1).values.max(-1).values[:, :, :, None, None]\n\n        matrix_tmp[torch.isnan(matrix_tmp)] = 0.0\n        matrix_tmp = torch.clamp(matrix_tmp, min=0.0, max=1.0)\n        weight_matrix = matrix_tmp.clone().detach()\n\n    min_val = weight_matrix.min().item()\n    max_val = weight_matrix.max().item()\n    assert min_val &gt;= 0, f\"The values of spectrum weight matrix should be &gt;= 0, but got Min: {min_val:.10f}\"\n    assert max_val &lt;= 1, f\"The values of spectrum weight matrix should be &lt;= 1, but got Max: {max_val:.10f}\"\n\n    # frequency distance using (squared) Euclidean distance\n    tmp = (recon_freq - real_freq) ** 2\n    freq_distance = tmp[..., 0] + tmp[..., 1]\n\n    # dynamic spectrum weighting (Hadamard product)\n    loss = weight_matrix * freq_distance\n    loss = loss.sum(dim=[-1, -2, -3])\n    if mean_batch:\n        loss = loss.mean()\n    return loss\n</code></pre>"},{"location":"api/#ml_networks.torch.loss.FocalFrequencyLoss.tensor2freq","title":"tensor2freq","text":"<pre><code>tensor2freq(x)\n</code></pre> Source code in <code>src/ml_networks/torch/loss.py</code> <pre><code>def tensor2freq(self, x: torch.Tensor) -&gt; torch.Tensor:\n    # crop image patches\n    patch_factor = self.patch_factor\n    _, _, h, w = x.shape\n    assert h % patch_factor == 0, \"Patch factor should be divisible by image height\"\n    assert w % patch_factor == 0, \"Patch factor should be divisible by image width\"\n    patch_h = h // patch_factor\n    patch_w = w // patch_factor\n    patch_list: list[torch.Tensor] = [\n        x[:, :, i * patch_h : (i + 1) * patch_h, j * patch_w : (j + 1) * patch_w]\n        for i in range(patch_factor)\n        for j in range(patch_factor)\n    ]\n\n    # stack to patch tensor\n    y = torch.stack(patch_list, 1)\n\n    # perform 2D DFT (real-to-complex, orthonormalization)\n    if IS_HIGH_VERSION:\n        freq = torch.fft.fft2(y, norm=\"ortho\")\n        freq = torch.stack([freq.real, freq.imag], -1)\n    else:\n        freq = torch.rfft(y, 2, onesided=False, normalized=True)  # type: ignore[attr-defined]\n    return freq\n</code></pre>"},{"location":"api/#_8","title":"\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3","text":""},{"location":"api/#ml_networks.torch.torch_utils.get_optimizer","title":"get_optimizer","text":"<pre><code>get_optimizer(param, name, **kwargs)\n</code></pre> <p>Get optimizer from torch.optim or pytorch_optimizer.</p> Args: <p>param : Iterator[nn.Parameter]     Parameters of models to optimize. name : str     Optimizer name. kwargs : dict     Optimizer arguments(settings).</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_optimizer([nn.Parameter(torch.randn(1, 3))], \"Adam\", lr=0.01)\nAdam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.01\n    maximize: False\n    weight_decay: 0\n)\n</code></pre> Source code in <code>src/ml_networks/torch/torch_utils.py</code> <pre><code>def get_optimizer(\n    param: Iterator[nn.Parameter],\n    name: str,\n    **kwargs: float | str | bool,\n) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Get optimizer from torch.optim or pytorch_optimizer.\n\n    Args:\n    -----\n    param : Iterator[nn.Parameter]\n        Parameters of models to optimize.\n    name : str\n        Optimizer name.\n    kwargs : dict\n        Optimizer arguments(settings).\n\n    Returns\n    -------\n    torch.optim.Optimizer\n\n    Examples\n    --------\n    &gt;&gt;&gt; get_optimizer([nn.Parameter(torch.randn(1, 3))], \"Adam\", lr=0.01)\n    Adam (\n    Parameter Group 0\n        amsgrad: False\n        betas: (0.9, 0.999)\n        capturable: False\n        differentiable: False\n        eps: 1e-08\n        foreach: None\n        fused: None\n        lr: 0.01\n        maximize: False\n        weight_decay: 0\n    )\n    \"\"\"\n    if hasattr(schedulefree, name):\n        optimizer = getattr(schedulefree, name)\n    elif hasattr(torch.optim, name):\n        optimizer = getattr(torch.optim, name)\n    elif hasattr(pytorch_optimizer, name):\n        optimizer = getattr(pytorch_optimizer, name)\n    else:\n        msg = f\"Optimizer {name} is not implemented in torch.optim or pytorch_optimizer, schedulefree. \"\n        msg += \"Please check the name and capitalization.\"\n        raise NotImplementedError(msg)\n    return optimizer(param, **kwargs)\n</code></pre>"},{"location":"api/#ml_networks.torch.torch_utils.torch_fix_seed","title":"torch_fix_seed","text":"<pre><code>torch_fix_seed(seed=42)\n</code></pre> <p>\u4e71\u6570\u3092\u56fa\u5b9a\u3059\u308b\u95a2\u6570.</p> References <ul> <li>https://qiita.com/north_redwing/items/1e153139125d37829d2d</li> </ul> Source code in <code>src/ml_networks/torch/torch_utils.py</code> <pre><code>def torch_fix_seed(seed: int = 42) -&gt; None:\n    \"\"\"\n    \u4e71\u6570\u3092\u56fa\u5b9a\u3059\u308b\u95a2\u6570.\n\n    References\n    ----------\n    - https://qiita.com/north_redwing/items/1e153139125d37829d2d\n    \"\"\"\n    random.seed(seed)\n    pl.seed_everything(seed, workers=True)\n    torch.set_float32_matmul_precision(\"medium\")\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n</code></pre>"},{"location":"api/#ml_networks.utils.save_blosc2","title":"save_blosc2","text":"<pre><code>save_blosc2(path, x)\n</code></pre> <p>Save numpy array with blosc2 compression.</p> Args: <p>path : str     Path to save. x : np.ndarray     Numpy array to save.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; save_blosc2(\"test.blosc2\", np.random.randn(10, 10))\n</code></pre> Source code in <code>src/ml_networks/utils.py</code> <pre><code>def save_blosc2(path: str, x: np.ndarray) -&gt; None:\n    \"\"\"Save numpy array with blosc2 compression.\n\n    Args:\n    -----\n    path : str\n        Path to save.\n    x : np.ndarray\n        Numpy array to save.\n\n    Examples\n    --------\n    &gt;&gt;&gt; save_blosc2(\"test.blosc2\", np.random.randn(10, 10))\n\n    \"\"\"\n    Path(path).write_bytes(blosc2.pack_array2(x))\n</code></pre>"},{"location":"api/#ml_networks.utils.load_blosc2","title":"load_blosc2","text":"<pre><code>load_blosc2(path)\n</code></pre> <p>Load numpy array with blosc2 compression.</p> Args: <p>path : str     Path to load.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = load_blosc2(\"test.blosc2\")\n&gt;&gt;&gt; type(data)\n&lt;class 'numpy.ndarray'&gt;\n</code></pre> Source code in <code>src/ml_networks/utils.py</code> <pre><code>def load_blosc2(path: str) -&gt; np.ndarray:\n    \"\"\"Load numpy array with blosc2 compression.\n\n    Args:\n    -----\n    path : str\n        Path to load.\n\n    Returns\n    -------\n    np.ndarray\n        Numpy array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; data = load_blosc2(\"test.blosc2\")\n    &gt;&gt;&gt; type(data)\n    &lt;class 'numpy.ndarray'&gt;\n    \"\"\"\n    return blosc2.unpack_array2(Path(path).read_bytes())\n</code></pre>"},{"location":"api/activations/","title":"\u6d3b\u6027\u5316\u95a2\u6570","text":"<p>\u30ab\u30b9\u30bf\u30e0\u6d3b\u6027\u5316\u95a2\u6570\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p> <p><code>ml_networks.torch.activations</code>\uff08PyTorch\uff09\u3068<code>ml_networks.jax.activations</code>\uff08JAX\uff09\u306e\u4e21\u65b9\u3067\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> <p>PyTorch\u306b\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\u6d3b\u6027\u5316\u95a2\u6570\u306b\u52a0\u3048\u3066\u3001\u4ee5\u4e0b\u306e\u30ab\u30b9\u30bf\u30e0\u6d3b\u6027\u5316\u95a2\u6570\u304c\u4f7f\u3048\u307e\u3059\u3002</p>"},{"location":"api/activations/#activation","title":"Activation","text":""},{"location":"api/activations/#ml_networks.torch.activations.Activation","title":"Activation","text":"<pre><code>Activation(activation, **kwargs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Generic activation function.</p> Source code in <code>src/ml_networks/torch/activations.py</code> <pre><code>def __init__(self, activation: str, **kwargs: Any) -&gt; None:\n    super().__init__()\n    if \"glu\" not in activation.lower():\n        kwargs.pop(\"dim\", None)\n    try:\n        self.activation = getattr(nn, activation)(**kwargs)\n    except AttributeError as err:\n        if activation == \"TanhExp\":\n            self.activation = TanhExp()\n        elif activation == \"REReLU\":\n            self.activation = REReLU(**kwargs)\n        elif activation in {\"SiGLU\", \"SwiGLU\"}:\n            self.activation = SiGLU(**kwargs)\n        elif activation == \"CRReLU\":\n            self.activation = CRReLU(**kwargs)\n        elif activation == \"L2Norm\":\n            self.activation = L2Norm()\n        else:\n            msg = f\"Activation: '{activation}' is not implemented yet.\"\n            raise NotImplementedError(msg) from err\n</code></pre>"},{"location":"api/activations/#ml_networks.torch.activations.Activation-attributes","title":"Attributes","text":""},{"location":"api/activations/#ml_networks.torch.activations.Activation.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = getattr(nn, activation)(**kwargs)\n</code></pre>"},{"location":"api/activations/#ml_networks.torch.activations.Activation-functions","title":"Functions","text":""},{"location":"api/activations/#ml_networks.torch.activations.Activation.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> Source code in <code>src/ml_networks/torch/activations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return self.activation(x)\n</code></pre>"},{"location":"api/activations/#rerelu","title":"REReLU","text":"<p>Reparametrized ReLU: \u9006\u4f1d\u64ad\u304cGELU\u7b49\u306b\u306a\u308bReLU\u3002See paper</p>"},{"location":"api/activations/#ml_networks.torch.activations.REReLU","title":"REReLU","text":"<pre><code>REReLU(reparametarize_fn='gelu')\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Reparametarized ReLU activation function. This backward pass is differentiable.</p> References <p>https://openreview.net/forum?id=lNCnZwcH5Z</p> <p>Parameters:</p> Name Type Description Default <code>reparametarize_fn</code> <code>str</code> <p>Reparametarization function. Default is GELU.</p> <code>'gelu'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rerelu = REReLU()\n&gt;&gt;&gt; x = torch.randn(1, 3)\n&gt;&gt;&gt; output = rerelu(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 3])\n</code></pre> Source code in <code>src/ml_networks/torch/activations.py</code> <pre><code>def __init__(self, reparametarize_fn: str = \"gelu\") -&gt; None:\n    super().__init__()\n    reparametarize_fn = reparametarize_fn.lower()\n    self.reparametarize_fn = getattr(F, reparametarize_fn)\n</code></pre>"},{"location":"api/activations/#ml_networks.torch.activations.REReLU-attributes","title":"Attributes","text":""},{"location":"api/activations/#ml_networks.torch.activations.REReLU.reparametarize_fn","title":"reparametarize_fn  <code>instance-attribute</code>","text":"<pre><code>reparametarize_fn = getattr(functional, reparametarize_fn)\n</code></pre>"},{"location":"api/activations/#ml_networks.torch.activations.REReLU-functions","title":"Functions","text":""},{"location":"api/activations/#ml_networks.torch.activations.REReLU.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> Source code in <code>src/ml_networks/torch/activations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return F.relu(x).detach() + self.reparametarize_fn(x) - self.reparametarize_fn(x).detach()\n</code></pre>"},{"location":"api/activations/#siglu","title":"SiGLU","text":"<p>SiLU + GLU: SiLU(Swish)\u3068GLU\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u6d3b\u6027\u5316\u95a2\u6570\u3002See paper</p>"},{"location":"api/activations/#ml_networks.torch.activations.SiGLU","title":"SiGLU","text":"<pre><code>SiGLU(dim=-1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>SiGLU activation function.</p> <p>This is equivalent to SwiGLU (Swish variant of Gated Linear Unit) activation function.</p> References <p>https://arxiv.org/abs/2102.11972</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension to split the tensor. Default is -1.</p> <code>-1</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; siglu = SiGLU()\n&gt;&gt;&gt; x = torch.randn(1, 4)\n&gt;&gt;&gt; output = siglu(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 2])\n</code></pre> <pre><code>&gt;&gt;&gt; siglu = SiGLU(dim=0)\n&gt;&gt;&gt; x = torch.randn(4, 1)\n&gt;&gt;&gt; output = siglu(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([2, 1])\n</code></pre> Source code in <code>src/ml_networks/torch/activations.py</code> <pre><code>def __init__(self, dim: int = -1) -&gt; None:\n    super().__init__()\n    self.dim = dim\n</code></pre>"},{"location":"api/activations/#ml_networks.torch.activations.SiGLU-attributes","title":"Attributes","text":""},{"location":"api/activations/#ml_networks.torch.activations.SiGLU.dim","title":"dim  <code>instance-attribute</code>","text":"<pre><code>dim = dim\n</code></pre>"},{"location":"api/activations/#ml_networks.torch.activations.SiGLU-functions","title":"Functions","text":""},{"location":"api/activations/#ml_networks.torch.activations.SiGLU.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> Source code in <code>src/ml_networks/torch/activations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    x1, x2 = x.chunk(2, dim=self.dim)\n    return x1 * F.silu(x2)\n</code></pre>"},{"location":"api/activations/#crrelu","title":"CRReLU","text":"<p>Correction Regularized ReLU: \u6b63\u5247\u5316\u3055\u308c\u305fReLU\u3002See paper</p>"},{"location":"api/activations/#ml_networks.torch.activations.CRReLU","title":"CRReLU","text":"<pre><code>CRReLU(lr=0.01)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Correction Regularized ReLU activation function. This is a variant of ReLU activation function.</p> References <p>https://openreview.net/forum?id=7TZYM6Hm9p</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>Learning rate. Default is 0.01.</p> <code>0.01</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; crrelu = CRReLU()\n&gt;&gt;&gt; x = torch.randn(1, 3)\n&gt;&gt;&gt; output = crrelu(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 3])\n</code></pre> Source code in <code>src/ml_networks/torch/activations.py</code> <pre><code>def __init__(self, lr: float = 0.01) -&gt; None:\n    super().__init__()\n    self.lr = nn.Parameter(torch.tensor(lr).float())\n</code></pre>"},{"location":"api/activations/#ml_networks.torch.activations.CRReLU-attributes","title":"Attributes","text":""},{"location":"api/activations/#ml_networks.torch.activations.CRReLU.lr","title":"lr  <code>instance-attribute</code>","text":"<pre><code>lr = Parameter(float())\n</code></pre>"},{"location":"api/activations/#ml_networks.torch.activations.CRReLU-functions","title":"Functions","text":""},{"location":"api/activations/#ml_networks.torch.activations.CRReLU.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> Source code in <code>src/ml_networks/torch/activations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return F.relu(x) + self.lr * x * torch.exp(-(x**2) / 2)\n</code></pre>"},{"location":"api/activations/#tanhexp","title":"TanhExp","text":"<p>Mish\u306e\u6539\u5584\u7248\u3068\u3044\u3046\u4f4d\u7f6e\u4ed8\u3051\u3002See article</p>"},{"location":"api/activations/#ml_networks.torch.activations.TanhExp","title":"TanhExp","text":"<p>               Bases: <code>Module</code></p> <p>TanhExp activation function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tanhexp = TanhExp()\n&gt;&gt;&gt; x = torch.randn(1, 3)\n&gt;&gt;&gt; output = tanhexp(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 3])\n</code></pre>"},{"location":"api/activations/#ml_networks.torch.activations.TanhExp-functions","title":"Functions","text":""},{"location":"api/activations/#ml_networks.torch.activations.TanhExp.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> Source code in <code>src/ml_networks/torch/activations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return TanhExpBase.apply(x)\n</code></pre>"},{"location":"api/activations/#l2norm","title":"L2Norm","text":"<p>L2\u6b63\u898f\u5316\u30ec\u30a4\u30e4\u30fc\u3002\u7279\u5fb4\u91cf\u3092\u5358\u4f4d\u8d85\u7403\u4e0a\u306b\u5c04\u5f71\u3057\u307e\u3059\u3002</p>"},{"location":"api/activations/#ml_networks.torch.activations.L2Norm","title":"L2Norm","text":"<p>               Bases: <code>Module</code></p> <p>L2 Normalization layer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; l2norm = L2Norm()\n&gt;&gt;&gt; x = torch.randn(2, 3)\n&gt;&gt;&gt; output = l2norm(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([2, 3])\n</code></pre>"},{"location":"api/activations/#ml_networks.torch.activations.L2Norm-functions","title":"Functions","text":""},{"location":"api/activations/#ml_networks.torch.activations.L2Norm.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> Source code in <code>src/ml_networks/torch/activations.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return F.normalize(x, p=2, dim=-1)\n</code></pre>"},{"location":"api/config/","title":"\u8a2d\u5b9a","text":"<p>\u8a2d\u5b9a\u30af\u30e9\u30b9\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p>"},{"location":"api/config/#mlpconfig","title":"MLPConfig","text":""},{"location":"api/config/#ml_networks.config.MLPConfig","title":"MLPConfig  <code>dataclass</code>","text":"<pre><code>MLPConfig(hidden_dim, n_layers, output_activation, linear_cfg)\n</code></pre> <p>Multi-layer perceptron configuration.</p> <p>Attributes:</p> Name Type Description <code>hidden_dim</code> <code>int</code> <p>Number of hidden units.</p> <code>n_layers</code> <code>int</code> <p>Number of layers.</p> <code>output_activation</code> <code>str</code> <p>Activation function for output layer.</p> <code>linear_cfg</code> <code>LinearConfig</code> <p>Linear layer configuration.</p>"},{"location":"api/config/#ml_networks.config.MLPConfig-attributes","title":"Attributes","text":""},{"location":"api/config/#ml_networks.config.MLPConfig.hidden_dim","title":"hidden_dim  <code>instance-attribute</code>","text":"<pre><code>hidden_dim\n</code></pre>"},{"location":"api/config/#ml_networks.config.MLPConfig.linear_cfg","title":"linear_cfg  <code>instance-attribute</code>","text":"<pre><code>linear_cfg\n</code></pre>"},{"location":"api/config/#ml_networks.config.MLPConfig.n_layers","title":"n_layers  <code>instance-attribute</code>","text":"<pre><code>n_layers\n</code></pre>"},{"location":"api/config/#ml_networks.config.MLPConfig.output_activation","title":"output_activation  <code>instance-attribute</code>","text":"<pre><code>output_activation\n</code></pre>"},{"location":"api/config/#ml_networks.config.MLPConfig-functions","title":"Functions","text":""},{"location":"api/config/#ml_networks.config.MLPConfig.dictcfg2dict","title":"dictcfg2dict","text":"<pre><code>dictcfg2dict()\n</code></pre> <p>Convert dictConfig to dict for <code>MLPConfig</code>.</p> Source code in <code>src/ml_networks/config.py</code> <pre><code>def dictcfg2dict(self) -&gt; None:\n    \"\"\"Convert dictConfig to dict for `MLPConfig`.\"\"\"\n    self.linear_cfg.dictcfg2dict()\n    for key, value in self.__dict__.items():\n        if isinstance(value, DictConfig | ListConfig | list | tuple | dict):\n            setattr(self, key, convert_dictconfig_to_dict(value))\n</code></pre>"},{"location":"api/config/#linearconfig","title":"LinearConfig","text":""},{"location":"api/config/#ml_networks.config.LinearConfig","title":"LinearConfig  <code>dataclass</code>","text":"<pre><code>LinearConfig(activation, norm='none', norm_cfg=dict(), dropout=0.0, norm_first=False, bias=True)\n</code></pre> <p>A linear layer configuration.</p> <p>Attributes:</p> Name Type Description <code>activation</code> <code>str</code> <p>Activation function.</p> <code>norm</code> <code>Literal['layer', 'rms', 'none']</code> <p>Normalization layer. If it's set to \"none\", normalization is not applied. Default is \"none\".</p> <code>norm_cfg</code> <code>dict</code> <p>Normalization layer configuration. Default is {}.</p> <code>dropout</code> <code>float</code> <p>Dropout rate. If it's set to 0.0, dropout is not applied. Default is 0.0.</p> <code>norm_first</code> <code>bool</code> <p>Whether to apply normalization before linear layer. Default is False.</p> <code>bias</code> <code>bool</code> <p>Whether to use bias. Default is True.</p>"},{"location":"api/config/#ml_networks.config.LinearConfig-attributes","title":"Attributes","text":""},{"location":"api/config/#ml_networks.config.LinearConfig.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation\n</code></pre>"},{"location":"api/config/#ml_networks.config.LinearConfig.bias","title":"bias  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bias = True\n</code></pre>"},{"location":"api/config/#ml_networks.config.LinearConfig.dropout","title":"dropout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dropout = 0.0\n</code></pre>"},{"location":"api/config/#ml_networks.config.LinearConfig.norm","title":"norm  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>norm = 'none'\n</code></pre>"},{"location":"api/config/#ml_networks.config.LinearConfig.norm_cfg","title":"norm_cfg  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>norm_cfg = field(default_factory=dict)\n</code></pre>"},{"location":"api/config/#ml_networks.config.LinearConfig.norm_first","title":"norm_first  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>norm_first = False\n</code></pre>"},{"location":"api/config/#ml_networks.config.LinearConfig-functions","title":"Functions","text":""},{"location":"api/config/#ml_networks.config.LinearConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Set <code>norm_cfg</code>.</p> Source code in <code>src/ml_networks/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Set `norm_cfg`.\"\"\"\n    if self.norm == \"none\":\n        self.norm_cfg = {}\n    else:\n        self.norm_cfg = dict(self.norm_cfg)\n</code></pre>"},{"location":"api/config/#ml_networks.config.LinearConfig.dictcfg2dict","title":"dictcfg2dict","text":"<pre><code>dictcfg2dict()\n</code></pre> <p>Convert dictConfig to dict for <code>LinearConfig</code>.</p> Source code in <code>src/ml_networks/config.py</code> <pre><code>def dictcfg2dict(self) -&gt; None:\n    \"\"\"Convert dictConfig to dict for `LinearConfig`.\"\"\"\n    self.norm_cfg = dict(self.norm_cfg)\n    for key, value in self.__dict__.items():\n        if isinstance(value, DictConfig | ListConfig | list | tuple | dict):\n            setattr(self, key, convert_dictconfig_to_dict(value))\n</code></pre>"},{"location":"api/config/#convconfig","title":"ConvConfig","text":""},{"location":"api/config/#ml_networks.config.ConvConfig","title":"ConvConfig  <code>dataclass</code>","text":"<pre><code>ConvConfig(activation, kernel_size, stride, padding, output_padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', dropout=0.0, norm='none', norm_cfg=dict(), norm_first=False, scale_factor=0)\n</code></pre> <p>A convolutional layer configuration.</p> <p>Attributes:</p> Name Type Description <code>activation</code> <code>str</code> <p>Activation function.</p> <code>kernel_size</code> <code>int</code> <p>Kernel size.</p> <code>stride</code> <code>int</code> <p>Stride.</p> <code>padding</code> <code>int</code> <p>Padding.</p> <code>output_padding</code> <code>int</code> <p>Output padding, especially for transposed convolution. Default is 0.</p> <code>dilation</code> <code>int</code> <p>Dilation.</p> <code>groups</code> <code>int</code> <p>Number of groups. Default is 1. See https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html.</p> <code>bias</code> <code>bool</code> <p>Whether to use bias. Default is True.</p> <code>dropout</code> <code>float</code> <p>Dropout rate. If it's set to 0.0, dropout is not applied. Default is 0.0.</p> <code>norm</code> <code>Literal['batch', 'group', 'none']</code> <p>Normalization layer. If it's set to \"none\", normalization is not applied. Default is \"none\".</p> <code>norm_cfg</code> <code>dict</code> <p>Normalization layer configuration. If you want to use Instance, Layer, or Group normalization, set norm to \"group\" and set norm_cfg with \"num_groups=$in_channel, 1, or any value\". Default is {}.</p> <code>scale_factor</code> <code>int</code> <p>Scale factor for upsample, especially for PixelShuffle or PixelUnshuffle. If it's set to &gt;0, upsample is applied. If it's set to &lt;0 downsample is applied. Otherwise, no upsample or downsample is applied. Default is 0.</p>"},{"location":"api/config/#ml_networks.config.ConvConfig-attributes","title":"Attributes","text":""},{"location":"api/config/#ml_networks.config.ConvConfig.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvConfig.bias","title":"bias  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bias = True\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvConfig.dilation","title":"dilation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dilation = 1\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvConfig.dropout","title":"dropout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dropout = 0.0\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvConfig.groups","title":"groups  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>groups = 1\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvConfig.kernel_size","title":"kernel_size  <code>instance-attribute</code>","text":"<pre><code>kernel_size\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvConfig.norm","title":"norm  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>norm = 'none'\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvConfig.norm_cfg","title":"norm_cfg  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>norm_cfg = field(default_factory=dict)\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvConfig.norm_first","title":"norm_first  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>norm_first = False\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvConfig.output_padding","title":"output_padding  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_padding = 0\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvConfig.padding","title":"padding  <code>instance-attribute</code>","text":"<pre><code>padding\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvConfig.padding_mode","title":"padding_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>padding_mode = 'zeros'\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvConfig.scale_factor","title":"scale_factor  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scale_factor = 0\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvConfig.stride","title":"stride  <code>instance-attribute</code>","text":"<pre><code>stride\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvConfig-functions","title":"Functions","text":""},{"location":"api/config/#ml_networks.config.ConvConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Set <code>.norm_cfg</code>.</p> Source code in <code>src/ml_networks/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Set `.norm_cfg`.\"\"\"\n    if self.norm == \"none\":\n        self.norm_cfg = {}\n    else:\n        self.norm_cfg = dict(self.norm_cfg)\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvConfig.dictcfg2dict","title":"dictcfg2dict","text":"<pre><code>dictcfg2dict()\n</code></pre> <p>Convert dictConfig to dict for <code>ConvConfig</code>.</p> Source code in <code>src/ml_networks/config.py</code> <pre><code>def dictcfg2dict(self) -&gt; None:\n    \"\"\"Convert dictConfig to dict for `ConvConfig`.\"\"\"\n    self.norm_cfg = dict(self.norm_cfg)\n\n    for key, value in self.__dict__.items():\n        if isinstance(value, DictConfig | ListConfig | list | tuple | dict):\n            setattr(self, key, convert_dictconfig_to_dict(value))\n</code></pre>"},{"location":"api/config/#convnetconfig","title":"ConvNetConfig","text":""},{"location":"api/config/#ml_networks.config.ConvNetConfig","title":"ConvNetConfig  <code>dataclass</code>","text":"<pre><code>ConvNetConfig(channels, conv_cfgs, attention=None, init_channel=16)\n</code></pre> <p>Convolutional neural network layers configuration.</p> <p>Attributes:</p> Name Type Description <code>channels</code> <code>Tuple[int, ...]</code> <p>Number of channels for each layer.</p> <code>conv_cfgs</code> <code>Tuple[ConvConfig, ...]</code> <p>Convolutional layer configurations. The length of conv_cfgs should be the same as the length of channels.</p> <code>init_channel</code> <code>int</code> <p>Initial number of channels, especially for transposed convolution.</p>"},{"location":"api/config/#ml_networks.config.ConvNetConfig-attributes","title":"Attributes","text":""},{"location":"api/config/#ml_networks.config.ConvNetConfig.attention","title":"attention  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>attention = None\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvNetConfig.channels","title":"channels  <code>instance-attribute</code>","text":"<pre><code>channels\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvNetConfig.conv_cfgs","title":"conv_cfgs  <code>instance-attribute</code>","text":"<pre><code>conv_cfgs\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvNetConfig.init_channel","title":"init_channel  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>init_channel = 16\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvNetConfig-functions","title":"Functions","text":""},{"location":"api/config/#ml_networks.config.ConvNetConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Set <code>channels</code> and <code>conv_cfgs</code> as tuple.</p> Source code in <code>src/ml_networks/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Set `channels` and `conv_cfgs` as tuple.\"\"\"\n    self.conv_cfgs = tuple(self.conv_cfgs)\n    self.channels = tuple(self.channels)\n</code></pre>"},{"location":"api/config/#ml_networks.config.ConvNetConfig.dictcfg2dict","title":"dictcfg2dict","text":"<pre><code>dictcfg2dict()\n</code></pre> <p>Convert dictConfig to dict for <code>ConvNetConfig</code>.</p> Source code in <code>src/ml_networks/config.py</code> <pre><code>def dictcfg2dict(self) -&gt; None:\n    \"\"\"Convert dictConfig to dict for `ConvNetConfig`.\"\"\"\n    self.channels = tuple(self.channels)\n    conv_cfgs = []\n    for cfg_item in self.conv_cfgs:\n        conv_cfg = cfg_item\n        if isinstance(conv_cfg, DictConfig):\n            conv_cfg_dict = convert_dictconfig_to_dict(conv_cfg)\n            conv_cfg = ConvConfig(**conv_cfg_dict)\n        conv_cfg.dictcfg2dict()\n        conv_cfgs.append(conv_cfg)\n    self.conv_cfgs = tuple(conv_cfgs)\n    for key, value in self.__dict__.items():\n        if isinstance(value, DictConfig | ListConfig | list | tuple | dict):\n            setattr(self, key, convert_dictconfig_to_dict(value))\n</code></pre>"},{"location":"api/config/#resnetconfig","title":"ResNetConfig","text":""},{"location":"api/config/#ml_networks.config.ResNetConfig","title":"ResNetConfig  <code>dataclass</code>","text":"<pre><code>ResNetConfig(conv_channel, conv_kernel, f_kernel, conv_activation, out_activation, n_res_blocks, scale_factor=2, n_scaling=2, norm='none', norm_cfg=dict(), dropout=0.0, init_channel=16, padding_mode='zeros', attention=None)\n</code></pre> <p>Residual neural network layers configuration.</p> <p>Attributes:</p> Name Type Description <code>conv_channel</code> <code>int</code> <p>Number of channels for convolutional layer. In ResNet, common number of channels is used for all layers.</p> <code>conv_kernel</code> <code>int</code> <p>Kernel size for convolutional layer. In ResNet, common kernel size is used for all layers.</p> <code>f_kernel</code> <code>int</code> <p>Kernel size for final or first convolutional layer. This depends on whether PixelShuffle or PixelUnshuffle is used.</p> <code>conv_activation</code> <code>str</code> <p>Activation function for convolutional layer.</p> <code>out_activation</code> <code>str</code> <p>Activation function for output layer.</p> <code>n_res_blocks</code> <code>int</code> <p>Number of residual blocks.</p> <code>scale_factor</code> <code>int</code> <p>Scale factor for upsample, especially for PixelShuffle or PixelUnshuffle.</p> <code>n_scaling</code> <code>int</code> <p>Number of upsample or downsample layers. The image size is scaled by scalefactor^n_scaling.</p> <code>norm</code> <code>Literal['batch', 'group', 'none']</code> <p>Normalization layer. If it's set to \"none\", normalization is not applied. Default is \"none\".</p> <code>norm_cfg</code> <code>dict</code> <p>Normalization layer configuration. If you want to use Instance, Layer, or Group normalization, set norm to \"group\" and set norm_cfg with \"num_groups=$in_channel, 1, or any value\". Default is {}.</p> <code>dropout</code> <code>float</code> <p>Dropout rate. If it's set to 0.0, dropout is not applied. Default is 0.0.</p> <code>init_channel</code> <code>int</code> <p>Initial number of channels, especially for decoder.</p>"},{"location":"api/config/#ml_networks.config.ResNetConfig-attributes","title":"Attributes","text":""},{"location":"api/config/#ml_networks.config.ResNetConfig.attention","title":"attention  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>attention = None\n</code></pre>"},{"location":"api/config/#ml_networks.config.ResNetConfig.conv_activation","title":"conv_activation  <code>instance-attribute</code>","text":"<pre><code>conv_activation\n</code></pre>"},{"location":"api/config/#ml_networks.config.ResNetConfig.conv_channel","title":"conv_channel  <code>instance-attribute</code>","text":"<pre><code>conv_channel\n</code></pre>"},{"location":"api/config/#ml_networks.config.ResNetConfig.conv_kernel","title":"conv_kernel  <code>instance-attribute</code>","text":"<pre><code>conv_kernel\n</code></pre>"},{"location":"api/config/#ml_networks.config.ResNetConfig.dropout","title":"dropout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dropout = 0.0\n</code></pre>"},{"location":"api/config/#ml_networks.config.ResNetConfig.f_kernel","title":"f_kernel  <code>instance-attribute</code>","text":"<pre><code>f_kernel\n</code></pre>"},{"location":"api/config/#ml_networks.config.ResNetConfig.init_channel","title":"init_channel  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>init_channel = 16\n</code></pre>"},{"location":"api/config/#ml_networks.config.ResNetConfig.n_res_blocks","title":"n_res_blocks  <code>instance-attribute</code>","text":"<pre><code>n_res_blocks\n</code></pre>"},{"location":"api/config/#ml_networks.config.ResNetConfig.n_scaling","title":"n_scaling  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_scaling = 2\n</code></pre>"},{"location":"api/config/#ml_networks.config.ResNetConfig.norm","title":"norm  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>norm = 'none'\n</code></pre>"},{"location":"api/config/#ml_networks.config.ResNetConfig.norm_cfg","title":"norm_cfg  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>norm_cfg = field(default_factory=dict)\n</code></pre>"},{"location":"api/config/#ml_networks.config.ResNetConfig.out_activation","title":"out_activation  <code>instance-attribute</code>","text":"<pre><code>out_activation\n</code></pre>"},{"location":"api/config/#ml_networks.config.ResNetConfig.padding_mode","title":"padding_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>padding_mode = 'zeros'\n</code></pre>"},{"location":"api/config/#ml_networks.config.ResNetConfig.scale_factor","title":"scale_factor  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scale_factor = 2\n</code></pre>"},{"location":"api/config/#ml_networks.config.ResNetConfig-functions","title":"Functions","text":""},{"location":"api/config/#ml_networks.config.ResNetConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Set <code>norm_cfg</code>.</p> Source code in <code>src/ml_networks/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Set `norm_cfg`.\"\"\"\n    if self.norm == \"none\":\n        self.norm_cfg = {}\n    else:\n        self.norm_cfg = dict(self.norm_cfg)\n</code></pre>"},{"location":"api/config/#ml_networks.config.ResNetConfig.dictcfg2dict","title":"dictcfg2dict","text":"<pre><code>dictcfg2dict()\n</code></pre> <p>Convert dictConfig to dict for <code>ResNetConfig</code>.</p> Source code in <code>src/ml_networks/config.py</code> <pre><code>def dictcfg2dict(self) -&gt; None:\n    \"\"\"Convert dictConfig to dict for `ResNetConfig`.\"\"\"\n    self.norm_cfg = dict(self.norm_cfg)\n    for key, value in self.__dict__.items():\n        if isinstance(value, DictConfig | ListConfig | list | tuple | dict):\n            setattr(self, key, convert_dictconfig_to_dict(value))\n</code></pre>"},{"location":"api/config/#encoderconfig","title":"EncoderConfig","text":""},{"location":"api/config/#ml_networks.config.EncoderConfig","title":"EncoderConfig  <code>dataclass</code>","text":"<pre><code>EncoderConfig(backbone, full_connection)\n</code></pre> <p>Encoder configuration.</p> <p>Attributes:</p> Name Type Description <code>backbone</code> <code>Union[ConvNetConfig, ResNetConfig]</code> <p>Backbone configuration.</p> <code>full_connection</code> <code>Union[MLPConfig, LinearConfig, SpatialSoftmaxConfig]</code> <p>Full connection configuration.</p>"},{"location":"api/config/#ml_networks.config.EncoderConfig-attributes","title":"Attributes","text":""},{"location":"api/config/#ml_networks.config.EncoderConfig.backbone","title":"backbone  <code>instance-attribute</code>","text":"<pre><code>backbone\n</code></pre>"},{"location":"api/config/#ml_networks.config.EncoderConfig.full_connection","title":"full_connection  <code>instance-attribute</code>","text":"<pre><code>full_connection\n</code></pre>"},{"location":"api/config/#ml_networks.config.EncoderConfig-functions","title":"Functions","text":""},{"location":"api/config/#ml_networks.config.EncoderConfig.dictcfg2dict","title":"dictcfg2dict","text":"<pre><code>dictcfg2dict()\n</code></pre> <p>Convert dictConfig to dict for <code>EncoderConfig</code>.</p> Source code in <code>src/ml_networks/config.py</code> <pre><code>def dictcfg2dict(self) -&gt; None:\n    \"\"\"Convert dictConfig to dict for `EncoderConfig`.\"\"\"\n    if hasattr(self.backbone, \"dictcfg2dict\"):\n        self.backbone.dictcfg2dict()\n    if hasattr(self.full_connection, \"dictcfg2dict\"):\n        self.full_connection.dictcfg2dict()\n    for key, value in self.__dict__.items():\n        if isinstance(value, DictConfig | ListConfig | list | tuple | dict):\n            setattr(self, key, convert_dictconfig_to_dict(value))\n</code></pre>"},{"location":"api/config/#decoderconfig","title":"DecoderConfig","text":""},{"location":"api/config/#ml_networks.config.DecoderConfig","title":"DecoderConfig  <code>dataclass</code>","text":"<pre><code>DecoderConfig(backbone, full_connection)\n</code></pre> <p>Decoder configuration.</p> <p>Attributes:</p> Name Type Description <code>backbone</code> <code>Union[ConvNetConfig, ResNetConfig]</code> <p>Backbone configuration.</p> <code>full_connection</code> <code>Union[MLPConfig, LinearConfig, SpatialSoftmaxConfig]</code> <p>Full connection configuration.</p>"},{"location":"api/config/#ml_networks.config.DecoderConfig-attributes","title":"Attributes","text":""},{"location":"api/config/#ml_networks.config.DecoderConfig.backbone","title":"backbone  <code>instance-attribute</code>","text":"<pre><code>backbone\n</code></pre>"},{"location":"api/config/#ml_networks.config.DecoderConfig.full_connection","title":"full_connection  <code>instance-attribute</code>","text":"<pre><code>full_connection\n</code></pre>"},{"location":"api/config/#ml_networks.config.DecoderConfig-functions","title":"Functions","text":""},{"location":"api/config/#ml_networks.config.DecoderConfig.dictcfg2dict","title":"dictcfg2dict","text":"<pre><code>dictcfg2dict()\n</code></pre> <p>Convert dictConfig to dict for <code>DecoderConfig</code>.</p> Source code in <code>src/ml_networks/config.py</code> <pre><code>def dictcfg2dict(self) -&gt; None:\n    \"\"\"Convert dictConfig to dict for `DecoderConfig`.\"\"\"\n    self.backbone.dictcfg2dict()\n    if hasattr(self.full_connection, \"dictcfg2dict\"):\n        self.full_connection.dictcfg2dict()\n    for key, value in self.__dict__.items():\n        if isinstance(value, DictConfig | ListConfig | list | tuple | dict):\n            setattr(self, key, convert_dictconfig_to_dict(value))\n</code></pre>"},{"location":"api/config/#vitconfig","title":"ViTConfig","text":""},{"location":"api/config/#ml_networks.config.ViTConfig","title":"ViTConfig  <code>dataclass</code>","text":"<pre><code>ViTConfig(patch_size, transformer_cfg, cls_token=True, init_channel=16, unpatchify=False)\n</code></pre> <p>Vision Transformer configuration.</p> <p>Attributes:</p> Name Type Description <code>patch_size</code> <code>int</code> <p>Patch size.</p> <code>transformer_cfg</code> <code>TransformerConfig</code> <p>Transformer configuration.</p> <code>cls_token</code> <code>bool</code> <p>Whether to use class token. Default is True.</p> <code>init_channel</code> <code>int</code> <p>Initial number of channels. Default is 16.</p>"},{"location":"api/config/#ml_networks.config.ViTConfig-attributes","title":"Attributes","text":""},{"location":"api/config/#ml_networks.config.ViTConfig.cls_token","title":"cls_token  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cls_token = True\n</code></pre>"},{"location":"api/config/#ml_networks.config.ViTConfig.init_channel","title":"init_channel  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>init_channel = 16\n</code></pre>"},{"location":"api/config/#ml_networks.config.ViTConfig.patch_size","title":"patch_size  <code>instance-attribute</code>","text":"<pre><code>patch_size\n</code></pre>"},{"location":"api/config/#ml_networks.config.ViTConfig.transformer_cfg","title":"transformer_cfg  <code>instance-attribute</code>","text":"<pre><code>transformer_cfg\n</code></pre>"},{"location":"api/config/#ml_networks.config.ViTConfig.unpatchify","title":"unpatchify  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>unpatchify = False\n</code></pre>"},{"location":"api/config/#ml_networks.config.ViTConfig-functions","title":"Functions","text":""},{"location":"api/config/#unetconfig","title":"UNetConfig","text":""},{"location":"api/config/#ml_networks.config.UNetConfig","title":"UNetConfig  <code>dataclass</code>","text":"<pre><code>UNetConfig(channels, conv_cfg, cond_pred_scale=False, nhead=None, has_attn=False, use_shuffle=False, use_hypernet=False, hyper_mlp_cfg=None)\n</code></pre> <p>UNet configuration.</p> <p>Attributes:</p> Name Type Description <code>channels</code> <code>Tuple[int, ...]</code> <p>Number of channels for each layer.</p> <code>conv_cfg</code> <code>ConvConfig</code> <p>Convolutional layer configuration.</p> <code>cond_cfg</code> <code>MLPConfig</code> <p>Conditional configuration for UNet.</p> <code>cond_pred_scale</code> <code>bool</code> <p>Whether to scale the conditional prediction. Default is False.</p> <code>nhead</code> <code>Optional[int]</code> <p>Number of heads for attention mechanism. If it's set to None, attention is not applied. Default is None.</p> <code>has_attn</code> <code>bool</code> <p>Whether to use attention mechanism. Default is False.</p> <code>use_shuffle</code> <code>bool</code> <p>Whether to use PixelShuffle or PixelUnshuffle. Default is False.</p> <code>use_hypernet</code> <code>bool</code> <p>Whether to use hypernetwork. Default is False.</p> <code>hyper_mlp_cfg</code> <code>Optional[MLPConfig]</code> <p>Hypernetwork configuration. If it's set to None, hypernetwork is not used. Default is None.</p>"},{"location":"api/config/#ml_networks.config.UNetConfig-attributes","title":"Attributes","text":""},{"location":"api/config/#ml_networks.config.UNetConfig.channels","title":"channels  <code>instance-attribute</code>","text":"<pre><code>channels\n</code></pre>"},{"location":"api/config/#ml_networks.config.UNetConfig.cond_pred_scale","title":"cond_pred_scale  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cond_pred_scale = False\n</code></pre>"},{"location":"api/config/#ml_networks.config.UNetConfig.conv_cfg","title":"conv_cfg  <code>instance-attribute</code>","text":"<pre><code>conv_cfg\n</code></pre>"},{"location":"api/config/#ml_networks.config.UNetConfig.has_attn","title":"has_attn  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>has_attn = False\n</code></pre>"},{"location":"api/config/#ml_networks.config.UNetConfig.hyper_mlp_cfg","title":"hyper_mlp_cfg  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>hyper_mlp_cfg = None\n</code></pre>"},{"location":"api/config/#ml_networks.config.UNetConfig.nhead","title":"nhead  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>nhead = None\n</code></pre>"},{"location":"api/config/#ml_networks.config.UNetConfig.use_hypernet","title":"use_hypernet  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_hypernet = False\n</code></pre>"},{"location":"api/config/#ml_networks.config.UNetConfig.use_shuffle","title":"use_shuffle  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_shuffle = False\n</code></pre>"},{"location":"api/config/#ml_networks.config.UNetConfig-functions","title":"Functions","text":""},{"location":"api/config/#ml_networks.config.UNetConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Post-initialization processing.</p> Source code in <code>src/ml_networks/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Post-initialization processing.\"\"\"\n    if self.has_attn:\n        assert self.nhead is not None, \"nhead must be specified when has_attn is True.\"\n    if isinstance(self.channels, list | ListConfig):\n        self.channels = tuple(self.channels)\n</code></pre>"},{"location":"api/config/#spatialsoftmaxconfig","title":"SpatialSoftmaxConfig","text":""},{"location":"api/config/#ml_networks.config.SpatialSoftmaxConfig","title":"SpatialSoftmaxConfig  <code>dataclass</code>","text":"<pre><code>SpatialSoftmaxConfig(temperature=1.0, eps=1e-06, is_argmax=False, is_straight_through=False, additional_layer=None)\n</code></pre> <p>Spatial softmax configuration.</p> <p>Attributes:</p> Name Type Description <code>temperature</code> <code>float</code> <p>Softmax temperature. If it's set to 0.0, the layer outputs the coordinates of the maximum value. Otherwise, the layer outputs the expectation of the coordinates with softmax function. Default is 0.0.</p> <code>eps</code> <code>float</code> <p>Epsilon value for numerical stability in softmax. Default is 1e-6.</p> <code>is_argmax</code> <code>bool</code> <p>Whether to use argmax instead of softmax. Default is False.</p> <code>is_straight_through</code> <code>bool</code> <p>Whether to use straight-through estimator for backpropagation. Default is False.</p> <code>additional_layer</code> <code>Optional[Union[MLPConfig, LinearConfig]]</code> <p>Additional layer configuration. If it's set to None, no additional layer is applied. Default is None.</p>"},{"location":"api/config/#ml_networks.config.SpatialSoftmaxConfig-attributes","title":"Attributes","text":""},{"location":"api/config/#ml_networks.config.SpatialSoftmaxConfig.additional_layer","title":"additional_layer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>additional_layer = None\n</code></pre>"},{"location":"api/config/#ml_networks.config.SpatialSoftmaxConfig.eps","title":"eps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>eps = 1e-06\n</code></pre>"},{"location":"api/config/#ml_networks.config.SpatialSoftmaxConfig.is_argmax","title":"is_argmax  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_argmax = False\n</code></pre>"},{"location":"api/config/#ml_networks.config.SpatialSoftmaxConfig.is_straight_through","title":"is_straight_through  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_straight_through = False\n</code></pre>"},{"location":"api/config/#ml_networks.config.SpatialSoftmaxConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature = 1.0\n</code></pre>"},{"location":"api/config/#ml_networks.config.SpatialSoftmaxConfig-functions","title":"Functions","text":""},{"location":"api/config/#attentionconfig","title":"AttentionConfig","text":""},{"location":"api/config/#ml_networks.config.AttentionConfig","title":"AttentionConfig  <code>dataclass</code>","text":"<pre><code>AttentionConfig(nhead, patch_size)\n</code></pre> <p>Attention configuration.</p> <p>Attributes:</p> Name Type Description <code>nhead</code> <code>int</code> <p>Number of heads.</p> <code>patch_size</code> <code>int</code> <p>Patch size.</p>"},{"location":"api/config/#ml_networks.config.AttentionConfig-attributes","title":"Attributes","text":""},{"location":"api/config/#ml_networks.config.AttentionConfig.nhead","title":"nhead  <code>instance-attribute</code>","text":"<pre><code>nhead\n</code></pre>"},{"location":"api/config/#ml_networks.config.AttentionConfig.patch_size","title":"patch_size  <code>instance-attribute</code>","text":"<pre><code>patch_size\n</code></pre>"},{"location":"api/config/#ml_networks.config.AttentionConfig-functions","title":"Functions","text":""},{"location":"api/config/#transformerconfig","title":"TransformerConfig","text":""},{"location":"api/config/#ml_networks.config.TransformerConfig","title":"TransformerConfig  <code>dataclass</code>","text":"<pre><code>TransformerConfig(d_model, nhead, dim_ff, n_layers, dropout=0.1, hidden_activation='GELU', output_activation='GeLU')\n</code></pre> <p>Transformer configuration.</p> <p>Attributes:</p> Name Type Description <code>d_model</code> <code>int</code> <p>Dimension of model.</p> <code>nhead</code> <code>int</code> <p>Number of heads.</p> <code>dim_ff</code> <code>int</code> <p>Dimension of feedforward network.</p> <code>n_layers</code> <code>int</code> <p>Number of layers.</p> <code>dropout</code> <code>float</code> <p>Dropout rate. Default is 0.1.</p> <code>hidden_activation</code> <code>Literal['ReLU', 'GELU']</code> <p>Activation function for hidden layer. Default is \"GELU\".</p> <code>output_activation</code> <code>str</code> <p>Activation function for output layer. Default is \"GeLU\".</p>"},{"location":"api/config/#ml_networks.config.TransformerConfig-attributes","title":"Attributes","text":""},{"location":"api/config/#ml_networks.config.TransformerConfig.d_model","title":"d_model  <code>instance-attribute</code>","text":"<pre><code>d_model\n</code></pre>"},{"location":"api/config/#ml_networks.config.TransformerConfig.dim_ff","title":"dim_ff  <code>instance-attribute</code>","text":"<pre><code>dim_ff\n</code></pre>"},{"location":"api/config/#ml_networks.config.TransformerConfig.dropout","title":"dropout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dropout = 0.1\n</code></pre>"},{"location":"api/config/#ml_networks.config.TransformerConfig.hidden_activation","title":"hidden_activation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>hidden_activation = 'GELU'\n</code></pre>"},{"location":"api/config/#ml_networks.config.TransformerConfig.n_layers","title":"n_layers  <code>instance-attribute</code>","text":"<pre><code>n_layers\n</code></pre>"},{"location":"api/config/#ml_networks.config.TransformerConfig.nhead","title":"nhead  <code>instance-attribute</code>","text":"<pre><code>nhead\n</code></pre>"},{"location":"api/config/#ml_networks.config.TransformerConfig.output_activation","title":"output_activation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_activation = 'GeLU'\n</code></pre>"},{"location":"api/config/#ml_networks.config.TransformerConfig-functions","title":"Functions","text":""},{"location":"api/config/#adaptiveaveragepoolingconfig","title":"AdaptiveAveragePoolingConfig","text":""},{"location":"api/config/#ml_networks.config.AdaptiveAveragePoolingConfig","title":"AdaptiveAveragePoolingConfig  <code>dataclass</code>","text":"<pre><code>AdaptiveAveragePoolingConfig(output_size=(1, 1), additional_layer=None)\n</code></pre> <p>Adaptive average pooling configuration.</p> <p>Attributes:</p> Name Type Description <code>output_size</code> <code>Union[int, Tuple[int, ...]]</code> <p>Output size of the pooling layer. If it's an integer, it will be used for both height and width. If it's a tuple, it should contain two integers for height and width.</p>"},{"location":"api/config/#ml_networks.config.AdaptiveAveragePoolingConfig-attributes","title":"Attributes","text":""},{"location":"api/config/#ml_networks.config.AdaptiveAveragePoolingConfig.additional_layer","title":"additional_layer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>additional_layer = None\n</code></pre>"},{"location":"api/config/#ml_networks.config.AdaptiveAveragePoolingConfig.output_size","title":"output_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_size = (1, 1)\n</code></pre>"},{"location":"api/config/#ml_networks.config.AdaptiveAveragePoolingConfig-functions","title":"Functions","text":""},{"location":"api/config/#ml_networks.config.AdaptiveAveragePoolingConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Ensure output_size is a tuple.</p> Source code in <code>src/ml_networks/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Ensure output_size is a tuple.\"\"\"\n    if isinstance(self.output_size, int):\n        self.output_size = (self.output_size, self.output_size)\n    elif isinstance(self.output_size, list | ListConfig):\n        self.output_size = tuple(self.output_size)\n</code></pre>"},{"location":"api/config/#softmaxtransconfig","title":"SoftmaxTransConfig","text":""},{"location":"api/config/#ml_networks.config.SoftmaxTransConfig","title":"SoftmaxTransConfig  <code>dataclass</code>","text":"<pre><code>SoftmaxTransConfig(vector, sigma, n_ignore=0, max=1.0, min=-1.0)\n</code></pre> <p>Softmax transformation configuration.</p> <p>Attributes:</p> Name Type Description <code>vector</code> <code>int</code> <p>Vector size.</p> <code>sigma</code> <code>float</code> <p>Sigma value.</p> <code>n_ignore</code> <code>int</code> <p>Number of ignored elements. Default is 0.</p> <code>max</code> <code>float</code> <p>Maximum value. Default is 1.0.</p> <code>min</code> <code>float</code> <p>Minimum value. Default is -1.0.</p>"},{"location":"api/config/#ml_networks.config.SoftmaxTransConfig-attributes","title":"Attributes","text":""},{"location":"api/config/#ml_networks.config.SoftmaxTransConfig.max","title":"max  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max = 1.0\n</code></pre>"},{"location":"api/config/#ml_networks.config.SoftmaxTransConfig.min","title":"min  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min = -1.0\n</code></pre>"},{"location":"api/config/#ml_networks.config.SoftmaxTransConfig.n_ignore","title":"n_ignore  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_ignore = 0\n</code></pre>"},{"location":"api/config/#ml_networks.config.SoftmaxTransConfig.sigma","title":"sigma  <code>instance-attribute</code>","text":"<pre><code>sigma\n</code></pre>"},{"location":"api/config/#ml_networks.config.SoftmaxTransConfig.vector","title":"vector  <code>instance-attribute</code>","text":"<pre><code>vector\n</code></pre>"},{"location":"api/config/#ml_networks.config.SoftmaxTransConfig-functions","title":"Functions","text":""},{"location":"api/config/#contrastivelearningconfig","title":"ContrastiveLearningConfig","text":""},{"location":"api/config/#ml_networks.config.ContrastiveLearningConfig","title":"ContrastiveLearningConfig  <code>dataclass</code>","text":"<pre><code>ContrastiveLearningConfig(dim_feature, eval_func, dim_input2=None, cross_entropy_like=False)\n</code></pre> <p>Contrastive learning configuration.</p>"},{"location":"api/config/#ml_networks.config.ContrastiveLearningConfig-attributes","title":"Attributes","text":""},{"location":"api/config/#ml_networks.config.ContrastiveLearningConfig.cross_entropy_like","title":"cross_entropy_like  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cross_entropy_like = False\n</code></pre>"},{"location":"api/config/#ml_networks.config.ContrastiveLearningConfig.dim_feature","title":"dim_feature  <code>instance-attribute</code>","text":"<pre><code>dim_feature\n</code></pre>"},{"location":"api/config/#ml_networks.config.ContrastiveLearningConfig.dim_input2","title":"dim_input2  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dim_input2 = None\n</code></pre>"},{"location":"api/config/#ml_networks.config.ContrastiveLearningConfig.eval_func","title":"eval_func  <code>instance-attribute</code>","text":"<pre><code>eval_func\n</code></pre>"},{"location":"api/config/#ml_networks.config.ContrastiveLearningConfig-functions","title":"Functions","text":""},{"location":"api/distributions/","title":"\u5206\u5e03","text":"<p>\u5206\u5e03\u95a2\u9023\u306e\u30af\u30e9\u30b9\u3068\u95a2\u6570\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p> <p><code>ml_networks.torch.distributions</code>\uff08PyTorch\uff09\u3068<code>ml_networks.jax.distributions</code>\uff08JAX\uff09\u306e\u4e21\u65b9\u3067\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"api/distributions/#distribution","title":"Distribution","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.Distribution","title":"Distribution","text":"<pre><code>Distribution(in_dim, dist, n_groups=1, spherical=False)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A distribution function.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input dimension.</p> required <code>dist</code> <code>Literal['normal', 'categorical', 'bernoulli']</code> <p>Distribution type.</p> required <code>n_groups</code> <code>int</code> <p>Number of groups. Default is 1. This is used for the categorical and Bernoulli distributions.</p> <code>1</code> <code>spherical</code> <code>bool</code> <p>Whether to project samples to the unit sphere. Default is False. This is used for the categorical and Bernoulli distributions. If True and dist==\"categorical\", the samples are projected from {0, 1} to {-1, 1}. If True and dist==\"bernoulli\", the samples are projected from {0, 1} to the unit sphere.</p> <p>refer to https://arxiv.org/abs/2406.07548</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dist = Distribution(10, \"normal\")\n&gt;&gt;&gt; data = torch.randn(2, 20)\n&gt;&gt;&gt; posterior = dist(data)\n&gt;&gt;&gt; posterior.__class__.__name__\n'NormalStoch'\n&gt;&gt;&gt; posterior.shape\nNormalShape(mean=torch.Size([2, 10]), std=torch.Size([2, 10]), stoch=torch.Size([2, 10]))\n</code></pre> <pre><code>&gt;&gt;&gt; dist = Distribution(10, \"categorical\", n_groups=2)\n&gt;&gt;&gt; data = torch.randn(2, 10)\n&gt;&gt;&gt; posterior = dist(data)\n&gt;&gt;&gt; posterior.__class__.__name__\n'CategoricalStoch'\n&gt;&gt;&gt; posterior.shape\nCategoricalShape(logits=torch.Size([2, 2, 5]), probs=torch.Size([2, 2, 5]), stoch=torch.Size([2, 10]))\n</code></pre> <pre><code>&gt;&gt;&gt; dist = Distribution(10, \"bernoulli\", n_groups=2)\n&gt;&gt;&gt; data = torch.randn(2, 10)\n&gt;&gt;&gt; posterior = dist(data)\n&gt;&gt;&gt; posterior.__class__.__name__\n'BernoulliStoch'\n&gt;&gt;&gt; posterior.shape\nBernoulliShape(logits=torch.Size([2, 2, 5]), probs=torch.Size([2, 2, 5]), stoch=torch.Size([2, 10]))\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __init__(\n    self,\n    in_dim: int,\n    dist: Literal[\"normal\", \"categorical\", \"bernoulli\"],\n    n_groups: int = 1,\n    spherical: bool = False,\n) -&gt; None:\n    super().__init__()\n\n    self.dist = dist\n    self.spherical = spherical\n    self.n_class = in_dim // n_groups\n    self.in_dim = in_dim\n    self.n_groups = n_groups\n\n    if dist == \"normal\":\n        self.posterior = self.normal  # type: ignore[assignment]\n    elif dist == \"categorical\":\n        self.posterior = self.categorical  # type: ignore[assignment]\n    elif dist == \"bernoulli\":\n        self.posterior = self.bernoulli  # type: ignore[assignment]\n    else:\n        raise NotImplementedError\n\n    if spherical:\n        self.codebook = BSQCodebook(self.n_class)\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.Distribution-attributes","title":"Attributes","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.Distribution.codebook","title":"codebook  <code>instance-attribute</code>","text":"<pre><code>codebook = BSQCodebook(n_class)\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.Distribution.dist","title":"dist  <code>instance-attribute</code>","text":"<pre><code>dist = dist\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.Distribution.in_dim","title":"in_dim  <code>instance-attribute</code>","text":"<pre><code>in_dim = in_dim\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.Distribution.n_class","title":"n_class  <code>instance-attribute</code>","text":"<pre><code>n_class = in_dim // n_groups\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.Distribution.n_groups","title":"n_groups  <code>instance-attribute</code>","text":"<pre><code>n_groups = n_groups\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.Distribution.posterior","title":"posterior  <code>instance-attribute</code>","text":"<pre><code>posterior = normal\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.Distribution.spherical","title":"spherical  <code>instance-attribute</code>","text":"<pre><code>spherical = spherical\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.Distribution-functions","title":"Functions","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.Distribution.bernoulli","title":"bernoulli","text":"<pre><code>bernoulli(logits, deterministic=False, inv_tmp=1.0)\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def bernoulli(self, logits: torch.Tensor, deterministic: bool = False, inv_tmp: float = 1.0) -&gt; BernoulliStoch:\n    batch_shape = logits.shape[:-1]\n    chunked_logits = torch.chunk(logits, self.n_groups, dim=-1)\n    logits = torch.stack(chunked_logits, dim=-2)\n    logits = logits * inv_tmp\n    probs = torch.sigmoid(logits)\n\n    dist = BernoulliStraightThrough(probs=probs)\n    posterior_dist = D.Independent(dist, 1)\n\n    sample = posterior_dist.rsample()\n\n    if self.spherical:\n        sample = self.codebook.bits_to_codes(sample)\n\n    if deterministic:\n        sample = (\n            torch.where(sample &gt; 0.5, torch.ones_like(sample), torch.zeros_like(sample)) + probs - probs.detach()\n        )\n\n    return BernoulliStoch(\n        logits,\n        probs,\n        sample.reshape([*batch_shape, -1]),\n    )\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.Distribution.categorical","title":"categorical","text":"<pre><code>categorical(logits, deterministic=False, inv_tmp=1.0)\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def categorical(self, logits: torch.Tensor, deterministic: bool = False, inv_tmp: float = 1.0) -&gt; CategoricalStoch:\n    batch_shape = logits.shape[:-1]\n    logits_chunk = torch.chunk(logits, self.n_groups, dim=-1)\n    logits = torch.stack(logits_chunk, dim=-2)\n    logits = logits\n    probs = softmax(logits, dim=-1, temperature=1 / inv_tmp)\n    dist = D.OneHotCategoricalStraightThrough(probs=probs)\n    posterior_dist = D.Independent(dist, 1)\n\n    sample = posterior_dist.rsample()\n\n    if self.spherical:\n        sample = sample * 2 - 1\n\n    return CategoricalStoch(\n        logits,\n        probs,\n        sample.reshape([*batch_shape, -1])\n        if not deterministic\n        else self.deterministic_onehot(probs).reshape([*batch_shape, -1]),\n    )\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.Distribution.deterministic_onehot","title":"deterministic_onehot","text":"<pre><code>deterministic_onehot(input)\n</code></pre> <p>Compute the one-hot vector by argmax.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>One-hot vector.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; input = torch.arange(6).reshape(2, 3) / 5.0\n&gt;&gt;&gt; dist = Distribution(3, \"categorical\")\n&gt;&gt;&gt; onehot = dist.deterministic_onehot(input)\n&gt;&gt;&gt; onehot\ntensor([[0., 0., 1.],\n        [0., 0., 1.]])\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def deterministic_onehot(self, input: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the one-hot vector by argmax.\n\n    Parameters\n    ----------\n    input : torch.Tensor\n        Input tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        One-hot vector.\n\n    Examples\n    --------\n    &gt;&gt;&gt; input = torch.arange(6).reshape(2, 3) / 5.0\n    &gt;&gt;&gt; dist = Distribution(3, \"categorical\")\n    &gt;&gt;&gt; onehot = dist.deterministic_onehot(input)\n    &gt;&gt;&gt; onehot\n    tensor([[0., 0., 1.],\n            [0., 0., 1.]])\n    \"\"\"\n    return F.one_hot(input.argmax(dim=-1), num_classes=self.n_class) + input - input.detach()\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.Distribution.forward","title":"forward","text":"<pre><code>forward(x, deterministic=False, inv_tmp=1.0)\n</code></pre> <p>Compute the posterior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>deterministic</code> <code>bool</code> <p>Whether to use the deterministic mode. Default is False. if True and dist==\"normal\", the mean is returned. if True and dist==\"categorical\", the one-hot vector computed by argmax is returned. if True and dist==\"bernoulli\", 1 is returned if x &gt; 0.5 or 0 is returned if x &lt;= 0.5.</p> <code>False</code> <code>inv_tmp</code> <code>float</code> <p>Inverse temperature. Default is 1.0. This is used for the categorical and Bernoulli distributions.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>StochState</code> <p>Posterior distribution.</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    deterministic: bool = False,\n    inv_tmp: float = 1.0,\n) -&gt; StochState:\n    \"\"\"\n    Compute the posterior distribution.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n    deterministic : bool, optional\n        Whether to use the deterministic mode. Default is False.\n        if True and dist==\"normal\", the mean is returned.\n        if True and dist==\"categorical\", the one-hot vector computed by argmax is returned.\n        if True and dist==\"bernoulli\", 1 is returned if x &gt; 0.5 or 0 is returned if x &lt;= 0.5.\n\n    inv_tmp : float, optional\n        Inverse temperature. Default is 1.0.\n        This is used for the categorical and Bernoulli distributions.\n\n    Returns\n    -------\n    StochState\n        Posterior distribution.\n\n\n    \"\"\"\n    return self.posterior(x, deterministic=deterministic, inv_tmp=inv_tmp)\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.Distribution.normal","title":"normal","text":"<pre><code>normal(mu_std, deterministic=False, inv_tmp=1.0)\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def normal(self, mu_std: torch.Tensor, deterministic: bool = False, inv_tmp: float = 1.0) -&gt; NormalStoch:\n    assert mu_std.shape[-1] == self.in_dim * 2, (\n        f\"mu_std.shape[-1] {mu_std.shape[-1]} and in_dim {self.in_dim} must be the same.\"\n    )\n\n    mu, std = torch.chunk(mu_std, 2, dim=-1)\n    std = F.softplus(std) + 1e-6\n\n    normal_dist = D.Normal(mu, std)\n    posterior_dist = D.Independent(normal_dist, 1)\n\n    sample = posterior_dist.rsample() if not deterministic else mu\n\n    return NormalStoch(mu, std, sample if not deterministic else mu)\n</code></pre>"},{"location":"api/distributions/#normalstoch","title":"NormalStoch","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.NormalStoch","title":"NormalStoch  <code>dataclass</code>","text":"<pre><code>NormalStoch(mean, std, stoch)\n</code></pre> <p>Parameters of a normal distribution and its stochastic sample.</p> <p>Attributes:</p> Name Type Description <code>mean</code> <code>Tensor</code> <p>Mean of the normal distribution.</p> <code>std</code> <code>Tensor</code> <p>Standard deviation of the normal distribution.</p> <code>stoch</code> <code>Tensor</code> <p>sample from the normal distribution with reparametrization trick.</p>"},{"location":"api/distributions/#ml_networks.torch.distributions.NormalStoch-attributes","title":"Attributes","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.NormalStoch.mean","title":"mean  <code>instance-attribute</code>","text":"<pre><code>mean\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.NormalStoch.shape","title":"shape  <code>property</code>","text":"<pre><code>shape\n</code></pre> <p>mean, std, stoch \u306e shape \u3092\u30bf\u30d7\u30eb\u3067\u8fd4\u3059.</p>"},{"location":"api/distributions/#ml_networks.torch.distributions.NormalStoch.std","title":"std  <code>instance-attribute</code>","text":"<pre><code>std\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.NormalStoch.stoch","title":"stoch  <code>instance-attribute</code>","text":"<pre><code>stoch\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.NormalStoch-functions","title":"Functions","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.NormalStoch.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name)\n</code></pre> <p>torch.Tensor \u306b\u542b\u307e\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3092\u547c\u3073\u51fa\u3057\u305f\u3089\u3001\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b.</p> <p>\u4f8b: normal.flatten() \u2192 NormalStoch(mean.flatten(), std.flatten(), stoch.flatten()).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>\u30e1\u30bd\u30c3\u30c9\u540d\u3002</p> required <p>Returns:</p> Type Description <code>callable</code> <p>torch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3092\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b\u95a2\u6570\u3002</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>\u6307\u5b9a\u3055\u308c\u305f\u540d\u524d\u304ctorch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3067\u306a\u3044\u5834\u5408\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"torch.Tensor \u306b\u542b\u307e\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3092\u547c\u3073\u51fa\u3057\u305f\u3089\u3001\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b.\n\n    \u4f8b: normal.flatten() \u2192 NormalStoch(mean.flatten(), std.flatten(), stoch.flatten()).\n\n    Parameters\n    ----------\n    name : str\n        \u30e1\u30bd\u30c3\u30c9\u540d\u3002\n\n    Returns\n    -------\n    callable\n        torch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3092\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b\u95a2\u6570\u3002\n\n    Raises\n    ------\n    AttributeError\n        \u6307\u5b9a\u3055\u308c\u305f\u540d\u524d\u304ctorch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3067\u306a\u3044\u5834\u5408\u3002\n    \"\"\"\n    if hasattr(torch.Tensor, name):  # torch.Tensor \u306e\u30e1\u30bd\u30c3\u30c9\u304b\u78ba\u8a8d\n\n        def method(*args: Any, **kwargs: Any) -&gt; NormalStoch:\n            return NormalStoch(\n                getattr(self.mean, name)(*args, **kwargs),\n                getattr(self.std, name)(*args, **kwargs),\n                getattr(self.stoch, name)(*args, **kwargs),\n            )\n\n        return method\n    msg = f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    raise AttributeError(msg)\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.NormalStoch.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u30a2\u30af\u30bb\u30b9.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int or slice or tuple</code> <p>\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u6307\u5b9a\u3002</p> required <p>Returns:</p> Type Description <code>NormalStoch</code> <p>\u6307\u5b9a\u3055\u308c\u305f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5bfe\u5fdc\u3059\u308b<code>NormalStoch</code>\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __getitem__(self, idx: int | slice | tuple) -&gt; NormalStoch:\n    \"\"\"\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u30a2\u30af\u30bb\u30b9.\n\n    Parameters\n    ----------\n    idx : int or slice or tuple\n        \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u6307\u5b9a\u3002\n\n    Returns\n    -------\n    NormalStoch\n        \u6307\u5b9a\u3055\u308c\u305f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5bfe\u5fdc\u3059\u308b`NormalStoch`\u3002\n    \"\"\"\n    return NormalStoch(self.mean[idx], self.std[idx], self.stoch[idx])\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.NormalStoch.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>\u9577\u3055\u3092\u8fd4\u3059.</p> <p>Returns:</p> Type Description <code>int</code> <p>\u30d0\u30c3\u30c1\u6b21\u5143\u306e\u9577\u3055\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\u9577\u3055\u3092\u8fd4\u3059.\n\n    Returns\n    -------\n    int\n        \u30d0\u30c3\u30c1\u6b21\u5143\u306e\u9577\u3055\u3002\n    \"\"\"\n    return self.stoch.shape[0]\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.NormalStoch.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>\u521d\u671f\u5316\u5f8c\u306e\u51e6\u7406.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p><code>mean</code> \u3068 <code>std</code> \u306eshape\u304c\u7570\u306a\u308b\u5834\u5408\u3001\u307e\u305f\u306f<code>std</code>\u306b\u8ca0\u306e\u5024\u304c\u542b\u307e\u308c\u308b\u5834\u5408\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"\u521d\u671f\u5316\u5f8c\u306e\u51e6\u7406.\n\n    Raises\n    ------\n    ValueError\n        `mean` \u3068 `std` \u306eshape\u304c\u7570\u306a\u308b\u5834\u5408\u3001\u307e\u305f\u306f`std`\u306b\u8ca0\u306e\u5024\u304c\u542b\u307e\u308c\u308b\u5834\u5408\u3002\n    \"\"\"\n    if self.mean.shape != self.std.shape:\n        msg = f\"mean.shape {self.mean.shape} and std.shape {self.std.shape} must be the same.\"\n        raise ValueError(msg)\n    if (self.std &lt; 0).any():\n        msg = \"std must be non-negative.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.NormalStoch.get_distribution","title":"get_distribution","text":"<pre><code>get_distribution(independent=1)\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def get_distribution(self, independent: int = 1) -&gt; D.Independent:\n    return D.Independent(D.Normal(self.mean, self.std), independent)\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.NormalStoch.save","title":"save","text":"<pre><code>save(path)\n</code></pre> <p>Save the parameters of the normal distribution to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the parameters.</p> required Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"\n    Save the parameters of the normal distribution to the specified path.\n\n    Parameters\n    ----------\n    path : str\n        Path to save the parameters.\n\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n\n    save_blosc2(f\"{path}/mean.blosc2\", self.mean.detach().clone().cpu().numpy())\n    save_blosc2(f\"{path}/std.blosc2\", self.std.detach().clone().cpu().numpy())\n    save_blosc2(f\"{path}/stoch.blosc2\", self.stoch.detach().clone().cpu().numpy())\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.NormalStoch.squeeze","title":"squeeze","text":"<pre><code>squeeze(dim)\n</code></pre> <p>Squeeze the parameters of the normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension to squeeze.</p> required <p>Returns:</p> Type Description <code>NormalStoch</code> <p>Squeezed normal distribution.</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def squeeze(self, dim: int) -&gt; NormalStoch:\n    \"\"\"\n    Squeeze the parameters of the normal distribution.\n\n    Parameters\n    ----------\n    dim : int\n        Dimension to squeeze.\n\n    Returns\n    -------\n    NormalStoch\n        Squeezed normal distribution.\n\n    \"\"\"\n    return NormalStoch(\n        self.mean.squeeze(dim),\n        self.std.squeeze(dim),\n        self.stoch.squeeze(dim),\n    )\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.NormalStoch.unsqueeze","title":"unsqueeze","text":"<pre><code>unsqueeze(dim)\n</code></pre> <p>Unsqueeze the parameters of the normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension to unsqueeze.</p> required <p>Returns:</p> Type Description <code>NormalStoch</code> <p>Unsqueezed normal distribution.</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def unsqueeze(self, dim: int) -&gt; NormalStoch:\n    \"\"\"\n    Unsqueeze the parameters of the normal distribution.\n\n    Parameters\n    ----------\n    dim : int\n        Dimension to unsqueeze.\n\n    Returns\n    -------\n    NormalStoch\n        Unsqueezed normal distribution.\n\n    \"\"\"\n    return NormalStoch(\n        self.mean.unsqueeze(dim),\n        self.std.unsqueeze(dim),\n        self.stoch.unsqueeze(dim),\n    )\n</code></pre>"},{"location":"api/distributions/#categoricalstoch","title":"CategoricalStoch","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.CategoricalStoch","title":"CategoricalStoch  <code>dataclass</code>","text":"<pre><code>CategoricalStoch(logits, probs, stoch)\n</code></pre> <p>Parameters of a categorical distribution and its stochastic sample.</p> <p>Attributes:</p> Name Type Description <code>logits</code> <code>Tensor</code> <p>Logits of the categorical distribution.</p> <code>probs</code> <code>Tensor</code> <p>Probabilities of the categorical distribution.</p> <code>stoch</code> <code>Tensor</code> <p>sample from the categorical distribution with Straight-Through Estimator.</p>"},{"location":"api/distributions/#ml_networks.torch.distributions.CategoricalStoch-attributes","title":"Attributes","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.CategoricalStoch.logits","title":"logits  <code>instance-attribute</code>","text":"<pre><code>logits\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.CategoricalStoch.probs","title":"probs  <code>instance-attribute</code>","text":"<pre><code>probs\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.CategoricalStoch.shape","title":"shape  <code>property</code>","text":"<pre><code>shape\n</code></pre> <p>mean, std, stoch \u306e shape \u3092\u30bf\u30d7\u30eb\u3067\u8fd4\u3059.</p>"},{"location":"api/distributions/#ml_networks.torch.distributions.CategoricalStoch.stoch","title":"stoch  <code>instance-attribute</code>","text":"<pre><code>stoch\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.CategoricalStoch-functions","title":"Functions","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.CategoricalStoch.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name)\n</code></pre> <p>torch.Tensor \u306b\u542b\u307e\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3092\u547c\u3073\u51fa\u3057\u305f\u3089\u3001\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b.</p> <p>\u4f8b: normal.flatten() \u2192 NormalStoch(mean.flatten(), std.flatten(), stoch.flatten()).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>\u30e1\u30bd\u30c3\u30c9\u540d\u3002</p> required <p>Returns:</p> Type Description <code>callable</code> <p>torch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3092\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b\u95a2\u6570\u3002</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>\u6307\u5b9a\u3055\u308c\u305f\u540d\u524d\u304ctorch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3067\u306a\u3044\u5834\u5408\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"torch.Tensor \u306b\u542b\u307e\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3092\u547c\u3073\u51fa\u3057\u305f\u3089\u3001\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b.\n\n    \u4f8b: normal.flatten() \u2192 NormalStoch(mean.flatten(), std.flatten(), stoch.flatten()).\n\n    Parameters\n    ----------\n    name : str\n        \u30e1\u30bd\u30c3\u30c9\u540d\u3002\n\n    Returns\n    -------\n    callable\n        torch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3092\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b\u95a2\u6570\u3002\n\n    Raises\n    ------\n    AttributeError\n        \u6307\u5b9a\u3055\u308c\u305f\u540d\u524d\u304ctorch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3067\u306a\u3044\u5834\u5408\u3002\n    \"\"\"\n    if hasattr(torch.Tensor, name):  # torch.Tensor \u306e\u30e1\u30bd\u30c3\u30c9\u304b\u78ba\u8a8d\n\n        def method(*args: Any, **kwargs: Any) -&gt; CategoricalStoch:\n            return CategoricalStoch(\n                getattr(self.logits, name)(*args, **kwargs),\n                getattr(self.probs, name)(*args, **kwargs),\n                getattr(self.stoch, name)(*args, **kwargs),\n            )\n\n        return method\n    msg = f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    raise AttributeError(msg)\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.CategoricalStoch.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u30a2\u30af\u30bb\u30b9.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int or slice or tuple</code> <p>\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u6307\u5b9a\u3002</p> required <p>Returns:</p> Type Description <code>CategoricalStoch</code> <p>\u6307\u5b9a\u3055\u308c\u305f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5bfe\u5fdc\u3059\u308b<code>CategoricalStoch</code>\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __getitem__(self, idx: int | slice | tuple) -&gt; CategoricalStoch:\n    \"\"\"\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u30a2\u30af\u30bb\u30b9.\n\n    Parameters\n    ----------\n    idx : int or slice or tuple\n        \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u6307\u5b9a\u3002\n\n    Returns\n    -------\n    CategoricalStoch\n        \u6307\u5b9a\u3055\u308c\u305f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5bfe\u5fdc\u3059\u308b`CategoricalStoch`\u3002\n    \"\"\"\n    return CategoricalStoch(self.logits[idx], self.probs[idx], self.stoch[idx])\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.CategoricalStoch.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>\u9577\u3055\u3092\u8fd4\u3059.</p> <p>Returns:</p> Type Description <code>int</code> <p>\u30d0\u30c3\u30c1\u6b21\u5143\u306e\u9577\u3055\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\u9577\u3055\u3092\u8fd4\u3059.\n\n    Returns\n    -------\n    int\n        \u30d0\u30c3\u30c1\u6b21\u5143\u306e\u9577\u3055\u3002\n    \"\"\"\n    return self.stoch.shape[0]\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.CategoricalStoch.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>\u521d\u671f\u5316\u5f8c\u306e\u51e6\u7406.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p><code>logits</code> \u3068 <code>probs</code> \u306eshape\u304c\u7570\u306a\u308b\u5834\u5408\u3001 \u3042\u308b\u3044\u306f<code>probs</code>\u304c[0, 1]\u306e\u7bc4\u56f2\u5916\u3001\u307e\u305f\u306f\u548c\u304c1\u304b\u3089\u5927\u304d\u304f\u305a\u308c\u3066\u3044\u308b\u5834\u5408\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"\u521d\u671f\u5316\u5f8c\u306e\u51e6\u7406.\n\n    Raises\n    ------\n    ValueError\n        `logits` \u3068 `probs` \u306eshape\u304c\u7570\u306a\u308b\u5834\u5408\u3001\n        \u3042\u308b\u3044\u306f`probs`\u304c[0, 1]\u306e\u7bc4\u56f2\u5916\u3001\u307e\u305f\u306f\u548c\u304c1\u304b\u3089\u5927\u304d\u304f\u305a\u308c\u3066\u3044\u308b\u5834\u5408\u3002\n    \"\"\"\n    if self.logits.shape != self.probs.shape:\n        msg = f\"logits.shape {self.logits.shape} and probs.shape {self.probs.shape} must be the same.\"\n        raise ValueError(msg)\n    if (self.probs &lt; 0).any() or (self.probs &gt; 1).any():\n        msg = \"probs must be in the range [0, 1].\"\n        raise ValueError(msg)\n    if (self.probs.sum(dim=-1) - 1).abs().max() &gt; 1e-6:\n        msg = \"probs must sum to 1.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.CategoricalStoch.get_distribution","title":"get_distribution","text":"<pre><code>get_distribution(independent=1)\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def get_distribution(self, independent: int = 1) -&gt; D.Independent:\n    return D.Independent(D.OneHotCategoricalStraightThrough(self.probs), independent)\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.CategoricalStoch.save","title":"save","text":"<pre><code>save(path)\n</code></pre> <p>Save the parameters of the categorical distribution to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the parameters.</p> required Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"\n    Save the parameters of the categorical distribution to the specified path.\n\n    Parameters\n    ----------\n    path : str\n        Path to save the parameters.\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n\n    save_blosc2(f\"{path}/logits.blosc2\", self.logits.detach().clone().cpu().numpy())\n    save_blosc2(f\"{path}/probs.blosc2\", self.probs.detach().clone().cpu().numpy())\n    save_blosc2(f\"{path}/stoch.blosc2\", self.stoch.detach().clone().cpu().numpy())\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.CategoricalStoch.squeeze","title":"squeeze","text":"<pre><code>squeeze(dim)\n</code></pre> <p>Squeeze the parameters of the categorical distribution.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension to squeeze.</p> required <p>Returns:</p> Type Description <code>CategoricalStoch</code> <p>Squeezed categorical distribution.</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def squeeze(self, dim: int) -&gt; CategoricalStoch:\n    \"\"\"\n    Squeeze the parameters of the categorical distribution.\n\n    Parameters\n    ----------\n    dim : int\n        Dimension to squeeze.\n\n    Returns\n    -------\n    CategoricalStoch\n        Squeezed categorical distribution.\n\n    \"\"\"\n    return CategoricalStoch(\n        self.logits.squeeze(dim),\n        self.probs.squeeze(dim),\n        self.stoch.squeeze(dim),\n    )\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.CategoricalStoch.unsqueeze","title":"unsqueeze","text":"<pre><code>unsqueeze(dim)\n</code></pre> <p>Unsqueeze the parameters of the categorical distribution.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension to unsqueeze.</p> required <p>Returns:</p> Type Description <code>CategoricalStoch</code> <p>Unsqueezed categorical distribution.</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def unsqueeze(self, dim: int) -&gt; CategoricalStoch:\n    \"\"\"\n    Unsqueeze the parameters of the categorical distribution.\n\n    Parameters\n    ----------\n    dim : int\n        Dimension to unsqueeze.\n\n    Returns\n    -------\n    CategoricalStoch\n        Unsqueezed categorical distribution.\n\n    \"\"\"\n    return CategoricalStoch(\n        self.logits.unsqueeze(dim),\n        self.probs.unsqueeze(dim),\n        self.stoch.unsqueeze(dim),\n    )\n</code></pre>"},{"location":"api/distributions/#bernoullistoch","title":"BernoulliStoch","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.BernoulliStoch","title":"BernoulliStoch  <code>dataclass</code>","text":"<pre><code>BernoulliStoch(logits, probs, stoch)\n</code></pre> <p>Parameters of a Bernoulli distribution and its stochastic sample.</p> <p>Attributes:</p> Name Type Description <code>logits</code> <code>Tensor</code> <p>Logits of the Bernoulli distribution.</p> <code>probs</code> <code>Tensor</code> <p>Probabilities of the Bernoulli distribution.</p> <code>stoch</code> <code>Tensor</code> <p>sample from the Bernoulli distribution with Straight-Through Estimator.</p>"},{"location":"api/distributions/#ml_networks.torch.distributions.BernoulliStoch-attributes","title":"Attributes","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.BernoulliStoch.logits","title":"logits  <code>instance-attribute</code>","text":"<pre><code>logits\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.BernoulliStoch.probs","title":"probs  <code>instance-attribute</code>","text":"<pre><code>probs\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.BernoulliStoch.shape","title":"shape  <code>property</code>","text":"<pre><code>shape\n</code></pre> <p>mean, std, stoch \u306e shape \u3092\u30bf\u30d7\u30eb\u3067\u8fd4\u3059.</p>"},{"location":"api/distributions/#ml_networks.torch.distributions.BernoulliStoch.stoch","title":"stoch  <code>instance-attribute</code>","text":"<pre><code>stoch\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.BernoulliStoch-functions","title":"Functions","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.BernoulliStoch.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name)\n</code></pre> <p>torch.Tensor \u306b\u542b\u307e\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3092\u547c\u3073\u51fa\u3057\u305f\u3089\u3001\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b.</p> <p>\u4f8b: normal.flatten() \u2192 NormalStoch(mean.flatten(), std.flatten(), stoch.flatten()).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>\u30e1\u30bd\u30c3\u30c9\u540d\u3002</p> required <p>Returns:</p> Type Description <code>callable</code> <p>torch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3092\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b\u95a2\u6570\u3002</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>\u6307\u5b9a\u3055\u308c\u305f\u540d\u524d\u304ctorch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3067\u306a\u3044\u5834\u5408\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"torch.Tensor \u306b\u542b\u307e\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3092\u547c\u3073\u51fa\u3057\u305f\u3089\u3001\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b.\n\n    \u4f8b: normal.flatten() \u2192 NormalStoch(mean.flatten(), std.flatten(), stoch.flatten()).\n\n    Parameters\n    ----------\n    name : str\n        \u30e1\u30bd\u30c3\u30c9\u540d\u3002\n\n    Returns\n    -------\n    callable\n        torch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3092\u5404\u30e1\u30f3\u30d0\u306b\u9069\u7528\u3059\u308b\u95a2\u6570\u3002\n\n    Raises\n    ------\n    AttributeError\n        \u6307\u5b9a\u3055\u308c\u305f\u540d\u524d\u304ctorch.Tensor\u306e\u30e1\u30bd\u30c3\u30c9\u3067\u306a\u3044\u5834\u5408\u3002\n    \"\"\"\n    if hasattr(torch.Tensor, name):  # torch.Tensor \u306e\u30e1\u30bd\u30c3\u30c9\u304b\u78ba\u8a8d\n\n        def method(*args: Any, **kwargs: Any) -&gt; CategoricalStoch:\n            return CategoricalStoch(\n                getattr(self.logits, name)(*args, **kwargs),\n                getattr(self.probs, name)(*args, **kwargs),\n                getattr(self.stoch, name)(*args, **kwargs),\n            )\n\n        return method\n    msg = f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n    raise AttributeError(msg)\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.BernoulliStoch.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u30a2\u30af\u30bb\u30b9.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int or slice or tuple</code> <p>\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u6307\u5b9a\u3002</p> required <p>Returns:</p> Type Description <code>CategoricalStoch</code> <p>\u6307\u5b9a\u3055\u308c\u305f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5bfe\u5fdc\u3059\u308b<code>CategoricalStoch</code>\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __getitem__(self, idx: int | slice | tuple) -&gt; CategoricalStoch:\n    \"\"\"\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u30a2\u30af\u30bb\u30b9.\n\n    Parameters\n    ----------\n    idx : int or slice or tuple\n        \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u6307\u5b9a\u3002\n\n    Returns\n    -------\n    CategoricalStoch\n        \u6307\u5b9a\u3055\u308c\u305f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5bfe\u5fdc\u3059\u308b`CategoricalStoch`\u3002\n    \"\"\"\n    return CategoricalStoch(self.logits[idx], self.probs[idx], self.stoch[idx])\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.BernoulliStoch.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>\u9577\u3055\u3092\u8fd4\u3059.</p> <p>Returns:</p> Type Description <code>int</code> <p>\u30d0\u30c3\u30c1\u6b21\u5143\u306e\u9577\u3055\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\u9577\u3055\u3092\u8fd4\u3059.\n\n    Returns\n    -------\n    int\n        \u30d0\u30c3\u30c1\u6b21\u5143\u306e\u9577\u3055\u3002\n    \"\"\"\n    return self.stoch.shape[0]\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.BernoulliStoch.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>\u521d\u671f\u5316\u5f8c\u306e\u51e6\u7406.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p><code>logits</code> \u3068 <code>probs</code> \u306eshape\u304c\u7570\u306a\u308b\u5834\u5408\u3001\u3042\u308b\u3044\u306f<code>probs</code>\u304c[0, 1]\u306e\u7bc4\u56f2\u5916\u306e\u5834\u5408\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"\u521d\u671f\u5316\u5f8c\u306e\u51e6\u7406.\n\n    Raises\n    ------\n    ValueError\n        `logits` \u3068 `probs` \u306eshape\u304c\u7570\u306a\u308b\u5834\u5408\u3001\u3042\u308b\u3044\u306f`probs`\u304c[0, 1]\u306e\u7bc4\u56f2\u5916\u306e\u5834\u5408\u3002\n    \"\"\"\n    if self.logits.shape != self.probs.shape:\n        msg = f\"logits.shape {self.logits.shape} and probs.shape {self.probs.shape} must be the same.\"\n        raise ValueError(msg)\n    if (self.probs &lt; 0).any() or (self.probs &gt; 1).any():\n        msg = \"probs must be in the range [0, 1].\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.BernoulliStoch.get_distribution","title":"get_distribution","text":"<pre><code>get_distribution(independent=1)\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def get_distribution(self, independent: int = 1) -&gt; D.Independent:\n    return D.Independent(BernoulliStraightThrough(self.probs), independent)\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.BernoulliStoch.save","title":"save","text":"<pre><code>save(path)\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    os.makedirs(path, exist_ok=True)\n\n    save_blosc2(f\"{path}/logits.blosc2\", self.logits.detach().clone().cpu().numpy())\n    save_blosc2(f\"{path}/probs.blosc2\", self.probs.detach().clone().cpu().numpy())\n    save_blosc2(f\"{path}/stoch.blosc2\", self.stoch.detach().clone().cpu().numpy())\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.BernoulliStoch.squeeze","title":"squeeze","text":"<pre><code>squeeze(dim)\n</code></pre> <p>Squeeze the parameters of the Bernoulli distribution.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension to squeeze.</p> required <p>Returns:</p> Type Description <code>BernoulliStoch</code> <p>Squeezed Bernoulli distribution.</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def squeeze(self, dim: int) -&gt; BernoulliStoch:\n    \"\"\"\n    Squeeze the parameters of the Bernoulli distribution.\n\n    Parameters\n    ----------\n    dim : int\n        Dimension to squeeze.\n\n    Returns\n    -------\n    BernoulliStoch\n        Squeezed Bernoulli distribution.\n\n    \"\"\"\n    return BernoulliStoch(\n        self.logits.squeeze(dim),\n        self.probs.squeeze(dim),\n        self.stoch.squeeze(dim),\n    )\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.BernoulliStoch.unsqueeze","title":"unsqueeze","text":"<pre><code>unsqueeze(dim)\n</code></pre> <p>Unsqueeze the parameters of the Bernoulli distribution.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension to unsqueeze.</p> required <p>Returns:</p> Type Description <code>BernoulliStoch</code> <p>Unsqueezed Bernoulli distribution.</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def unsqueeze(self, dim: int) -&gt; BernoulliStoch:\n    \"\"\"\n    Unsqueeze the parameters of the Bernoulli distribution.\n\n    Parameters\n    ----------\n    dim : int\n        Dimension to unsqueeze.\n\n    Returns\n    -------\n    BernoulliStoch\n        Unsqueezed Bernoulli distribution.\n\n    \"\"\"\n    return BernoulliStoch(\n        self.logits.unsqueeze(dim),\n        self.probs.unsqueeze(dim),\n        self.stoch.unsqueeze(dim),\n    )\n</code></pre>"},{"location":"api/distributions/#stochstate","title":"StochState","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.StochState","title":"StochState  <code>module-attribute</code>","text":"<pre><code>StochState = NormalStoch | CategoricalStoch | BernoulliStoch\n</code></pre>"},{"location":"api/distributions/#bsqcodebook","title":"BSQCodebook","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.BSQCodebook","title":"BSQCodebook","text":"<pre><code>BSQCodebook(codebook_dim)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Binary Spherical Quantization codebook.</p> Reference <p>https://arxiv.org/abs/2406.07548</p> <p>Parameters:</p> Name Type Description Default <code>codebook_dim</code> <code>int</code> <p>Dimension of the codebook.</p> required <p>Attributes:</p> Name Type Description <code>codebook_dim</code> <code>int</code> <p>Dimension of the codebook.</p> <code>codebook_size</code> <code>int</code> <p>Size of the codebook. This is equal to 2 ** codebook_dim.</p> <code>codebook</code> <code>Tensor</code> <p>Codebook.</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def __init__(\n    self,\n    codebook_dim: int,\n) -&gt; None:\n    super().__init__()\n    self.codebook_dim = codebook_dim\n    self.codebook_size = 2**codebook_dim\n    mask = 2 ** torch.arange(codebook_dim - 1, -1, -1)\n    self.mask: torch.Tensor\n    self.register_buffer(\"mask\", mask)\n    self.mask = mask\n    all_codes = torch.arange(self.codebook_size)\n    bits = ((all_codes[..., None].int() &amp; self.mask) != 0).float()\n    codebook = self.bits_to_codes(bits)\n    self.register_buffer(\"codebook\", codebook.float(), persistent=False)\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.BSQCodebook-attributes","title":"Attributes","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.BSQCodebook.codebook_dim","title":"codebook_dim  <code>instance-attribute</code>","text":"<pre><code>codebook_dim = codebook_dim\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.BSQCodebook.codebook_size","title":"codebook_size  <code>instance-attribute</code>","text":"<pre><code>codebook_size = 2 ** codebook_dim\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.BSQCodebook.mask","title":"mask  <code>instance-attribute</code>","text":"<pre><code>mask = mask\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.BSQCodebook-functions","title":"Functions","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.BSQCodebook.bits_to_codes","title":"bits_to_codes  <code>staticmethod</code>","text":"<pre><code>bits_to_codes(bits)\n</code></pre> <p>Convert bits to codes, which are bits of either 0 or 1.</p> <p>Parameters:</p> Name Type Description Default <code>bits</code> <code>Tensor</code> <p>Bits of either 0 or 1.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Codes, which are bits depending on codebook_dim(dimension of the sphery)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bits = torch.tensor([[0., 1.], [1., 0.]])\n&gt;&gt;&gt; BSQCodebook.bits_to_codes(bits)\ntensor([[-0.7071,  0.7071],\n        [ 0.7071, -0.7071]])\n</code></pre> <pre><code>&gt;&gt;&gt; bits = torch.tensor([[0., 0., 1.], [1., 0., 0.]])\n&gt;&gt;&gt; BSQCodebook.bits_to_codes(bits)\ntensor([[-0.5774, -0.5774,  0.5774],\n        [ 0.5774, -0.5774, -0.5774]])\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>@staticmethod\ndef bits_to_codes(bits: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Convert bits to codes, which are bits of either 0 or 1.\n\n    Parameters\n    ----------\n    bits : torch.Tensor\n        Bits of either 0 or 1.\n\n    Returns\n    -------\n    torch.Tensor\n        Codes, which are bits depending on codebook_dim(dimension of the sphery)\n\n    Examples\n    --------\n    &gt;&gt;&gt; bits = torch.tensor([[0., 1.], [1., 0.]])\n    &gt;&gt;&gt; BSQCodebook.bits_to_codes(bits)\n    tensor([[-0.7071,  0.7071],\n            [ 0.7071, -0.7071]])\n\n    &gt;&gt;&gt; bits = torch.tensor([[0., 0., 1.], [1., 0., 0.]])\n    &gt;&gt;&gt; BSQCodebook.bits_to_codes(bits)\n    tensor([[-0.5774, -0.5774,  0.5774],\n            [ 0.5774, -0.5774, -0.5774]])\n\n\n    \"\"\"\n    bits = bits * 2 - 1\n    return F.normalize(bits, dim=-1)\n</code></pre>"},{"location":"api/distributions/#ml_networks.torch.distributions.BSQCodebook.indices_to_codes","title":"indices_to_codes","text":"<pre><code>indices_to_codes(indices)\n</code></pre> <p>Convert indices to codes, which are bits of either -1 or 1.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>Tensor</code> <p>Indices.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Codes, which are bits depending on codebook_dim(dimension of the sphery)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; indices = torch.tensor([[31], [19]])\n&gt;&gt;&gt; codebook = BSQCodebook(5)\n&gt;&gt;&gt; codebook.indices_to_codes(indices)\ntensor([[ 0.4472,  0.4472,  0.4472,  0.4472,  0.4472],\n        [ 0.4472, -0.4472, -0.4472,  0.4472,  0.4472]])\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def indices_to_codes(\n    self,\n    indices: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Convert indices to codes, which are bits of either -1 or 1.\n\n    Parameters\n    ----------\n    indices : torch.Tensor\n        Indices.\n\n    Returns\n    -------\n    torch.Tensor\n        Codes, which are bits depending on codebook_dim(dimension of the sphery)\n\n    Examples\n    --------\n    &gt;&gt;&gt; indices = torch.tensor([[31], [19]])\n    &gt;&gt;&gt; codebook = BSQCodebook(5)\n    &gt;&gt;&gt; codebook.indices_to_codes(indices)\n    tensor([[ 0.4472,  0.4472,  0.4472,  0.4472,  0.4472],\n            [ 0.4472, -0.4472, -0.4472,  0.4472,  0.4472]])\n\n    \"\"\"\n    indices = indices.squeeze(-1)\n\n    # indices to codes, which are bits of either -1 or 1\n\n    bits = ((indices[..., None].int() &amp; self.mask) != 0).float()\n\n    return self.bits_to_codes(bits)\n</code></pre>"},{"location":"api/distributions/#stack_dist","title":"stack_dist","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.stack_dist","title":"stack_dist","text":"<pre><code>stack_dist(stochs, dim=0)\n</code></pre> <p>Stack the parameters of the distributions.</p> <p>Parameters:</p> Name Type Description Default <code>stochs</code> <code>Tuple[StochState, ...]</code> <p>Tuple of the distributions.</p> required <code>dim</code> <code>int</code> <p>Dimension to stack the parameters of the distributions. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>StochState</code> <p>Stacked distribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dist1 = NormalStoch(torch.randn(2, 3), torch.rand(2, 3), torch.randn(2, 3))\n&gt;&gt;&gt; dist2 = NormalStoch(torch.randn(2, 3), torch.rand(2, 3), torch.randn(2, 3))\n&gt;&gt;&gt; stack_dist = stack_dist((dist1, dist2))\n&gt;&gt;&gt; stack_dist.shape\nNormalShape(mean=torch.Size([2, 2, 3]), std=torch.Size([2, 2, 3]), stoch=torch.Size([2, 2, 3]))\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def stack_dist(stochs: tuple[StochState, ...], dim: int = 0) -&gt; StochState | None:\n    \"\"\"\n    Stack the parameters of the distributions.\n\n    Parameters\n    ----------\n    stochs : Tuple[StochState, ...]\n        Tuple of the distributions.\n    dim : int, optional\n        Dimension to stack the parameters of the distributions. Default is 0.\n\n    Returns\n    -------\n    StochState\n        Stacked distribution.\n\n    Examples\n    --------\n    &gt;&gt;&gt; dist1 = NormalStoch(torch.randn(2, 3), torch.rand(2, 3), torch.randn(2, 3))\n    &gt;&gt;&gt; dist2 = NormalStoch(torch.randn(2, 3), torch.rand(2, 3), torch.randn(2, 3))\n    &gt;&gt;&gt; stack_dist = stack_dist((dist1, dist2))\n    &gt;&gt;&gt; stack_dist.shape\n    NormalShape(mean=torch.Size([2, 2, 3]), std=torch.Size([2, 2, 3]), stoch=torch.Size([2, 2, 3]))\n\n    \"\"\"\n    if isinstance(stochs[0], NormalStoch):\n        return NormalStoch(\n            torch.stack([stoch.mean for stoch in stochs], dim=dim),\n            torch.stack([stoch.std for stoch in stochs], dim=dim),\n            torch.stack([stoch.stoch for stoch in stochs], dim=dim),\n        )\n    if isinstance(stochs[0], CategoricalStoch):\n        return CategoricalStoch(\n            torch.stack([stoch.logits for stoch in stochs], dim=dim),\n            torch.stack([stoch.probs for stoch in stochs], dim=dim),\n            torch.stack([stoch.stoch for stoch in stochs], dim=dim),\n        )\n    if isinstance(stochs[0], BernoulliStoch):\n        return BernoulliStoch(\n            torch.stack([stoch.logits for stoch in stochs], dim=dim),\n            torch.stack([stoch.probs for stoch in stochs], dim=dim),\n            torch.stack([stoch.stoch for stoch in stochs], dim=dim),\n        )\n    return None\n</code></pre>"},{"location":"api/distributions/#cat_dist","title":"cat_dist","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.cat_dist","title":"cat_dist","text":"<pre><code>cat_dist(stochs, dim=-1)\n</code></pre> <p>Concatenate the parameters of the distributions.</p> <p>Parameters:</p> Name Type Description Default <code>stochs</code> <code>Tuple[StochState, ...]</code> <p>Tuple of the distributions.</p> required <code>dim</code> <code>int</code> <p>Dimension to concatenate the parameters of the distributions. Default is -1.</p> <code>-1</code> <p>Returns:</p> Type Description <code>StochState</code> <p>Concatenated distribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dist1 = NormalStoch(torch.randn(2, 3), torch.rand(2, 3), torch.randn(2, 3))\n&gt;&gt;&gt; dist2 = NormalStoch(torch.randn(2, 3), torch.rand(2, 3), torch.randn(2, 3))\n&gt;&gt;&gt; cat_dist = cat_dist((dist1, dist2))\n&gt;&gt;&gt; cat_dist.shape\nNormalShape(mean=torch.Size([2, 6]), std=torch.Size([2, 6]), stoch=torch.Size([2, 6]))\n</code></pre> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def cat_dist(stochs: tuple[StochState, ...], dim: int = -1) -&gt; StochState | None:\n    \"\"\"\n    Concatenate the parameters of the distributions.\n\n    Parameters\n    ----------\n    stochs : Tuple[StochState, ...]\n        Tuple of the distributions.\n    dim : int, optional\n        Dimension to concatenate the parameters of the distributions.\n        Default is -1.\n\n    Returns\n    -------\n    StochState\n        Concatenated distribution.\n\n    Examples\n    --------\n    &gt;&gt;&gt; dist1 = NormalStoch(torch.randn(2, 3), torch.rand(2, 3), torch.randn(2, 3))\n    &gt;&gt;&gt; dist2 = NormalStoch(torch.randn(2, 3), torch.rand(2, 3), torch.randn(2, 3))\n    &gt;&gt;&gt; cat_dist = cat_dist((dist1, dist2))\n    &gt;&gt;&gt; cat_dist.shape\n    NormalShape(mean=torch.Size([2, 6]), std=torch.Size([2, 6]), stoch=torch.Size([2, 6]))\n\n    \"\"\"\n    if isinstance(stochs[0], NormalStoch):\n        return NormalStoch(\n            torch.cat([stoch.mean for stoch in stochs], dim=dim),\n            torch.cat([stoch.std for stoch in stochs], dim=dim),\n            torch.cat([stoch.stoch for stoch in stochs], dim=dim),\n        )\n    if isinstance(stochs[0], CategoricalStoch):\n        return CategoricalStoch(\n            torch.cat([stoch.logits for stoch in stochs], dim=dim),\n            torch.cat([stoch.probs for stoch in stochs], dim=dim),\n            torch.cat([stoch.stoch for stoch in stochs], dim=dim),\n        )\n    if isinstance(stochs[0], BernoulliStoch):\n        return BernoulliStoch(\n            torch.cat([stoch.logits for stoch in stochs], dim=dim),\n            torch.cat([stoch.probs for stoch in stochs], dim=dim),\n            torch.cat([stoch.stoch for stoch in stochs], dim=dim),\n        )\n    return None\n</code></pre>"},{"location":"api/distributions/#get_dist","title":"get_dist","text":""},{"location":"api/distributions/#ml_networks.torch.distributions.get_dist","title":"get_dist","text":"<pre><code>get_dist(state)\n</code></pre> <p>\u78ba\u7387\u7684\u72b6\u614b\u304b\u3089\u5206\u5e03\u3092\u53d6\u5f97\u3059\u308b.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>StochState</code> <p>\u6b63\u898f\u5206\u5e03\u30fb\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5206\u5e03\u30fb\u30d9\u30eb\u30cc\u30fc\u30a4\u5206\u5e03\u306e\u3044\u305a\u308c\u304b\u306e\u78ba\u7387\u7684\u72b6\u614b\u3002</p> required <p>Returns:</p> Type Description <code>Independent</code> <p>\u4e0e\u3048\u3089\u308c\u305f\u72b6\u614b\u306b\u5bfe\u5fdc\u3059\u308b<code>torch.distributions.Independent</code>\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3002</p> Source code in <code>src/ml_networks/torch/distributions.py</code> <pre><code>def get_dist(state: StochState) -&gt; D.Independent:\n    \"\"\"\u78ba\u7387\u7684\u72b6\u614b\u304b\u3089\u5206\u5e03\u3092\u53d6\u5f97\u3059\u308b.\n\n    Parameters\n    ----------\n    state : StochState\n        \u6b63\u898f\u5206\u5e03\u30fb\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5206\u5e03\u30fb\u30d9\u30eb\u30cc\u30fc\u30a4\u5206\u5e03\u306e\u3044\u305a\u308c\u304b\u306e\u78ba\u7387\u7684\u72b6\u614b\u3002\n\n    Returns\n    -------\n    D.Independent\n        \u4e0e\u3048\u3089\u308c\u305f\u72b6\u614b\u306b\u5bfe\u5fdc\u3059\u308b`torch.distributions.Independent`\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3002\n    \"\"\"\n    if isinstance(state, NormalStoch):\n        normal = D.Normal(state.mean, state.std)\n        return D.Independent(normal, 1)\n    if isinstance(state, CategoricalStoch):\n        categorical = D.OneHotCategoricalStraightThrough(probs=state.probs)\n        return D.Independent(categorical, 1)\n    if isinstance(state, BernoulliStoch):\n        bernoulli = BernoulliStraightThrough(probs=state.probs)\n        return D.Independent(bernoulli, 2)\n    raise NotImplementedError\n</code></pre>"},{"location":"api/jax/","title":"JAX API \u30ea\u30d5\u30a1\u30ec\u30f3\u30b9","text":"<p>JAX\uff08Flax NNX\uff09\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u306eAPI\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u3067\u3059\u3002</p> <p>PyTorch\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3068\u540c\u4e00\u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3092\u63d0\u4f9b\u3057\u3066\u3044\u307e\u3059\u3002\u8a73\u7d30\u306f\u5404PyTorch API\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u30da\u30fc\u30b8\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"api/jax/#ml_networksjaxlayers","title":"\u30ec\u30a4\u30e4\u30fc (<code>ml_networks.jax.layers</code>)","text":""},{"location":"api/jax/#ml_networks.jax.layers.MLPLayer","title":"MLPLayer","text":"<pre><code>MLPLayer(input_dim, output_dim, cfg, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Multi-layer perceptron layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input dimension.</p> required <code>output_dim</code> <code>int</code> <p>Output dimension.</p> required <code>cfg</code> <code>MLPConfig</code> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; rngs = nnx.Rngs(0)\n&gt;&gt;&gt; cfg = MLPConfig(\n...     hidden_dim=16,\n...     n_layers=3,\n...     output_activation=\"ReLU\",\n...     linear_cfg=LinearConfig(\n...         activation=\"ReLU\",\n...         norm=\"layer\",\n...         norm_cfg={\"eps\": 1e-05, \"elementwise_affine\": True, \"bias\": True},\n...         dropout=0.1,\n...         norm_first=False,\n...         bias=True\n...     )\n... )\n&gt;&gt;&gt; mlp = MLPLayer(32, 16, cfg, rngs=rngs)\n&gt;&gt;&gt; x = jnp.ones((1, 32))\n&gt;&gt;&gt; output = mlp(x)\n&gt;&gt;&gt; output.shape\n(1, 16)\n</code></pre> Source code in <code>src/ml_networks/jax/layers.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    output_dim: int,\n    cfg: MLPConfig,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    self.cfg = deepcopy(cfg)\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.hidden_dim = cfg.hidden_dim\n    self.n_layers = cfg.n_layers\n    self.layers = nnx.List(self._build_dense(rngs=rngs))\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.MLPLayer-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.layers.MLPLayer.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = deepcopy(cfg)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.MLPLayer.hidden_dim","title":"hidden_dim  <code>instance-attribute</code>","text":"<pre><code>hidden_dim = hidden_dim\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.MLPLayer.input_dim","title":"input_dim  <code>instance-attribute</code>","text":"<pre><code>input_dim = input_dim\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.MLPLayer.layers","title":"layers  <code>instance-attribute</code>","text":"<pre><code>layers = List(_build_dense(rngs=rngs))\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.MLPLayer.n_layers","title":"n_layers  <code>instance-attribute</code>","text":"<pre><code>n_layers = n_layers\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.MLPLayer.output_dim","title":"output_dim  <code>instance-attribute</code>","text":"<pre><code>output_dim = output_dim\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.MLPLayer-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.layers.MLPLayer.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor of shape (*, input_dim)</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Output tensor of shape (*, output_dim)</p> Source code in <code>src/ml_networks/jax/layers.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : jax.Array\n        Input tensor of shape (*, input_dim)\n\n    Returns\n    -------\n    jax.Array\n        Output tensor of shape (*, output_dim)\n    \"\"\"\n    for layer in self.layers:\n        x = layer(x)\n    return x\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.LinearNormActivation","title":"LinearNormActivation","text":"<pre><code>LinearNormActivation(input_dim, output_dim, cfg, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Linear layer with normalization and activation, and dropouts.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input dimension.</p> required <code>output_dim</code> <code>int</code> <p>Output dimension.</p> required <code>cfg</code> <code>LinearConfig</code> <p>Linear layer configuration.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; rngs = nnx.Rngs(0)\n&gt;&gt;&gt; cfg = LinearConfig(\n...     activation=\"ReLU\",\n...     norm=\"layer\",\n...     norm_cfg={\"eps\": 1e-05, \"elementwise_affine\": True, \"bias\": True},\n...     dropout=0.1,\n...     norm_first=False,\n...     bias=True\n... )\n&gt;&gt;&gt; linear = LinearNormActivation(32, 16, cfg, rngs=rngs)\n&gt;&gt;&gt; x = jnp.ones((1, 32))\n&gt;&gt;&gt; output = linear(x)\n&gt;&gt;&gt; output.shape\n(1, 16)\n</code></pre> Source code in <code>src/ml_networks/jax/layers.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    output_dim: int,\n    cfg: LinearConfig,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    out_features = output_dim * 2 if \"glu\" in cfg.activation.lower() else output_dim\n\n    self.linear = nnx.Linear(input_dim, out_features, use_bias=cfg.bias, rngs=rngs)\n\n    normalized_shape = input_dim if cfg.norm_first else out_features\n\n    norm_cfg = dict(cfg.norm_cfg)\n    norm_cfg[\"normalized_shape\"] = normalized_shape\n    self.norm = get_norm(cfg.norm, rngs=rngs, **norm_cfg)\n    self.activation = Activation(cfg.activation)\n    self.dropout: nnx.Module\n    if cfg.dropout &gt; 0:\n        self.dropout = nnx.Dropout(rate=cfg.dropout, rngs=rngs)\n    else:\n        self.dropout = Identity()\n    self.norm_first = cfg.norm_first\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.LinearNormActivation-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.layers.LinearNormActivation.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = Activation(activation)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.LinearNormActivation.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.LinearNormActivation.linear","title":"linear  <code>instance-attribute</code>","text":"<pre><code>linear = Linear(input_dim, out_features, use_bias=bias, rngs=rngs)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.LinearNormActivation.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = get_norm(norm, rngs=rngs, **norm_cfg)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.LinearNormActivation.norm_first","title":"norm_first  <code>instance-attribute</code>","text":"<pre><code>norm_first = norm_first\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.LinearNormActivation-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.layers.LinearNormActivation.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor of shape (*, input_dim)</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Output tensor of shape (*, output_dim)</p> Source code in <code>src/ml_networks/jax/layers.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : jax.Array\n        Input tensor of shape (*, input_dim)\n\n    Returns\n    -------\n    jax.Array\n        Output tensor of shape (*, output_dim)\n    \"\"\"\n    if self.norm_first:\n        x = self.norm(x)\n        x = self.linear(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n    else:\n        x = self.linear(x)\n        x = self.norm(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n    return x\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvNormActivation","title":"ConvNormActivation","text":"<pre><code>ConvNormActivation(in_channels, out_channels, cfg, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Convolutional layer with normalization and activation, and dropouts.</p> <p>Uses NHWC (channels-last) format.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Input channels.</p> required <code>out_channels</code> <code>int</code> <p>Output channels.</p> required <code>cfg</code> <code>ConvConfig</code> <p>Convolutional layer configuration.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; rngs = nnx.Rngs(0)\n&gt;&gt;&gt; cfg = ConvConfig(\n...     activation=\"ReLU\",\n...     kernel_size=3,\n...     stride=1,\n...     padding=1,\n...     dilation=1,\n...     groups=1,\n...     bias=True,\n...     dropout=0.1,\n...     norm=\"batch\",\n...     norm_cfg={\"affine\": True, \"track_running_stats\": True},\n...     scale_factor=0\n... )\n&gt;&gt;&gt; conv = ConvNormActivation(3, 16, cfg, rngs=rngs)\n&gt;&gt;&gt; x = jnp.ones((1, 32, 32, 3))\n&gt;&gt;&gt; output = conv(x)\n&gt;&gt;&gt; output.shape\n(1, 32, 32, 16)\n</code></pre> Source code in <code>src/ml_networks/jax/layers.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    cfg: ConvConfig,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    out_channels_ = out_channels\n    if \"glu\" in cfg.activation.lower():\n        out_channels_ *= 2\n    if cfg.scale_factor &gt; 0:\n        out_channels_ *= abs(cfg.scale_factor) ** 2\n    elif cfg.scale_factor &lt; 0:\n        out_channels_ //= abs(cfg.scale_factor) ** 2\n\n    # Handle padding mode\n    self.padding_mode = cfg.padding_mode\n    self.manual_padding = cfg.padding if cfg.padding_mode != \"zeros\" else 0\n    conv_padding: Any\n    if cfg.padding_mode != \"zeros\":\n        conv_padding = \"VALID\"\n    else:\n        conv_padding = ((cfg.padding, cfg.padding), (cfg.padding, cfg.padding))\n\n    self.conv = nnx.Conv(\n        in_features=in_channels,\n        out_features=out_channels_,\n        kernel_size=(cfg.kernel_size, cfg.kernel_size),\n        strides=(cfg.stride, cfg.stride),\n        padding=conv_padding,\n        kernel_dilation=(cfg.dilation, cfg.dilation),\n        feature_group_count=cfg.groups,\n        use_bias=cfg.bias,\n        rngs=rngs,\n    )\n\n    norm_cfg = dict(cfg.norm_cfg) if cfg.norm_cfg else {}\n    if cfg.norm not in {\"none\", \"group\"}:\n        norm_cfg[\"num_features\"] = out_channels_\n    elif cfg.norm == \"group\":\n        norm_cfg[\"num_channels\"] = in_channels if cfg.norm_first else out_channels_\n\n    norm_type: Literal[\"layer\", \"rms\", \"group\", \"batch2d\", \"batch1d\", \"none\"] = (\n        \"batch2d\" if cfg.norm == \"batch\" else cfg.norm\n    )  # type: ignore[assignment]\n    self.norm = get_norm(norm_type, rngs=rngs, **norm_cfg)\n\n    self.scale_factor = cfg.scale_factor\n    self.activation = Activation(cfg.activation, dim=-1)\n    self.dropout: nnx.Module = nnx.Dropout(rate=cfg.dropout, rngs=rngs) if cfg.dropout &gt; 0 else Identity()\n    self.norm_first = cfg.norm_first\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvNormActivation-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.layers.ConvNormActivation.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = Activation(activation, dim=-1)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvNormActivation.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = Conv(in_features=in_channels, out_features=out_channels_, kernel_size=(kernel_size, kernel_size), strides=(stride, stride), padding=conv_padding, kernel_dilation=(dilation, dilation), feature_group_count=groups, use_bias=bias, rngs=rngs)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvNormActivation.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = Dropout(rate=dropout, rngs=rngs) if dropout &gt; 0 else Identity()\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvNormActivation.manual_padding","title":"manual_padding  <code>instance-attribute</code>","text":"<pre><code>manual_padding = padding if padding_mode != 'zeros' else 0\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvNormActivation.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = get_norm(norm_type, rngs=rngs, **norm_cfg)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvNormActivation.norm_first","title":"norm_first  <code>instance-attribute</code>","text":"<pre><code>norm_first = norm_first\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvNormActivation.padding_mode","title":"padding_mode  <code>instance-attribute</code>","text":"<pre><code>padding_mode = padding_mode\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvNormActivation.scale_factor","title":"scale_factor  <code>instance-attribute</code>","text":"<pre><code>scale_factor = scale_factor\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvNormActivation-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.layers.ConvNormActivation.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor of shape (B, H, W, in_channels) or (H, W, in_channels)</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Output tensor of shape (B, H', W', out_channels) or (H', W', out_channels)</p> Source code in <code>src/ml_networks/jax/layers.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : jax.Array\n        Input tensor of shape (B, H, W, in_channels) or (H, W, in_channels)\n\n    Returns\n    -------\n    jax.Array\n        Output tensor of shape (B, H', W', out_channels) or (H', W', out_channels)\n    \"\"\"\n    if self.norm_first:\n        x = self.norm(x)\n        x = _pad_input(x, self.manual_padding, self.padding_mode, n_spatial_dims=2)\n        x = self.conv(x)\n        x = self._apply_shuffle(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n    else:\n        x = _pad_input(x, self.manual_padding, self.padding_mode, n_spatial_dims=2)\n        x = self.conv(x)\n        x = self.norm(x)\n        x = self._apply_shuffle(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n    return x\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvTransposeNormActivation","title":"ConvTransposeNormActivation","text":"<pre><code>ConvTransposeNormActivation(in_channels, out_channels, cfg, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Transposed convolutional layer with normalization and activation, and dropouts.</p> <p>Uses NHWC (channels-last) format.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Input channels.</p> required <code>out_channels</code> <code>int</code> <p>Output channels.</p> required <code>cfg</code> <code>ConvConfig</code> <p>Convolutional layer configuration.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; rngs = nnx.Rngs(0)\n&gt;&gt;&gt; cfg = ConvConfig(\n...     activation=\"ReLU\",\n...     kernel_size=3,\n...     stride=1,\n...     padding=1,\n...     output_padding=0,\n...     dilation=1,\n...     groups=1,\n...     bias=True,\n...     dropout=0.1,\n...     norm=\"batch\",\n...     norm_cfg={\"affine\": True, \"track_running_stats\": True}\n... )\n&gt;&gt;&gt; conv = ConvTransposeNormActivation(3, 16, cfg, rngs=rngs)\n&gt;&gt;&gt; x = jnp.ones((1, 32, 32, 3))\n&gt;&gt;&gt; output = conv(x)\n&gt;&gt;&gt; output.shape\n(1, 32, 32, 16)\n</code></pre> Source code in <code>src/ml_networks/jax/layers.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    cfg: ConvConfig,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    out_features = out_channels * 2 if \"glu\" in cfg.activation.lower() else out_channels\n\n    self.conv = nnx.ConvTranspose(\n        in_features=in_channels,\n        out_features=out_features,\n        kernel_size=(cfg.kernel_size, cfg.kernel_size),\n        strides=(cfg.stride, cfg.stride),\n        padding=((cfg.padding, cfg.padding + cfg.output_padding), (cfg.padding, cfg.padding + cfg.output_padding)),\n        kernel_dilation=(cfg.dilation, cfg.dilation),\n        use_bias=cfg.bias,\n        rngs=rngs,\n    )\n\n    norm_cfg = dict(cfg.norm_cfg) if cfg.norm_cfg else {}\n    if cfg.norm not in {\"none\", \"group\"}:\n        norm_cfg[\"num_features\"] = out_features\n    elif cfg.norm == \"group\":\n        norm_cfg[\"num_channels\"] = out_features\n    norm_type: Literal[\"layer\", \"rms\", \"group\", \"batch2d\", \"batch1d\", \"none\"] = (\n        \"batch2d\" if cfg.norm == \"batch\" else cfg.norm\n    )  # type: ignore[assignment]\n    self.norm = get_norm(norm_type, rngs=rngs, **norm_cfg)\n    self.activation = Activation(cfg.activation, dim=-1)\n    self.dropout: nnx.Module = nnx.Dropout(rate=cfg.dropout, rngs=rngs) if cfg.dropout &gt; 0 else Identity()\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvTransposeNormActivation-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.layers.ConvTransposeNormActivation.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = Activation(activation, dim=-1)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvTransposeNormActivation.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = ConvTranspose(in_features=in_channels, out_features=out_features, kernel_size=(kernel_size, kernel_size), strides=(stride, stride), padding=((padding, padding + output_padding), (padding, padding + output_padding)), kernel_dilation=(dilation, dilation), use_bias=bias, rngs=rngs)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvTransposeNormActivation.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = Dropout(rate=dropout, rngs=rngs) if dropout &gt; 0 else Identity()\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvTransposeNormActivation.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = get_norm(norm_type, rngs=rngs, **norm_cfg)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.layers.ConvTransposeNormActivation-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.layers.ConvTransposeNormActivation.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor of shape (B, H, W, in_channels) or (H, W, in_channels)</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Output tensor of shape (B, H', W', out_channels) or (H', W', out_channels)</p> Source code in <code>src/ml_networks/jax/layers.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : jax.Array\n        Input tensor of shape (B, H, W, in_channels) or (H, W, in_channels)\n\n    Returns\n    -------\n    jax.Array\n        Output tensor of shape (B, H', W', out_channels) or (H', W', out_channels)\n    \"\"\"\n    x = self.conv(x)\n    x = self.norm(x)\n    x = self.activation(x)\n    return self.dropout(x)\n</code></pre>"},{"location":"api/jax/#ml_networksjaxvision","title":"\u30d3\u30b8\u30e7\u30f3 (<code>ml_networks.jax.vision</code>)","text":""},{"location":"api/jax/#ml_networks.jax.vision.Encoder","title":"Encoder","text":"<pre><code>Encoder(obs_shape, cfg, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Image encoder module (NHWC format).</p> <p>Parameters:</p> Name Type Description Default <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>Observation shape in (H, W, C) format.</p> required <code>cfg</code> <code>EncoderConfig</code> <p>Encoder configuration.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required Source code in <code>src/ml_networks/jax/vision.py</code> <pre><code>def __init__(\n    self,\n    obs_shape: tuple[int, int, int],\n    cfg: EncoderConfig,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    self.cfg = cfg\n    self.obs_shape = obs_shape\n    in_channels = obs_shape[2]  # NHWC: C is last\n\n    layers: list[nnx.Module] = []\n    spatial_shape: tuple[int, int] = (obs_shape[0], obs_shape[1])\n\n    for _i, (n_channels, kernel_size, stride) in enumerate(\n        zip(cfg.channels, cfg.kernel_sizes, cfg.strides, strict=False),\n    ):\n        layers.append(\n            ConvNormActivation(\n                in_channels,\n                n_channels,\n                ConvConfig(\n                    activation=cfg.activation,\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    padding=(kernel_size - 1) // 2,\n                    dilation=1,\n                    groups=1,\n                    bias=True,\n                    dropout=0.0,\n                    norm=cfg.norm,\n                    norm_cfg=cfg.norm_cfg,\n                ),\n                rngs=rngs,\n            ),\n        )\n        spatial_shape = conv_out_shape(\n            spatial_shape,\n            padding=(kernel_size - 1) // 2,\n            kernel_size=kernel_size,\n            stride=stride,\n        )\n        in_channels = n_channels\n\n    self.conv_layers = nnx.List(layers)\n    self.output_spatial_shape = spatial_shape\n    self.output_channels = in_channels\n    self.output_dim = in_channels * spatial_shape[0] * spatial_shape[1]\n\n    if cfg.use_spatial_softmax:\n        self.spatial_softmax = SpatialSoftmax(cfg.spatial_softmax_cfg)\n        self.output_dim = self.output_channels * 2\n    else:\n        self.spatial_softmax = None\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.Encoder-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.vision.Encoder.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.Encoder.conv_layers","title":"conv_layers  <code>instance-attribute</code>","text":"<pre><code>conv_layers = List(layers)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.Encoder.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.Encoder.output_channels","title":"output_channels  <code>instance-attribute</code>","text":"<pre><code>output_channels = in_channels\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.Encoder.output_dim","title":"output_dim  <code>instance-attribute</code>","text":"<pre><code>output_dim = in_channels * spatial_shape[0] * spatial_shape[1]\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.Encoder.output_spatial_shape","title":"output_spatial_shape  <code>instance-attribute</code>","text":"<pre><code>output_spatial_shape = spatial_shape\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.Encoder.spatial_softmax","title":"spatial_softmax  <code>instance-attribute</code>","text":"<pre><code>spatial_softmax = SpatialSoftmax(spatial_softmax_cfg)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.Encoder-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.vision.Encoder.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor of shape (B, H, W, C) in NHWC format.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Encoded tensor. Shape depends on spatial_softmax setting.</p> Source code in <code>src/ml_networks/jax/vision.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : jax.Array\n        Input tensor of shape (B, H, W, C) in NHWC format.\n\n    Returns\n    -------\n    jax.Array\n        Encoded tensor. Shape depends on spatial_softmax setting.\n    \"\"\"\n    for layer in self.conv_layers:\n        x = layer(x)\n    if self.spatial_softmax is not None:\n        return self.spatial_softmax(x)\n    return x.reshape(x.shape[0], -1)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.Decoder","title":"Decoder","text":"<pre><code>Decoder(obs_shape, cfg, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Image decoder module (NHWC format).</p> <p>Parameters:</p> Name Type Description Default <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>Observation shape in (H, W, C) format.</p> required <code>cfg</code> <code>DecoderConfig</code> <p>Decoder configuration.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required Source code in <code>src/ml_networks/jax/vision.py</code> <pre><code>def __init__(\n    self,\n    obs_shape: tuple[int, int, int],\n    cfg: DecoderConfig,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    self.cfg = cfg\n    self.obs_shape = obs_shape\n    out_channels = obs_shape[2]  # NHWC: C is last\n\n    channels = list(cfg.channels)\n    kernel_sizes = list(cfg.kernel_sizes)\n    strides = list(cfg.strides)\n\n    layers: list[nnx.Module] = []\n    for i in range(len(channels) - 1):\n        conv_cfg = ConvConfig(\n            activation=cfg.activation,\n            kernel_size=kernel_sizes[i],\n            stride=strides[i],\n            padding=(kernel_sizes[i] - 1) // 2,\n            output_padding=strides[i] - 1,\n            dilation=1,\n            groups=1,\n            bias=True,\n            dropout=0.0,\n            norm=cfg.norm,\n            norm_cfg=cfg.norm_cfg,\n        )\n        layers.append(\n            ConvTransposeNormActivation(\n                channels[i],\n                channels[i + 1],\n                conv_cfg,\n                rngs=rngs,\n            ),\n        )\n\n    # Final layer with Identity activation\n    final_cfg = ConvConfig(\n        activation=\"Identity\",\n        kernel_size=kernel_sizes[-1],\n        stride=strides[-1],\n        padding=(kernel_sizes[-1] - 1) // 2,\n        output_padding=strides[-1] - 1,\n        dilation=1,\n        groups=1,\n        bias=True,\n        dropout=0.0,\n        norm=\"none\",\n    )\n    layers.append(\n        ConvTransposeNormActivation(\n            channels[-1],\n            out_channels,\n            final_cfg,\n            rngs=rngs,\n        ),\n    )\n\n    self.conv_layers = nnx.List(layers)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.Decoder-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.vision.Decoder.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.Decoder.conv_layers","title":"conv_layers  <code>instance-attribute</code>","text":"<pre><code>conv_layers = List(layers)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.Decoder.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.Decoder-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.vision.Decoder.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor of shape (B, H, W, C) in NHWC format.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Decoded tensor of shape (B, H', W', C) in NHWC format.</p> Source code in <code>src/ml_networks/jax/vision.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : jax.Array\n        Input tensor of shape (B, H, W, C) in NHWC format.\n\n    Returns\n    -------\n    jax.Array\n        Decoded tensor of shape (B, H', W', C) in NHWC format.\n    \"\"\"\n    for layer in self.conv_layers:\n        x = layer(x)\n    return x\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ConvNet","title":"ConvNet","text":"<pre><code>ConvNet(obs_shape, cfg, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Convolutional network (NHWC format).</p> <p>Parameters:</p> Name Type Description Default <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>Observation shape in (H, W, C) format.</p> required <code>cfg</code> <code>ConvNetConfig</code> <p>Configuration.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required Source code in <code>src/ml_networks/jax/vision.py</code> <pre><code>def __init__(\n    self,\n    obs_shape: tuple[int, int, int],\n    cfg: ConvNetConfig,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    self.cfg = cfg\n    self.obs_shape = obs_shape\n    in_channels = obs_shape[2]  # NHWC\n\n    layers: list[nnx.Module] = []\n    attn_layers: list[nnx.Module] = []\n    spatial_shape: tuple[int, int] = (obs_shape[0], obs_shape[1])\n\n    for _i, (ch, ks, st) in enumerate(\n        zip(\n            cfg.channels,\n            cfg.conv_cfg.kernel_size\n            if isinstance(cfg.conv_cfg.kernel_size, list)\n            else [cfg.conv_cfg.kernel_size] * len(cfg.channels),\n            cfg.conv_cfg.stride\n            if isinstance(cfg.conv_cfg.stride, list)\n            else [cfg.conv_cfg.stride] * len(cfg.channels),\n            strict=False,\n        ),\n    ):\n        conv_cfg_i = deepcopy(cfg.conv_cfg)\n        conv_cfg_i.kernel_size = ks\n        conv_cfg_i.stride = st\n        conv_cfg_i.padding = (ks - 1) // 2\n        layers.append(ConvNormActivation(in_channels, ch, conv_cfg_i, rngs=rngs))\n\n        if cfg.has_attn and cfg.nhead is not None:\n            attn_layers.append(Attention2d(ch, cfg.nhead, rngs=rngs))\n        else:\n            attn_layers.append(Identity())\n\n        spatial_shape = conv_out_shape(\n            spatial_shape,\n            padding=(ks - 1) // 2,\n            kernel_size=ks,\n            stride=st,\n        )\n        in_channels = ch\n\n    self.conv_layers = nnx.List(layers)\n    self.attn_layers = nnx.List(attn_layers)\n    self.output_spatial_shape = spatial_shape\n    self.output_channels = in_channels\n    self.output_dim = in_channels * spatial_shape[0] * spatial_shape[1]\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ConvNet-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.vision.ConvNet.attn_layers","title":"attn_layers  <code>instance-attribute</code>","text":"<pre><code>attn_layers = List(attn_layers)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ConvNet.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ConvNet.conv_layers","title":"conv_layers  <code>instance-attribute</code>","text":"<pre><code>conv_layers = List(layers)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ConvNet.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ConvNet.output_channels","title":"output_channels  <code>instance-attribute</code>","text":"<pre><code>output_channels = in_channels\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ConvNet.output_dim","title":"output_dim  <code>instance-attribute</code>","text":"<pre><code>output_dim = in_channels * spatial_shape[0] * spatial_shape[1]\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ConvNet.output_spatial_shape","title":"output_spatial_shape  <code>instance-attribute</code>","text":"<pre><code>output_spatial_shape = spatial_shape\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ConvNet-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.vision.ConvNet.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor of shape (B, H, W, C) in NHWC format.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Flattened tensor of shape (B, output_dim).</p> Source code in <code>src/ml_networks/jax/vision.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : jax.Array\n        Input tensor of shape (B, H, W, C) in NHWC format.\n\n    Returns\n    -------\n    jax.Array\n        Flattened tensor of shape (B, output_dim).\n    \"\"\"\n    for conv, attn in zip(self.conv_layers, self.attn_layers, strict=False):\n        x = conv(x)\n        x = attn(x)\n    return x.reshape(x.shape[0], -1)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ConvTranspose","title":"ConvTranspose","text":"<pre><code>ConvTranspose(obs_shape, cfg, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Transposed convolutional network (NHWC format).</p> <p>Parameters:</p> Name Type Description Default <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>Output observation shape in (H, W, C) format.</p> required <code>cfg</code> <code>ConvNetConfig</code> <p>Configuration (channels are in reverse).</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required Source code in <code>src/ml_networks/jax/vision.py</code> <pre><code>def __init__(\n    self,\n    obs_shape: tuple[int, int, int],\n    cfg: ConvNetConfig,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    self.cfg = cfg\n    self.obs_shape = obs_shape\n    out_channels = obs_shape[2]  # NHWC\n\n    channels = list(cfg.channels)\n\n    layers: list[nnx.Module] = []\n    for i in range(len(channels) - 1):\n        conv_cfg_i = deepcopy(cfg.conv_cfg)\n        layers.append(\n            ConvTransposeNormActivation(channels[i], channels[i + 1], conv_cfg_i, rngs=rngs),\n        )\n\n    # Final layer\n    final_cfg = deepcopy(cfg.conv_cfg)\n    final_cfg.activation = \"Identity\"\n    final_cfg.norm = \"none\"\n    layers.append(\n        ConvTransposeNormActivation(channels[-1], out_channels, final_cfg, rngs=rngs),\n    )\n\n    self.conv_layers = nnx.List(layers)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ConvTranspose-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.vision.ConvTranspose.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ConvTranspose.conv_layers","title":"conv_layers  <code>instance-attribute</code>","text":"<pre><code>conv_layers = List(layers)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ConvTranspose.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ConvTranspose-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.vision.ConvTranspose.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor of shape (B, H, W, C) in NHWC format.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Output tensor of shape (B, H', W', out_C) in NHWC format.</p> Source code in <code>src/ml_networks/jax/vision.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : jax.Array\n        Input tensor of shape (B, H, W, C) in NHWC format.\n\n    Returns\n    -------\n    jax.Array\n        Output tensor of shape (B, H', W', out_C) in NHWC format.\n    \"\"\"\n    for layer in self.conv_layers:\n        x = layer(x)\n    return x\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ResNetPixUnshuffle","title":"ResNetPixUnshuffle","text":"<pre><code>ResNetPixUnshuffle(obs_shape, cfg, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>ResNet with PixelUnshuffle downsampling (NHWC format).</p> <p>Parameters:</p> Name Type Description Default <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>Input observation shape in (H, W, C) format.</p> required <code>cfg</code> <code>ResNetConfig</code> <p>Configuration.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required Source code in <code>src/ml_networks/jax/vision.py</code> <pre><code>def __init__(\n    self,\n    obs_shape: tuple[int, int, int],\n    cfg: ResNetConfig,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    self.cfg = cfg\n    self.obs_shape = obs_shape\n    in_channels = obs_shape[2]\n\n    channels = list(cfg.channels)\n\n    layers: list[nnx.Module] = []\n    # Initial conv from input channels\n    init_cfg = ConvConfig(\n        activation=cfg.conv_cfg.activation,\n        kernel_size=3,\n        stride=1,\n        padding=1,\n        norm=cfg.conv_cfg.norm,\n        norm_cfg=cfg.conv_cfg.norm_cfg,\n    )\n    layers.append(ConvNormActivation(in_channels, channels[0], init_cfg, rngs=rngs))\n\n    for i in range(len(channels) - 1):\n        layers.append(\n            ResidualBlock(\n                channels[i],\n                kernel_size=cfg.conv_cfg.kernel_size,\n                activation=cfg.conv_cfg.activation,\n                norm=cfg.conv_cfg.norm,\n                norm_cfg=cfg.conv_cfg.norm_cfg or None,\n                dropout=cfg.conv_cfg.dropout,\n                rngs=rngs,\n            ),\n        )\n        conv_cfg_i = deepcopy(cfg.conv_cfg)\n        conv_cfg_i.scale_factor = -cfg.scale_factor\n        layers.append(\n            ConvNormActivation(channels[i], channels[i + 1], conv_cfg_i, rngs=rngs),\n        )\n\n    self.conv_layers = nnx.List(layers)\n    self.output_channels = channels[-1]\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ResNetPixUnshuffle-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.vision.ResNetPixUnshuffle.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ResNetPixUnshuffle.conv_layers","title":"conv_layers  <code>instance-attribute</code>","text":"<pre><code>conv_layers = List(layers)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ResNetPixUnshuffle.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ResNetPixUnshuffle.output_channels","title":"output_channels  <code>instance-attribute</code>","text":"<pre><code>output_channels = channels[-1]\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ResNetPixUnshuffle-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.vision.ResNetPixUnshuffle.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor of shape (B, H, W, C) in NHWC format.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Downsampled output.</p> Source code in <code>src/ml_networks/jax/vision.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : jax.Array\n        Input tensor of shape (B, H, W, C) in NHWC format.\n\n    Returns\n    -------\n    jax.Array\n        Downsampled output.\n    \"\"\"\n    for layer in self.conv_layers:\n        x = layer(x)\n    return x\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ResNetPixShuffle","title":"ResNetPixShuffle","text":"<pre><code>ResNetPixShuffle(obs_shape, cfg, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>ResNet with PixelShuffle upsampling (NHWC format).</p> <p>Parameters:</p> Name Type Description Default <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>Output observation shape in (H, W, C) format.</p> required <code>cfg</code> <code>ResNetConfig</code> <p>Configuration.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required Source code in <code>src/ml_networks/jax/vision.py</code> <pre><code>def __init__(\n    self,\n    obs_shape: tuple[int, int, int],\n    cfg: ResNetConfig,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    self.cfg = cfg\n    self.obs_shape = obs_shape\n    out_channels = obs_shape[2]\n\n    channels = list(cfg.channels)\n\n    layers: list[nnx.Module] = []\n    for i in range(len(channels) - 1):\n        conv_cfg_i = deepcopy(cfg.conv_cfg)\n        conv_cfg_i.scale_factor = cfg.scale_factor\n        layers.extend((\n            ConvNormActivation(channels[i], channels[i + 1], conv_cfg_i, rngs=rngs),\n            ResidualBlock(\n                channels[i + 1],\n                kernel_size=cfg.conv_cfg.kernel_size,\n                activation=cfg.conv_cfg.activation,\n                norm=cfg.conv_cfg.norm,\n                norm_cfg=cfg.conv_cfg.norm_cfg or None,\n                dropout=cfg.conv_cfg.dropout,\n                rngs=rngs,\n            ),\n        ))\n\n    # Final conv to output channels\n    final_cfg = ConvConfig(\n        activation=\"Identity\",\n        kernel_size=3,\n        stride=1,\n        padding=1,\n        norm=\"none\",\n    )\n    layers.append(ConvNormActivation(channels[-1], out_channels, final_cfg, rngs=rngs))\n\n    self.conv_layers = nnx.List(layers)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ResNetPixShuffle-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.vision.ResNetPixShuffle.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ResNetPixShuffle.conv_layers","title":"conv_layers  <code>instance-attribute</code>","text":"<pre><code>conv_layers = List(layers)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ResNetPixShuffle.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ResNetPixShuffle-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.vision.ResNetPixShuffle.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor of shape (B, H, W, C) in NHWC format.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Upsampled output.</p> Source code in <code>src/ml_networks/jax/vision.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : jax.Array\n        Input tensor of shape (B, H, W, C) in NHWC format.\n\n    Returns\n    -------\n    jax.Array\n        Upsampled output.\n    \"\"\"\n    for layer in self.conv_layers:\n        x = layer(x)\n    return x\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ViT","title":"ViT","text":"<pre><code>ViT(obs_shape, cfg, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Vision Transformer (NHWC format).</p> <p>Parameters:</p> Name Type Description Default <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>Observation shape in (H, W, C) format.</p> required <code>cfg</code> <code>ViTConfig</code> <p>ViT configuration.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required Source code in <code>src/ml_networks/jax/vision.py</code> <pre><code>def __init__(\n    self,\n    obs_shape: tuple[int, int, int],\n    cfg: ViTConfig,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    self.cfg = cfg\n    self.obs_shape = obs_shape  # (H, W, C)\n\n    self.patch_embed = PatchEmbed(cfg.embed_dim, cfg.patch_size, obs_shape, rngs=rngs)\n    num_patches = self.patch_embed.patch_num\n\n    # CLS token\n    self.cls_token = nnx.Param(jax.random.normal(rngs(), (1, 1, cfg.embed_dim)) * 0.02)\n\n    # Position embedding\n    self.pos_embed = nnx.Param(jax.random.normal(rngs(), (1, num_patches + 1, cfg.embed_dim)) * 0.02)\n\n    # Transformer blocks (simple self-attention blocks)\n    blocks = [_ViTBlock(cfg.embed_dim, cfg.num_heads, cfg.mlp_ratio, rngs=rngs) for _ in range(cfg.depth)]\n    self.blocks = nnx.List(blocks)\n\n    self.norm = nnx.LayerNorm(num_features=cfg.embed_dim, rngs=rngs)\n    self.output_dim = cfg.embed_dim\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ViT-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.vision.ViT.blocks","title":"blocks  <code>instance-attribute</code>","text":"<pre><code>blocks = List(blocks)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ViT.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ViT.cls_token","title":"cls_token  <code>instance-attribute</code>","text":"<pre><code>cls_token = Param(normal(rngs(), (1, 1, embed_dim)) * 0.02)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ViT.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = LayerNorm(num_features=embed_dim, rngs=rngs)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ViT.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ViT.output_dim","title":"output_dim  <code>instance-attribute</code>","text":"<pre><code>output_dim = embed_dim\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ViT.patch_embed","title":"patch_embed  <code>instance-attribute</code>","text":"<pre><code>patch_embed = PatchEmbed(embed_dim, patch_size, obs_shape, rngs=rngs)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ViT.pos_embed","title":"pos_embed  <code>instance-attribute</code>","text":"<pre><code>pos_embed = Param(normal(rngs(), (1, num_patches + 1, embed_dim)) * 0.02)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.vision.ViT-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.vision.ViT.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor of shape (B, H, W, C) in NHWC format.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>CLS token output of shape (B, embed_dim).</p> Source code in <code>src/ml_networks/jax/vision.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : jax.Array\n        Input tensor of shape (B, H, W, C) in NHWC format.\n\n    Returns\n    -------\n    jax.Array\n        CLS token output of shape (B, embed_dim).\n    \"\"\"\n    # (B, H, W, C) -&gt; (B, Np, D)\n    x = self.patch_embed(x)\n    b = x.shape[0]\n\n    # Prepend CLS token\n    cls_tokens = jnp.broadcast_to(self.cls_token.value, (b, 1, self.cfg.embed_dim))\n    x = jnp.concatenate([cls_tokens, x], axis=1)\n\n    # Add position embedding\n    x = x + self.pos_embed.value\n\n    for block in self.blocks:\n        x = block(x)\n\n    x = self.norm(x)\n    return x[:, 0]  # CLS token\n</code></pre>"},{"location":"api/jax/#ml_networksjaxdistributions","title":"\u5206\u5e03 (<code>ml_networks.jax.distributions</code>)","text":""},{"location":"api/jax/#ml_networks.jax.distributions.Distribution","title":"Distribution","text":"<pre><code>Distribution(in_dim, dist, n_groups=1, spherical=False, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A distribution function.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input dimension.</p> required <code>dist</code> <code>Literal['normal', 'categorical', 'bernoulli']</code> <p>Distribution type.</p> required <code>n_groups</code> <code>int</code> <p>Number of groups. Default is 1.</p> <code>1</code> <code>spherical</code> <code>bool</code> <p>Whether to project samples to the unit sphere. Default is False.</p> <code>False</code> <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def __init__(\n    self,\n    in_dim: int,\n    dist: Literal[\"normal\", \"categorical\", \"bernoulli\"],\n    n_groups: int = 1,\n    spherical: bool = False,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    self.dist_type = dist\n    self.spherical = spherical\n    self.n_class = in_dim // n_groups\n    self.in_dim = in_dim\n    self.n_groups = n_groups\n    self.rngs = rngs\n\n    if spherical:\n        self.codebook = BSQCodebook(self.n_class)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.Distribution-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.distributions.Distribution.codebook","title":"codebook  <code>instance-attribute</code>","text":"<pre><code>codebook = BSQCodebook(n_class)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.Distribution.dist_type","title":"dist_type  <code>instance-attribute</code>","text":"<pre><code>dist_type = dist\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.Distribution.in_dim","title":"in_dim  <code>instance-attribute</code>","text":"<pre><code>in_dim = in_dim\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.Distribution.n_class","title":"n_class  <code>instance-attribute</code>","text":"<pre><code>n_class = in_dim // n_groups\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.Distribution.n_groups","title":"n_groups  <code>instance-attribute</code>","text":"<pre><code>n_groups = n_groups\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.Distribution.rngs","title":"rngs  <code>instance-attribute</code>","text":"<pre><code>rngs = rngs\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.Distribution.spherical","title":"spherical  <code>instance-attribute</code>","text":"<pre><code>spherical = spherical\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.Distribution-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.distributions.Distribution.__call__","title":"__call__","text":"<pre><code>__call__(x, deterministic=False, inv_tmp=1.0)\n</code></pre> <p>Compute the posterior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor.</p> required <code>deterministic</code> <code>bool</code> <p>Whether to use the deterministic mode. Default is False.</p> <code>False</code> <code>inv_tmp</code> <code>float</code> <p>Inverse temperature. Default is 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>StochState</code> <p>Posterior distribution.</p> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def __call__(\n    self,\n    x: jax.Array,\n    deterministic: bool = False,\n    inv_tmp: float = 1.0,\n) -&gt; StochState:\n    \"\"\"\n    Compute the posterior distribution.\n\n    Parameters\n    ----------\n    x : jax.Array\n        Input tensor.\n    deterministic : bool, optional\n        Whether to use the deterministic mode. Default is False.\n    inv_tmp : float, optional\n        Inverse temperature. Default is 1.0.\n\n    Returns\n    -------\n    StochState\n        Posterior distribution.\n    \"\"\"\n    if self.dist_type == \"normal\":\n        return self.normal(x, deterministic=deterministic, inv_tmp=inv_tmp)\n    if self.dist_type == \"categorical\":\n        return self.categorical(x, deterministic=deterministic, inv_tmp=inv_tmp)\n    if self.dist_type == \"bernoulli\":\n        return self.bernoulli(x, deterministic=deterministic, inv_tmp=inv_tmp)\n    raise NotImplementedError\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.Distribution.bernoulli","title":"bernoulli","text":"<pre><code>bernoulli(logits, deterministic=False, inv_tmp=1.0)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def bernoulli(self, logits: jax.Array, deterministic: bool = False, inv_tmp: float = 1.0) -&gt; BernoulliStoch:\n    batch_shape = logits.shape[:-1]\n    chunked_logits = jnp.split(logits, self.n_groups, axis=-1)\n    logits_stacked = jnp.stack(chunked_logits, axis=-2)\n    logits_stacked = logits_stacked * inv_tmp\n    probs = jax.nn.sigmoid(logits_stacked)\n\n    key = self.rngs()\n    # Bernoulli sampling with straight-through\n    u = jax.random.uniform(key, probs.shape)\n    sample = (u &lt; probs).astype(jnp.float32)\n    # Straight-through estimator\n    sample = sample + probs - jax.lax.stop_gradient(probs)\n\n    if self.spherical:\n        sample = self.codebook.bits_to_codes(sample)\n\n    if deterministic:\n        sample = (\n            jnp.where(sample &gt; 0.5, jnp.ones_like(sample), jnp.zeros_like(sample))\n            + probs\n            - jax.lax.stop_gradient(probs)\n        )\n\n    return BernoulliStoch(\n        logits_stacked,\n        probs,\n        sample.reshape(*batch_shape, -1),\n    )\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.Distribution.categorical","title":"categorical","text":"<pre><code>categorical(logits, deterministic=False, inv_tmp=1.0)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def categorical(self, logits: jax.Array, deterministic: bool = False, inv_tmp: float = 1.0) -&gt; CategoricalStoch:\n    batch_shape = logits.shape[:-1]\n    logits_chunks = jnp.split(logits, self.n_groups, axis=-1)\n    logits_stacked = jnp.stack(logits_chunks, axis=-2)\n    probs = softmax(logits_stacked, axis=-1, temperature=1 / inv_tmp)\n\n    key = self.rngs()\n    # Sample using Gumbel-max trick for one-hot with straight-through\n    gumbel_noise = -jnp.log(-jnp.log(jax.random.uniform(key, probs.shape, minval=1e-10, maxval=1.0)))\n    y = jnp.log(probs + 1e-10) + gumbel_noise\n    hard = jax.nn.one_hot(jnp.argmax(y, axis=-1), self.n_class)\n    sample = hard + probs - jax.lax.stop_gradient(probs)\n\n    if self.spherical:\n        sample = sample * 2 - 1\n\n    if deterministic:\n        stoch = self.deterministic_onehot(probs).reshape(*batch_shape, -1)\n    else:\n        stoch = sample.reshape(*batch_shape, -1)\n\n    return CategoricalStoch(logits_stacked, probs, stoch)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.Distribution.deterministic_onehot","title":"deterministic_onehot","text":"<pre><code>deterministic_onehot(input)\n</code></pre> <p>Compute the one-hot vector by argmax with straight-through.</p> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def deterministic_onehot(self, input: jax.Array) -&gt; jax.Array:\n    \"\"\"Compute the one-hot vector by argmax with straight-through.\"\"\"\n    hard = jax.nn.one_hot(jnp.argmax(input, axis=-1), self.n_class)\n    return hard + input - jax.lax.stop_gradient(input)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.Distribution.normal","title":"normal","text":"<pre><code>normal(mu_std, deterministic=False, inv_tmp=1.0)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def normal(self, mu_std: jax.Array, deterministic: bool = False, inv_tmp: float = 1.0) -&gt; NormalStoch:\n    assert mu_std.shape[-1] == self.in_dim * 2, (\n        f\"mu_std.shape[-1] {mu_std.shape[-1]} and in_dim {self.in_dim} must be the same.\"\n    )\n\n    mu, std = jnp.split(mu_std, 2, axis=-1)\n    std = jax.nn.softplus(std) + 1e-6\n\n    if deterministic:\n        sample = mu\n    else:\n        key = self.rngs()\n        sample = mu + std * jax.random.normal(key, mu.shape)\n\n    return NormalStoch(mu, std, sample)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.NormalStoch","title":"NormalStoch  <code>dataclass</code>","text":"<pre><code>NormalStoch(mean, std, stoch)\n</code></pre> <p>Parameters of a normal distribution and its stochastic sample.</p>"},{"location":"api/jax/#ml_networks.jax.distributions.NormalStoch-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.distributions.NormalStoch.mean","title":"mean  <code>instance-attribute</code>","text":"<pre><code>mean\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.NormalStoch.shape","title":"shape  <code>property</code>","text":"<pre><code>shape\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.NormalStoch.std","title":"std  <code>instance-attribute</code>","text":"<pre><code>std\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.NormalStoch.stoch","title":"stoch  <code>instance-attribute</code>","text":"<pre><code>stoch\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.NormalStoch-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.distributions.NormalStoch.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def __getitem__(self, idx: int | slice | tuple) -&gt; NormalStoch:\n    return NormalStoch(self.mean[idx], self.std[idx], self.stoch[idx])\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.NormalStoch.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def __len__(self) -&gt; int:\n    return self.stoch.shape[0]\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.NormalStoch.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.mean.shape != self.std.shape:\n        msg = f\"mean.shape {self.mean.shape} and std.shape {self.std.shape} must be the same.\"\n        raise ValueError(msg)\n    if (self.std &lt; 0).any():\n        msg = \"std must be non-negative.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.NormalStoch.detach","title":"detach","text":"<pre><code>detach()\n</code></pre> <p>Stop gradient equivalent.</p> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def detach(self) -&gt; NormalStoch:\n    \"\"\"Stop gradient equivalent.\"\"\"\n    return NormalStoch(\n        jax.lax.stop_gradient(self.mean),\n        jax.lax.stop_gradient(self.std),\n        jax.lax.stop_gradient(self.stoch),\n    )\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.NormalStoch.flatten","title":"flatten","text":"<pre><code>flatten(start_dim=0, end_dim=-1)\n</code></pre> <p>Flatten along specified dimensions.</p> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def flatten(self, start_dim: int = 0, end_dim: int = -1) -&gt; NormalStoch:\n    \"\"\"Flatten along specified dimensions.\"\"\"\n    ndim = self.mean.ndim\n    if end_dim &lt; 0:\n        end_dim = ndim + end_dim\n    new_shape = (*self.mean.shape[:start_dim], -1, *self.mean.shape[end_dim + 1 :])\n    return self.reshape(*new_shape)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.NormalStoch.get_distribution","title":"get_distribution","text":"<pre><code>get_distribution(independent=1)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def get_distribution(self, independent: int = 1) -&gt; distrax.Distribution:\n    return distrax.Independent(distrax.Normal(self.mean, self.std), independent)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.NormalStoch.reshape","title":"reshape","text":"<pre><code>reshape(*shape)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def reshape(self, *shape: int) -&gt; NormalStoch:\n    return NormalStoch(\n        self.mean.reshape(*shape),\n        self.std.reshape(*shape),\n        self.stoch.reshape(*shape),\n    )\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.NormalStoch.save","title":"save","text":"<pre><code>save(path)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    os.makedirs(path, exist_ok=True)\n    save_blosc2(f\"{path}/mean.blosc2\", np.asarray(self.mean))\n    save_blosc2(f\"{path}/std.blosc2\", np.asarray(self.std))\n    save_blosc2(f\"{path}/stoch.blosc2\", np.asarray(self.stoch))\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.CategoricalStoch","title":"CategoricalStoch  <code>dataclass</code>","text":"<pre><code>CategoricalStoch(logits, probs, stoch)\n</code></pre> <p>Parameters of a categorical distribution and its stochastic sample.</p>"},{"location":"api/jax/#ml_networks.jax.distributions.CategoricalStoch-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.distributions.CategoricalStoch.logits","title":"logits  <code>instance-attribute</code>","text":"<pre><code>logits\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.CategoricalStoch.probs","title":"probs  <code>instance-attribute</code>","text":"<pre><code>probs\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.CategoricalStoch.shape","title":"shape  <code>property</code>","text":"<pre><code>shape\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.CategoricalStoch.stoch","title":"stoch  <code>instance-attribute</code>","text":"<pre><code>stoch\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.CategoricalStoch-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.distributions.CategoricalStoch.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def __getitem__(self, idx: int | slice | tuple) -&gt; CategoricalStoch:\n    return CategoricalStoch(self.logits[idx], self.probs[idx], self.stoch[idx])\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.CategoricalStoch.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def __len__(self) -&gt; int:\n    return self.stoch.shape[0]\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.CategoricalStoch.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.logits.shape != self.probs.shape:\n        msg = f\"logits.shape {self.logits.shape} and probs.shape {self.probs.shape} must be the same.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.CategoricalStoch.detach","title":"detach","text":"<pre><code>detach()\n</code></pre> <p>Stop gradient equivalent.</p> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def detach(self) -&gt; CategoricalStoch:\n    \"\"\"Stop gradient equivalent.\"\"\"\n    return CategoricalStoch(\n        jax.lax.stop_gradient(self.logits),\n        jax.lax.stop_gradient(self.probs),\n        jax.lax.stop_gradient(self.stoch),\n    )\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.CategoricalStoch.get_distribution","title":"get_distribution","text":"<pre><code>get_distribution(independent=1)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def get_distribution(self, independent: int = 1) -&gt; distrax.Distribution:\n    return distrax.Independent(\n        distrax.OneHotCategorical(probs=self.probs),\n        independent,\n    )\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.CategoricalStoch.reshape","title":"reshape","text":"<pre><code>reshape(*shape)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def reshape(self, *shape: int) -&gt; CategoricalStoch:\n    return CategoricalStoch(\n        self.logits.reshape(*shape),\n        self.probs.reshape(*shape),\n        self.stoch.reshape(*shape),\n    )\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.CategoricalStoch.save","title":"save","text":"<pre><code>save(path)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    os.makedirs(path, exist_ok=True)\n    save_blosc2(f\"{path}/logits.blosc2\", np.asarray(self.logits))\n    save_blosc2(f\"{path}/probs.blosc2\", np.asarray(self.probs))\n    save_blosc2(f\"{path}/stoch.blosc2\", np.asarray(self.stoch))\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.CategoricalStoch.squeeze","title":"squeeze","text":"<pre><code>squeeze(axis)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def squeeze(self, axis: int) -&gt; CategoricalStoch:\n    return CategoricalStoch(\n        jnp.squeeze(self.logits, axis=axis),\n        jnp.squeeze(self.probs, axis=axis),\n        jnp.squeeze(self.stoch, axis=axis),\n    )\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.BernoulliStoch","title":"BernoulliStoch  <code>dataclass</code>","text":"<pre><code>BernoulliStoch(logits, probs, stoch)\n</code></pre> <p>Parameters of a Bernoulli distribution and its stochastic sample.</p>"},{"location":"api/jax/#ml_networks.jax.distributions.BernoulliStoch-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.distributions.BernoulliStoch.logits","title":"logits  <code>instance-attribute</code>","text":"<pre><code>logits\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.BernoulliStoch.probs","title":"probs  <code>instance-attribute</code>","text":"<pre><code>probs\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.BernoulliStoch.shape","title":"shape  <code>property</code>","text":"<pre><code>shape\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.BernoulliStoch.stoch","title":"stoch  <code>instance-attribute</code>","text":"<pre><code>stoch\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.BernoulliStoch-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.distributions.BernoulliStoch.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def __getitem__(self, idx: int | slice | tuple) -&gt; BernoulliStoch:\n    return BernoulliStoch(self.logits[idx], self.probs[idx], self.stoch[idx])\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.BernoulliStoch.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def __len__(self) -&gt; int:\n    return self.stoch.shape[0]\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.BernoulliStoch.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.logits.shape != self.probs.shape:\n        msg = f\"logits.shape {self.logits.shape} and probs.shape {self.probs.shape} must be the same.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.BernoulliStoch.detach","title":"detach","text":"<pre><code>detach()\n</code></pre> <p>Stop gradient equivalent.</p> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def detach(self) -&gt; BernoulliStoch:\n    \"\"\"Stop gradient equivalent.\"\"\"\n    return BernoulliStoch(\n        jax.lax.stop_gradient(self.logits),\n        jax.lax.stop_gradient(self.probs),\n        jax.lax.stop_gradient(self.stoch),\n    )\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.BernoulliStoch.get_distribution","title":"get_distribution","text":"<pre><code>get_distribution(independent=1)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def get_distribution(self, independent: int = 1) -&gt; distrax.Distribution:\n    return distrax.Independent(distrax.Bernoulli(probs=self.probs), independent)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.BernoulliStoch.reshape","title":"reshape","text":"<pre><code>reshape(*shape)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def reshape(self, *shape: int) -&gt; BernoulliStoch:\n    return BernoulliStoch(\n        self.logits.reshape(*shape),\n        self.probs.reshape(*shape),\n        self.stoch.reshape(*shape),\n    )\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.BernoulliStoch.save","title":"save","text":"<pre><code>save(path)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    os.makedirs(path, exist_ok=True)\n    save_blosc2(f\"{path}/logits.blosc2\", np.asarray(self.logits))\n    save_blosc2(f\"{path}/probs.blosc2\", np.asarray(self.probs))\n    save_blosc2(f\"{path}/stoch.blosc2\", np.asarray(self.stoch))\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.distributions.BernoulliStoch.squeeze","title":"squeeze","text":"<pre><code>squeeze(axis)\n</code></pre> Source code in <code>src/ml_networks/jax/distributions.py</code> <pre><code>def squeeze(self, axis: int) -&gt; BernoulliStoch:\n    return BernoulliStoch(\n        jnp.squeeze(self.logits, axis=axis),\n        jnp.squeeze(self.probs, axis=axis),\n        jnp.squeeze(self.stoch, axis=axis),\n    )\n</code></pre>"},{"location":"api/jax/#ml_networksjaxloss","title":"\u640d\u5931\u95a2\u6570 (<code>ml_networks.jax.loss</code>)","text":""},{"location":"api/jax/#ml_networks.jax.loss.focal_loss","title":"focal_loss","text":"<pre><code>focal_loss(prediction, target, gamma=2.0, sum_axis=-1)\n</code></pre> <p>Focal loss function. Mainly for multi-class classification.</p> Reference <p>Focal Loss for Dense Object Detection https://arxiv.org/abs/1708.02002</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Array</code> <p>The predicted tensor. This should be before softmax.</p> required <code>target</code> <code>Array</code> <p>The target tensor (integer class labels).</p> required <code>gamma</code> <code>float</code> <p>The gamma parameter. Default is 2.0.</p> <code>2.0</code> <code>sum_axis</code> <code>int</code> <p>The axis to sum the loss. Default is -1.</p> <code>-1</code> <p>Returns:</p> Type Description <code>Array</code> <p>The focal loss.</p> Source code in <code>src/ml_networks/jax/loss.py</code> <pre><code>def focal_loss(\n    prediction: jax.Array,\n    target: jax.Array,\n    gamma: float = 2.0,\n    sum_axis: int = -1,\n) -&gt; jax.Array:\n    \"\"\"\n    Focal loss function. Mainly for multi-class classification.\n\n    Reference\n    ---------\n    Focal Loss for Dense Object Detection\n    https://arxiv.org/abs/1708.02002\n\n    Parameters\n    ----------\n    prediction : jax.Array\n        The predicted tensor. This should be before softmax.\n    target : jax.Array\n        The target tensor (integer class labels).\n    gamma : float\n        The gamma parameter. Default is 2.0.\n    sum_axis : int\n        The axis to sum the loss. Default is -1.\n\n    Returns\n    -------\n    jax.Array\n        The focal loss.\n    \"\"\"\n    # Rearrange: unsqueeze(1), transpose sum_axis with 1, squeeze(-1)\n    prediction = jnp.expand_dims(prediction, axis=1)\n    prediction = jnp.moveaxis(prediction, sum_axis, 1)\n    prediction = jnp.squeeze(prediction, axis=-1)\n\n    if gamma:\n        log_prob = jax.nn.log_softmax(prediction, axis=1)\n        prob = jnp.exp(log_prob)\n        # nll_loss equivalent: -log_prob[target]\n        n_classes = prediction.shape[1]\n        target_one_hot = jax.nn.one_hot(target, n_classes)\n        loss = -jnp.sum(((1 - prob) ** gamma) * log_prob * target_one_hot, axis=1)\n    else:\n        # Cross entropy\n        n_classes = prediction.shape[1]\n        target_one_hot = jax.nn.one_hot(target, n_classes)\n        log_prob = jax.nn.log_softmax(prediction, axis=1)\n        loss = -jnp.sum(log_prob * target_one_hot, axis=1)\n    return loss.mean(axis=0).sum()\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.loss.binary_focal_loss","title":"binary_focal_loss","text":"<pre><code>binary_focal_loss(prediction, target, gamma=2.0, sum_axis=-1)\n</code></pre> <p>Binary focal loss function. Mainly for binary classification.</p> Reference <p>Focal Loss for Dense Object Detection https://arxiv.org/abs/1708.02002</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Array</code> <p>The predicted tensor. This should be before sigmoid.</p> required <code>target</code> <code>Array</code> <p>The target tensor.</p> required <code>gamma</code> <code>float</code> <p>The gamma parameter. Default is 2.0.</p> <code>2.0</code> <code>sum_axis</code> <code>int</code> <p>The axis to sum the loss. Default is -1.</p> <code>-1</code> <p>Returns:</p> Type Description <code>Array</code> <p>The binary focal loss.</p> Source code in <code>src/ml_networks/jax/loss.py</code> <pre><code>def binary_focal_loss(\n    prediction: jax.Array,\n    target: jax.Array,\n    gamma: float = 2.0,\n    sum_axis: int = -1,\n) -&gt; jax.Array:\n    \"\"\"\n    Binary focal loss function. Mainly for binary classification.\n\n    Reference\n    ---------\n    Focal Loss for Dense Object Detection\n    https://arxiv.org/abs/1708.02002\n\n    Parameters\n    ----------\n    prediction : jax.Array\n        The predicted tensor. This should be before sigmoid.\n    target : jax.Array\n        The target tensor.\n    gamma : float\n        The gamma parameter. Default is 2.0.\n    sum_axis : int\n        The axis to sum the loss. Default is -1.\n\n    Returns\n    -------\n    jax.Array\n        The binary focal loss.\n    \"\"\"\n    if gamma:\n        log_probs = jax.nn.log_sigmoid(prediction)\n        neg_log_probs = jax.nn.log_sigmoid(-prediction)\n        probs = jax.nn.sigmoid(prediction)\n        focal_weight = jnp.where(target == 1, (1 - probs) ** gamma, probs**gamma)\n        loss = jnp.where(target == 1, -log_probs, -neg_log_probs)\n        loss = focal_weight * loss\n    else:\n        # Binary cross entropy with logits\n        loss = jnp.maximum(prediction, 0) - prediction * target + jnp.log(1 + jnp.exp(-jnp.abs(prediction)))\n    return loss.sum(axis=sum_axis)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.loss.charbonnier","title":"charbonnier","text":"<pre><code>charbonnier(prediction, target, epsilon=0.001, alpha=1, sum_axes=None)\n</code></pre> <p>Charbonnier loss function.</p> Reference <p>A General and Adaptive Robust Loss Function http://arxiv.org/abs/1701.03077</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Array</code> <p>The predicted tensor.</p> required <code>target</code> <code>Array</code> <p>The target tensor.</p> required <code>epsilon</code> <code>float</code> <p>A small value to avoid division by zero. Default is 1e-3.</p> <code>0.001</code> <code>alpha</code> <code>float</code> <p>The alpha parameter. Default is 1.</p> <code>1</code> <code>sum_axes</code> <code>int | list[int] | tuple[int, ...] | None</code> <p>The axes to sum the loss. Default is None (sums over [-1, -2, -3]).</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>The Charbonnier loss.</p> Source code in <code>src/ml_networks/jax/loss.py</code> <pre><code>def charbonnier(\n    prediction: jax.Array,\n    target: jax.Array,\n    epsilon: float = 1e-3,\n    alpha: float = 1,\n    sum_axes: int | list[int] | tuple[int, ...] | None = None,\n) -&gt; jax.Array:\n    \"\"\"\n    Charbonnier loss function.\n\n    Reference\n    ---------\n    A General and Adaptive Robust Loss Function\n    http://arxiv.org/abs/1701.03077\n\n    Parameters\n    ----------\n    prediction : jax.Array\n        The predicted tensor.\n    target : jax.Array\n        The target tensor.\n    epsilon : float\n        A small value to avoid division by zero. Default is 1e-3.\n    alpha : float\n        The alpha parameter. Default is 1.\n    sum_axes : int | list[int] | tuple[int, ...] | None\n        The axes to sum the loss. Default is None (sums over [-1, -2, -3]).\n\n    Returns\n    -------\n    jax.Array\n        The Charbonnier loss.\n    \"\"\"\n    if sum_axes is None:\n        sum_axes = [-1, -2, -3]\n    x = prediction - target\n    loss = (x**2 + epsilon**2) ** (alpha / 2)\n    return jnp.sum(loss, axis=sum_axes)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.loss.kl_divergence","title":"kl_divergence","text":"<pre><code>kl_divergence(posterior, prior)\n</code></pre> <p>KL divergence between two StochState distributions.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>StochState</code> <p>The posterior distribution.</p> required <code>prior</code> <code>StochState</code> <p>The prior distribution.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>The KL divergence.</p> Source code in <code>src/ml_networks/jax/loss.py</code> <pre><code>def kl_divergence(posterior: StochState, prior: StochState) -&gt; jax.Array:\n    \"\"\"KL divergence between two StochState distributions.\n\n    Parameters\n    ----------\n    posterior : StochState\n        The posterior distribution.\n    prior : StochState\n        The prior distribution.\n\n    Returns\n    -------\n    jax.Array\n        The KL divergence.\n    \"\"\"\n    return posterior.get_distribution().kl_divergence(prior.get_distribution())\n</code></pre>"},{"location":"api/jax/#ml_networksjaxactivations","title":"\u6d3b\u6027\u5316\u95a2\u6570 (<code>ml_networks.jax.activations</code>)","text":""},{"location":"api/jax/#ml_networks.jax.activations.Activation","title":"Activation","text":"<pre><code>Activation(activation, **kwargs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Generic activation function.</p> Source code in <code>src/ml_networks/jax/activations.py</code> <pre><code>def __init__(self, activation: str, **kwargs: Any) -&gt; None:\n    if \"glu\" not in activation.lower():\n        kwargs.pop(\"dim\", None)\n\n    builtin_activations: dict[str, Any] = {\n        \"ReLU\": jax.nn.relu,\n        \"GELU\": jax.nn.gelu,\n        \"GeLU\": jax.nn.gelu,\n        \"SiLU\": jax.nn.silu,\n        \"Tanh\": jnp.tanh,\n        \"Sigmoid\": jax.nn.sigmoid,\n        \"ELU\": jax.nn.elu,\n        \"LeakyReLU\": jax.nn.leaky_relu,\n        \"Mish\": lambda x: x * jnp.tanh(jax.nn.softplus(x)),\n        \"Softplus\": jax.nn.softplus,\n        \"Identity\": lambda x: x,\n    }\n\n    if activation in builtin_activations:\n        self._fn = builtin_activations[activation]\n        self._module: nnx.Module | None = None\n    elif activation == \"TanhExp\":\n        self._fn = None\n        self._module = TanhExp()\n    elif activation == \"REReLU\":\n        self._fn = None\n        self._module = REReLU(**kwargs)\n    elif activation in {\"SiGLU\", \"SwiGLU\"}:\n        self._fn = None\n        self._module = SiGLU(**kwargs)\n    elif activation == \"CRReLU\":\n        self._fn = None\n        self._module = CRReLU(**kwargs)\n    elif activation == \"L2Norm\":\n        self._fn = None\n        self._module = L2Norm()\n    else:\n        msg = f\"Activation: '{activation}' is not implemented yet.\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.activations.Activation-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.activations.Activation.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> Source code in <code>src/ml_networks/jax/activations.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    if self._module is not None:\n        return self._module(x)\n    return self._fn(x)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.activations.REReLU","title":"REReLU","text":"<pre><code>REReLU(reparametarize_fn='gelu')\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Reparametarized ReLU activation function. This backward pass is differentiable.</p> References <p>https://openreview.net/forum?id=lNCnZwcH5Z</p> <p>Parameters:</p> Name Type Description Default <code>reparametarize_fn</code> <code>str</code> <p>Reparametarization function. Default is GELU.</p> <code>'gelu'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rerelu = REReLU()\n&gt;&gt;&gt; x = jnp.array([[1.0, -1.0, 0.5]])\n&gt;&gt;&gt; output = rerelu(x)\n&gt;&gt;&gt; output.shape\n(1, 3)\n</code></pre> Source code in <code>src/ml_networks/jax/activations.py</code> <pre><code>def __init__(self, reparametarize_fn: str = \"gelu\") -&gt; None:\n    reparam_fns: dict[str, Any] = {\n        \"gelu\": jax.nn.gelu,\n        \"relu\": jax.nn.relu,\n        \"silu\": jax.nn.silu,\n        \"elu\": jax.nn.elu,\n    }\n    reparametarize_fn = reparametarize_fn.lower()\n    if reparametarize_fn not in reparam_fns:\n        msg = f\"Reparametarization function '{reparametarize_fn}' is not supported.\"\n        raise ValueError(msg)\n    self.reparametarize_fn = reparam_fns[reparametarize_fn]\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.activations.REReLU-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.activations.REReLU.reparametarize_fn","title":"reparametarize_fn  <code>instance-attribute</code>","text":"<pre><code>reparametarize_fn = reparam_fns[reparametarize_fn]\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.activations.REReLU-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.activations.REReLU.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> Source code in <code>src/ml_networks/jax/activations.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    return (\n        jax.lax.stop_gradient(jax.nn.relu(x))\n        + self.reparametarize_fn(x)\n        - jax.lax.stop_gradient(self.reparametarize_fn(x))\n    )\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.activations.SiGLU","title":"SiGLU","text":"<pre><code>SiGLU(dim=-1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>SiGLU activation function.</p> <p>This is equivalent to SwiGLU (Swish variant of Gated Linear Unit) activation function.</p> References <p>https://arxiv.org/abs/2102.11972</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension to split the tensor. Default is -1.</p> <code>-1</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; siglu = SiGLU()\n&gt;&gt;&gt; x = jnp.ones((1, 4))\n&gt;&gt;&gt; output = siglu(x)\n&gt;&gt;&gt; output.shape\n(1, 2)\n</code></pre> <pre><code>&gt;&gt;&gt; siglu = SiGLU(dim=0)\n&gt;&gt;&gt; x = jnp.ones((4, 1))\n&gt;&gt;&gt; output = siglu(x)\n&gt;&gt;&gt; output.shape\n(2, 1)\n</code></pre> Source code in <code>src/ml_networks/jax/activations.py</code> <pre><code>def __init__(self, dim: int = -1) -&gt; None:\n    self.dim = dim\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.activations.SiGLU-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.activations.SiGLU.dim","title":"dim  <code>instance-attribute</code>","text":"<pre><code>dim = dim\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.activations.SiGLU-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.activations.SiGLU.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> Source code in <code>src/ml_networks/jax/activations.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    x1, x2 = jnp.split(x, 2, axis=self.dim)\n    return x1 * jax.nn.silu(x2)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.activations.CRReLU","title":"CRReLU","text":"<pre><code>CRReLU(lr=0.01)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Correction Regularized ReLU activation function. This is a variant of ReLU activation function.</p> References <p>https://openreview.net/forum?id=7TZYM6Hm9p</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>Learning rate. Default is 0.01.</p> <code>0.01</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; crrelu = CRReLU()\n&gt;&gt;&gt; x = jnp.array([[1.0, -1.0, 0.5]])\n&gt;&gt;&gt; output = crrelu(x)\n&gt;&gt;&gt; output.shape\n(1, 3)\n</code></pre> Source code in <code>src/ml_networks/jax/activations.py</code> <pre><code>def __init__(self, lr: float = 0.01) -&gt; None:\n    self.lr = nnx.Param(jnp.array(lr, dtype=jnp.float32))\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.activations.CRReLU-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.activations.CRReLU.lr","title":"lr  <code>instance-attribute</code>","text":"<pre><code>lr = Param(array(lr, dtype=float32))\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.activations.CRReLU-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.activations.CRReLU.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> Source code in <code>src/ml_networks/jax/activations.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    return jax.nn.relu(x) + self.lr.value * x * jnp.exp(-(x**2) / 2)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.activations.TanhExp","title":"TanhExp","text":"<p>               Bases: <code>Module</code></p> <p>TanhExp activation function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tanhexp = TanhExp()\n&gt;&gt;&gt; x = jnp.array([[1.0, -1.0, 0.5]])\n&gt;&gt;&gt; output = tanhexp(x)\n&gt;&gt;&gt; output.shape\n(1, 3)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.activations.TanhExp-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.activations.TanhExp.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> Source code in <code>src/ml_networks/jax/activations.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    return _tanhexp(x)\n</code></pre>"},{"location":"api/jax/#unet-ml_networksjaxunet","title":"UNet (<code>ml_networks.jax.unet</code>)","text":""},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet2d","title":"ConditionalUnet2d","text":"<pre><code>ConditionalUnet2d(feature_dim, obs_shape, cfg, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>\u6761\u4ef6\u4ed8\u304dUNet\u30e2\u30c7\u30eb (NHWC format).</p> <p>Parameters:</p> Name Type Description Default <code>feature_dim</code> <code>int</code> <p>\u6761\u4ef6\u4ed8\u304d\u7279\u5fb4\u91cf\u306e\u6b21\u5143\u6570</p> required <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>\u89b3\u6e2c\u30c7\u30fc\u30bf\u306e\u5f62\u72b6 (H, W, C) in NHWC format.</p> required <code>cfg</code> <code>UNetConfig</code> <p>UNet\u306e\u8a2d\u5b9a</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required Source code in <code>src/ml_networks/jax/unet.py</code> <pre><code>def __init__(\n    self,\n    feature_dim: int,\n    obs_shape: tuple[int, int, int],\n    cfg: UNetConfig,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    in_channels = obs_shape[2]  # NHWC: C is last\n    all_dims = [in_channels, *list(cfg.channels)]\n    start_dim = cfg.channels[0]\n    self.obs_shape = obs_shape\n\n    in_out = list(pairwise(all_dims))\n\n    mid_dim = all_dims[-1]\n    self.mid_modules = nnx.List([\n        ConditionalResidualBlock2d(\n            mid_dim,\n            mid_dim,\n            cond_dim=feature_dim,\n            conv_cfg=cfg.conv_cfg,\n            cond_predict_scale=cfg.cond_pred_scale,\n            rngs=rngs,\n        ),\n        Attention2d(mid_dim, cfg.nhead, rngs=rngs) if cfg.has_attn and cfg.nhead is not None else Identity(),\n        ConditionalResidualBlock2d(\n            mid_dim,\n            mid_dim,\n            cond_dim=feature_dim,\n            conv_cfg=cfg.conv_cfg,\n            cond_predict_scale=cfg.cond_pred_scale,\n            rngs=rngs,\n        ),\n    ])\n\n    down_modules = []\n    for ind, (dim_in, dim_out) in enumerate(in_out):\n        is_last = ind &gt;= (len(in_out) - 1)\n        down_modules.append(\n            nnx.List([\n                ConditionalResidualBlock2d(\n                    dim_in,\n                    dim_out,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                    rngs=rngs,\n                ),\n                Attention2d(dim_out, cfg.nhead, rngs=rngs)\n                if cfg.has_attn and cfg.nhead is not None\n                else Identity(),\n                ConditionalResidualBlock2d(\n                    dim_out,\n                    dim_out,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                    rngs=rngs,\n                ),\n                Downsample2d(dim_out, cfg.use_shuffle, rngs=rngs) if not is_last else Identity(),\n            ])\n        )\n    self.down_modules = nnx.List(down_modules)\n\n    up_modules = []\n    for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n        is_last = ind &gt;= (len(in_out) - 1)\n        up_modules.append(\n            nnx.List([\n                ConditionalResidualBlock2d(\n                    dim_out * 2,\n                    dim_in,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                    rngs=rngs,\n                ),\n                Attention2d(dim_in, cfg.nhead, rngs=rngs) if cfg.has_attn and cfg.nhead is not None else Identity(),\n                ConditionalResidualBlock2d(\n                    dim_in,\n                    dim_in,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                    rngs=rngs,\n                ),\n                Upsample2d(dim_in, cfg.use_shuffle, rngs=rngs) if not is_last else Identity(),\n            ])\n        )\n    self.up_modules = nnx.List(up_modules)\n\n    self.final_conv1 = ConvNormActivation(start_dim, start_dim, cfg.conv_cfg, rngs=rngs)\n    self.final_conv2 = nnx.Conv(\n        in_features=start_dim,\n        out_features=in_channels,\n        kernel_size=(1, 1),\n        rngs=rngs,\n    )\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet2d-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet2d.down_modules","title":"down_modules  <code>instance-attribute</code>","text":"<pre><code>down_modules = List(down_modules)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet2d.final_conv1","title":"final_conv1  <code>instance-attribute</code>","text":"<pre><code>final_conv1 = ConvNormActivation(start_dim, start_dim, conv_cfg, rngs=rngs)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet2d.final_conv2","title":"final_conv2  <code>instance-attribute</code>","text":"<pre><code>final_conv2 = Conv(in_features=start_dim, out_features=in_channels, kernel_size=(1, 1), rngs=rngs)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet2d.mid_modules","title":"mid_modules  <code>instance-attribute</code>","text":"<pre><code>mid_modules = List([ConditionalResidualBlock2d(mid_dim, mid_dim, cond_dim=feature_dim, conv_cfg=conv_cfg, cond_predict_scale=cond_pred_scale, rngs=rngs), Attention2d(mid_dim, nhead, rngs=rngs) if has_attn and nhead is not None else Identity(), ConditionalResidualBlock2d(mid_dim, mid_dim, cond_dim=feature_dim, conv_cfg=conv_cfg, cond_predict_scale=cond_pred_scale, rngs=rngs)])\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet2d.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet2d.up_modules","title":"up_modules  <code>instance-attribute</code>","text":"<pre><code>up_modules = List(up_modules)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet2d-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet2d.__call__","title":"__call__","text":"<pre><code>__call__(base, cond)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>Array</code> <p>Input tensor of shape (B, H, W, C) in NHWC format.</p> required <code>cond</code> <code>Array</code> <p>Conditional tensor of shape (B, cond_dim).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Output tensor of shape (B, H, W, C).</p> Source code in <code>src/ml_networks/jax/unet.py</code> <pre><code>def __call__(self, base: jax.Array, cond: jax.Array) -&gt; jax.Array:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    base : jax.Array\n        Input tensor of shape (B, H, W, C) in NHWC format.\n    cond : jax.Array\n        Conditional tensor of shape (B, cond_dim).\n\n    Returns\n    -------\n    jax.Array\n        Output tensor of shape (B, H, W, C).\n    \"\"\"\n    batch_shape = base.shape[:-3]\n    assert base.shape[-3:] == self.obs_shape, (\n        f\"Input shape {base.shape[-3:]} does not match expected shape {self.obs_shape}\"\n    )\n    base = base.reshape(-1, *self.obs_shape)\n    global_feature = cond.reshape(-1, cond.shape[-1])\n\n    x = base\n    h: list[jax.Array] = []\n    for modules in self.down_modules:\n        resnet, attn, resnet2, downsample = modules[0], modules[1], modules[2], modules[3]\n        x = resnet(x, global_feature)\n        x = attn(x)\n        x = resnet2(x, global_feature)\n        h.append(x)\n        x = downsample(x)\n\n    for mid_module in self.mid_modules:\n        x = mid_module(x) if isinstance(mid_module, Identity) else mid_module(x, global_feature)\n\n    for modules in self.up_modules:\n        resnet, attn, resnet2, upsample = modules[0], modules[1], modules[2], modules[3]\n        x = jnp.concatenate((x, h.pop()), axis=-1)  # NHWC: concat on C axis\n        x = resnet(x, global_feature)\n        x = attn(x)\n        x = resnet2(x, global_feature)\n        x = upsample(x)\n\n    x = self.final_conv1(x)\n    x = self.final_conv2(x)\n\n    return x.reshape(*batch_shape, *self.obs_shape)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet1d","title":"ConditionalUnet1d","text":"<pre><code>ConditionalUnet1d(feature_dim, obs_shape, cfg, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>\u6761\u4ef6\u4ed8\u304d1D UNet\u30e2\u30c7\u30eb (NLC format).</p> <p>Parameters:</p> Name Type Description Default <code>feature_dim</code> <code>int</code> <p>\u6761\u4ef6\u4ed8\u304d\u7279\u5fb4\u91cf\u306e\u6b21\u5143\u6570</p> required <code>obs_shape</code> <code>tuple[int, int]</code> <p>\u89b3\u6e2c\u30c7\u30fc\u30bf\u306e\u5f62\u72b6 (L, C) in NLC format.</p> required <code>cfg</code> <code>UNetConfig</code> <p>UNet\u306e\u8a2d\u5b9a</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required Source code in <code>src/ml_networks/jax/unet.py</code> <pre><code>def __init__(\n    self,\n    feature_dim: int,\n    obs_shape: tuple[int, int],\n    cfg: UNetConfig,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    in_channels = obs_shape[1]  # NLC: C is last\n    all_dims = [in_channels, *list(cfg.channels)]\n    start_dim = cfg.channels[0]\n    self.obs_shape = obs_shape\n\n    in_out = list(pairwise(all_dims))\n\n    mid_dim = all_dims[-1]\n    self.mid_modules = nnx.List([\n        ConditionalResidualBlock1d(\n            mid_dim,\n            mid_dim,\n            cond_dim=feature_dim,\n            conv_cfg=cfg.conv_cfg,\n            cond_predict_scale=cfg.cond_pred_scale,\n            rngs=rngs,\n        )\n        if not cfg.use_hypernet\n        else HyperConditionalResidualBlock1d(\n            mid_dim,\n            mid_dim,\n            cond_dim=feature_dim,\n            conv_cfg=cfg.conv_cfg,\n            hyper_mlp_cfg=cfg.hyper_mlp_cfg,\n            rngs=rngs,\n        ),\n        Attention1d(mid_dim, cfg.nhead, rngs=rngs) if cfg.has_attn and cfg.nhead is not None else Identity(),\n        ConditionalResidualBlock1d(\n            mid_dim,\n            mid_dim,\n            cond_dim=feature_dim,\n            conv_cfg=cfg.conv_cfg,\n            cond_predict_scale=cfg.cond_pred_scale,\n            rngs=rngs,\n        )\n        if not cfg.use_hypernet\n        else HyperConditionalResidualBlock1d(\n            mid_dim,\n            mid_dim,\n            cond_dim=feature_dim,\n            conv_cfg=cfg.conv_cfg,\n            hyper_mlp_cfg=cfg.hyper_mlp_cfg,\n            rngs=rngs,\n        ),\n    ])\n\n    down_modules = []\n    for ind, (dim_in, dim_out) in enumerate(in_out):\n        is_last = ind &gt;= (len(in_out) - 1)\n        down_modules.append(\n            nnx.List([\n                ConditionalResidualBlock1d(\n                    dim_in,\n                    dim_out,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                    rngs=rngs,\n                )\n                if not cfg.use_hypernet\n                else HyperConditionalResidualBlock1d(\n                    dim_in,\n                    dim_out,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    hyper_mlp_cfg=cfg.hyper_mlp_cfg,\n                    rngs=rngs,\n                ),\n                Attention1d(dim_out, cfg.nhead, rngs=rngs)\n                if cfg.has_attn and cfg.nhead is not None\n                else Identity(),\n                ConditionalResidualBlock1d(\n                    dim_out,\n                    dim_out,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                    rngs=rngs,\n                )\n                if not cfg.use_hypernet\n                else HyperConditionalResidualBlock1d(\n                    dim_out,\n                    dim_out,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    hyper_mlp_cfg=cfg.hyper_mlp_cfg,\n                    rngs=rngs,\n                ),\n                Downsample1d(dim_out, cfg.use_shuffle, rngs=rngs) if not is_last else Identity(),\n            ])\n        )\n    self.down_modules = nnx.List(down_modules)\n\n    up_modules = []\n    for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n        is_last = ind &gt;= (len(in_out) - 1)\n        up_modules.append(\n            nnx.List([\n                ConditionalResidualBlock1d(\n                    dim_out * 2,\n                    dim_in,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                    rngs=rngs,\n                )\n                if not cfg.use_hypernet\n                else HyperConditionalResidualBlock1d(\n                    dim_out * 2,\n                    dim_in,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    hyper_mlp_cfg=cfg.hyper_mlp_cfg,\n                    rngs=rngs,\n                ),\n                Attention1d(dim_in, cfg.nhead, rngs=rngs) if cfg.has_attn and cfg.nhead is not None else Identity(),\n                ConditionalResidualBlock1d(\n                    dim_in,\n                    dim_in,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                    rngs=rngs,\n                )\n                if not cfg.use_hypernet\n                else HyperConditionalResidualBlock1d(\n                    dim_in,\n                    dim_in,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    hyper_mlp_cfg=cfg.hyper_mlp_cfg,\n                    rngs=rngs,\n                ),\n                Upsample1d(dim_in, cfg.use_shuffle, rngs=rngs) if not is_last else Identity(),\n            ])\n        )\n    self.up_modules = nnx.List(up_modules)\n\n    self.final_conv1 = ConvNormActivation1d(start_dim, start_dim, cfg.conv_cfg, rngs=rngs)\n    self.final_conv2 = nnx.Conv(\n        in_features=start_dim,\n        out_features=in_channels,\n        kernel_size=(1,),\n        rngs=rngs,\n    )\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet1d-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet1d.down_modules","title":"down_modules  <code>instance-attribute</code>","text":"<pre><code>down_modules = List(down_modules)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet1d.final_conv1","title":"final_conv1  <code>instance-attribute</code>","text":"<pre><code>final_conv1 = ConvNormActivation1d(start_dim, start_dim, conv_cfg, rngs=rngs)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet1d.final_conv2","title":"final_conv2  <code>instance-attribute</code>","text":"<pre><code>final_conv2 = Conv(in_features=start_dim, out_features=in_channels, kernel_size=(1,), rngs=rngs)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet1d.mid_modules","title":"mid_modules  <code>instance-attribute</code>","text":"<pre><code>mid_modules = List([ConditionalResidualBlock1d(mid_dim, mid_dim, cond_dim=feature_dim, conv_cfg=conv_cfg, cond_predict_scale=cond_pred_scale, rngs=rngs) if not use_hypernet else HyperConditionalResidualBlock1d(mid_dim, mid_dim, cond_dim=feature_dim, conv_cfg=conv_cfg, hyper_mlp_cfg=hyper_mlp_cfg, rngs=rngs), Attention1d(mid_dim, nhead, rngs=rngs) if has_attn and nhead is not None else Identity(), ConditionalResidualBlock1d(mid_dim, mid_dim, cond_dim=feature_dim, conv_cfg=conv_cfg, cond_predict_scale=cond_pred_scale, rngs=rngs) if not use_hypernet else HyperConditionalResidualBlock1d(mid_dim, mid_dim, cond_dim=feature_dim, conv_cfg=conv_cfg, hyper_mlp_cfg=hyper_mlp_cfg, rngs=rngs)])\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet1d.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet1d.up_modules","title":"up_modules  <code>instance-attribute</code>","text":"<pre><code>up_modules = List(up_modules)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet1d-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.unet.ConditionalUnet1d.__call__","title":"__call__","text":"<pre><code>__call__(base, cond)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>Array</code> <p>Input tensor of shape (B, L, C) in NLC format.</p> required <code>cond</code> <code>Array</code> <p>Conditional tensor of shape (B, cond_dim).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Output tensor of shape (B, L, C).</p> Source code in <code>src/ml_networks/jax/unet.py</code> <pre><code>def __call__(self, base: jax.Array, cond: jax.Array) -&gt; jax.Array:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    base : jax.Array\n        Input tensor of shape (B, L, C) in NLC format.\n    cond : jax.Array\n        Conditional tensor of shape (B, cond_dim).\n\n    Returns\n    -------\n    jax.Array\n        Output tensor of shape (B, L, C).\n    \"\"\"\n    batch_shape = base.shape[:-2]\n    assert base.shape[-2:] == self.obs_shape, (\n        f\"Input shape {base.shape[-2:]} does not match expected shape {self.obs_shape}\"\n    )\n    base = base.reshape(-1, *self.obs_shape)\n    global_feature = cond.reshape(-1, cond.shape[-1])\n\n    x = base\n    h: list[jax.Array] = []\n    for modules in self.down_modules:\n        resnet, attn, resnet2, downsample = modules[0], modules[1], modules[2], modules[3]\n        x = resnet(x, global_feature)\n        x = attn(x)\n        x = resnet2(x, global_feature)\n        h.append(x)\n        x = downsample(x)\n\n    for mid_module in self.mid_modules:\n        x = mid_module(x) if isinstance(mid_module, Identity) else mid_module(x, global_feature)\n\n    for modules in self.up_modules:\n        resnet, attn, resnet2, upsample = modules[0], modules[1], modules[2], modules[3]\n        x = jnp.concatenate((x, h.pop()), axis=-1)  # NLC: concat on C axis\n        x = resnet(x, global_feature)\n        x = attn(x)\n        x = resnet2(x, global_feature)\n        x = upsample(x)\n\n    x = self.final_conv1(x)\n    x = self.final_conv2(x)\n\n    return x.reshape(*batch_shape, *self.obs_shape)\n</code></pre>"},{"location":"api/jax/#ml_networksjaxjax_utils","title":"\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3 (<code>ml_networks.jax.jax_utils</code>)","text":""},{"location":"api/jax/#ml_networks.jax.jax_utils.get_optimizer","title":"get_optimizer","text":"<pre><code>get_optimizer(name, **kwargs)\n</code></pre> <p>Get optimizer from optax.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Optimizer name (e.g. \"adam\", \"sgd\", \"adamw\", \"lamb\", \"rmsprop\").</p> required <code>kwargs</code> <code>dict</code> <p>Optimizer arguments (e.g. learning_rate=0.01).</p> <code>{}</code> <p>Returns:</p> Type Description <code>GradientTransformation</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; opt = get_optimizer(\"adam\", learning_rate=0.01)\n</code></pre> Source code in <code>src/ml_networks/jax/jax_utils.py</code> <pre><code>def get_optimizer(\n    name: str,\n    **kwargs: Any,\n) -&gt; optax.GradientTransformation:\n    \"\"\"\n    Get optimizer from optax.\n\n    Parameters\n    ----------\n    name : str\n        Optimizer name (e.g. \"adam\", \"sgd\", \"adamw\", \"lamb\", \"rmsprop\").\n    kwargs : dict\n        Optimizer arguments (e.g. learning_rate=0.01).\n\n    Returns\n    -------\n    optax.GradientTransformation\n\n    Examples\n    --------\n    &gt;&gt;&gt; opt = get_optimizer(\"adam\", learning_rate=0.01)\n    \"\"\"\n    # Map common PyTorch-style names to optax equivalents\n    name_map: dict[str, str] = {\n        \"Adam\": \"adam\",\n        \"AdamW\": \"adamw\",\n        \"SGD\": \"sgd\",\n        \"RMSprop\": \"rmsprop\",\n        \"Lamb\": \"lamb\",\n        \"Lars\": \"lars\",\n        \"Adagrad\": \"adagrad\",\n    }\n\n    optax_name = name_map.get(name, name)\n\n    # Map common PyTorch kwargs to optax kwargs\n    mapped_kwargs = dict(kwargs)\n    if \"lr\" in mapped_kwargs:\n        mapped_kwargs[\"learning_rate\"] = mapped_kwargs.pop(\"lr\")\n\n    if hasattr(optax, optax_name):\n        optimizer_fn = getattr(optax, optax_name)\n    else:\n        msg = f\"Optimizer {name} is not implemented in optax. \"\n        msg += \"Please check the name.\"\n        raise NotImplementedError(msg)\n    return optimizer_fn(**mapped_kwargs)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.jax_fix_seed","title":"jax_fix_seed","text":"<pre><code>jax_fix_seed(seed=42)\n</code></pre> <p>\u4e71\u6570\u3092\u56fa\u5b9a\u3059\u308b\u95a2\u6570.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <p>Returns:</p> Type Description <code>Array</code> <p>JAX PRNG key.</p> Source code in <code>src/ml_networks/jax/jax_utils.py</code> <pre><code>def jax_fix_seed(seed: int = 42) -&gt; jax.Array:\n    \"\"\"\n    \u4e71\u6570\u3092\u56fa\u5b9a\u3059\u308b\u95a2\u6570.\n\n    Parameters\n    ----------\n    seed : int\n        Random seed.\n\n    Returns\n    -------\n    jax.Array\n        JAX PRNG key.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    return jax.random.PRNGKey(seed)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.MinMaxNormalize","title":"MinMaxNormalize","text":"<pre><code>MinMaxNormalize(min_val, max_val, old_min=0.0, old_max=1.0)\n</code></pre> <p>MinMax \u6b63\u898f\u5316\u5909\u63db.</p> <p>JAX/NumPy\u7248\u3002\u5165\u529b\u306e\u5024\u57df [old_min, old_max] \u3092 [min_val, max_val] \u306b\u5909\u63db\u3059\u308b\u3002</p> Source code in <code>src/ml_networks/jax/jax_utils.py</code> <pre><code>def __init__(self, min_val: float, max_val: float, old_min: float = 0.0, old_max: float = 1.0) -&gt; None:\n    self.min_val = min_val\n    self.max_val = max_val\n    self.scale = (max_val - min_val) / (old_max - old_min)\n    self.shift = min_val - old_min * self.scale\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.MinMaxNormalize-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.jax_utils.MinMaxNormalize.max_val","title":"max_val  <code>instance-attribute</code>","text":"<pre><code>max_val = max_val\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.MinMaxNormalize.min_val","title":"min_val  <code>instance-attribute</code>","text":"<pre><code>min_val = min_val\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.MinMaxNormalize.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = (max_val - min_val) / (old_max - old_min)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.MinMaxNormalize.shift","title":"shift  <code>instance-attribute</code>","text":"<pre><code>shift = min_val - old_min * scale\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.MinMaxNormalize-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.jax_utils.MinMaxNormalize.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> Source code in <code>src/ml_networks/jax/jax_utils.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    return x * self.scale + self.shift\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.SoftmaxTransformation","title":"SoftmaxTransformation","text":"<pre><code>SoftmaxTransformation(cfg)\n</code></pre> <p>Softmax \u5909\u63db\u30af\u30e9\u30b9.</p> Source code in <code>src/ml_networks/jax/jax_utils.py</code> <pre><code>def __init__(\n    self,\n    cfg: SoftmaxTransConfig,\n) -&gt; None:\n    super().__init__()\n    self.vector = cfg.vector\n    self.sigma = cfg.sigma\n    self.n_ignore = cfg.n_ignore\n    self.max = cfg.max\n    self.min = cfg.min\n    self.k = jnp.linspace(self.min, self.max, self.vector)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.SoftmaxTransformation-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.jax_utils.SoftmaxTransformation.k","title":"k  <code>instance-attribute</code>","text":"<pre><code>k = linspace(min, max, vector)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.SoftmaxTransformation.max","title":"max  <code>instance-attribute</code>","text":"<pre><code>max = max\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.SoftmaxTransformation.min","title":"min  <code>instance-attribute</code>","text":"<pre><code>min = min\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.SoftmaxTransformation.n_ignore","title":"n_ignore  <code>instance-attribute</code>","text":"<pre><code>n_ignore = n_ignore\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.SoftmaxTransformation.sigma","title":"sigma  <code>instance-attribute</code>","text":"<pre><code>sigma = sigma\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.SoftmaxTransformation.vector","title":"vector  <code>instance-attribute</code>","text":"<pre><code>vector = vector\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.SoftmaxTransformation-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.jax_utils.SoftmaxTransformation.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> Source code in <code>src/ml_networks/jax/jax_utils.py</code> <pre><code>def __call__(self, x: jax.Array) -&gt; jax.Array:\n    return self.transform(x)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.SoftmaxTransformation.get_transformed_dim","title":"get_transformed_dim","text":"<pre><code>get_transformed_dim(dim)\n</code></pre> Source code in <code>src/ml_networks/jax/jax_utils.py</code> <pre><code>def get_transformed_dim(self, dim: int) -&gt; int:\n    return (dim - self.n_ignore) * self.vector + self.n_ignore\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.SoftmaxTransformation.inverse","title":"inverse","text":"<pre><code>inverse(x)\n</code></pre> <p>SoftmaxTransformation \u306e\u9006\u5909\u63db.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>\u5165\u529b\u30c6\u30f3\u30bd\u30eb.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>\u51fa\u529b\u30c6\u30f3\u30bd\u30eb.</p> Source code in <code>src/ml_networks/jax/jax_utils.py</code> <pre><code>def inverse(self, x: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    SoftmaxTransformation \u306e\u9006\u5909\u63db.\n\n    Parameters\n    ----------\n    x : jax.Array\n        \u5165\u529b\u30c6\u30f3\u30bd\u30eb.\n\n    Returns\n    -------\n    jax.Array\n        \u51fa\u529b\u30c6\u30f3\u30bd\u30eb.\n    \"\"\"\n    *batch, dim = x.shape\n    x = x.reshape(-1, dim)\n    if self.n_ignore:\n        data, ignored = x[:, : -self.n_ignore], x[:, -self.n_ignore :]\n    else:\n        data = x\n\n    data = data.reshape(len(data), -1, self.vector)\n\n    data = rearrange(data, \"b d v -&gt; v b d\")\n\n    data = jnp.stack([data[v] * self.k[v] for v in range(self.vector)]).sum(axis=0)\n\n    data = jnp.concatenate([data, ignored], axis=-1) if self.n_ignore else data\n    return data.reshape(*batch, -1)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.jax_utils.SoftmaxTransformation.transform","title":"transform","text":"<pre><code>transform(x)\n</code></pre> <p>SoftmaxTransformation \u306e\u5b9f\u884c.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>\u5165\u529b\u30c6\u30f3\u30bd\u30eb.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>\u51fa\u529b\u30c6\u30f3\u30bd\u30eb.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; trans = SoftmaxTransformation(SoftmaxTransConfig(vector=16, sigma=0.01, n_ignore=1, min=-1.0, max=1.0))\n&gt;&gt;&gt; x = jnp.ones((2, 3, 4))\n&gt;&gt;&gt; transformed = trans(x)\n&gt;&gt;&gt; transformed.shape\n(2, 3, 49)\n</code></pre> <pre><code>&gt;&gt;&gt; trans = SoftmaxTransformation(SoftmaxTransConfig(vector=11, sigma=0.05, n_ignore=0, min=-1.0, max=1.0))\n&gt;&gt;&gt; x = jnp.ones((2, 3, 4))\n&gt;&gt;&gt; transformed = trans(x)\n&gt;&gt;&gt; transformed.shape\n(2, 3, 44)\n</code></pre> Source code in <code>src/ml_networks/jax/jax_utils.py</code> <pre><code>def transform(self, x: jax.Array) -&gt; jax.Array:\n    \"\"\"\n    SoftmaxTransformation \u306e\u5b9f\u884c.\n\n    Parameters\n    ----------\n    x : jax.Array\n        \u5165\u529b\u30c6\u30f3\u30bd\u30eb.\n\n    Returns\n    -------\n    jax.Array\n        \u51fa\u529b\u30c6\u30f3\u30bd\u30eb.\n\n    Examples\n    --------\n    &gt;&gt;&gt; trans = SoftmaxTransformation(SoftmaxTransConfig(vector=16, sigma=0.01, n_ignore=1, min=-1.0, max=1.0))\n    &gt;&gt;&gt; x = jnp.ones((2, 3, 4))\n    &gt;&gt;&gt; transformed = trans(x)\n    &gt;&gt;&gt; transformed.shape\n    (2, 3, 49)\n\n    &gt;&gt;&gt; trans = SoftmaxTransformation(SoftmaxTransConfig(vector=11, sigma=0.05, n_ignore=0, min=-1.0, max=1.0))\n    &gt;&gt;&gt; x = jnp.ones((2, 3, 4))\n    &gt;&gt;&gt; transformed = trans(x)\n    &gt;&gt;&gt; transformed.shape\n    (2, 3, 44)\n\n    \"\"\"\n    *batch, dim = x.shape\n    x = x.reshape(-1, dim)\n    if self.n_ignore:\n        data, ignored = x[:, : -self.n_ignore], x[:, -self.n_ignore :]\n    else:\n        data = x\n\n    negative = jnp.stack([jnp.exp((-((data - self.k[v]) ** 2)) / self.sigma) for v in range(self.vector)])\n    negative_sum = negative.sum(axis=0)\n\n    transformed = negative / (negative_sum + 1e-8)\n    transformed = rearrange(transformed, \"v b d -&gt; b (d v)\")\n\n    transformed = jnp.concatenate([transformed, ignored], axis=-1) if self.n_ignore else transformed\n    return transformed.reshape(*batch, self.get_transformed_dim(dim))\n</code></pre>"},{"location":"api/jax/#_1","title":"\u305d\u306e\u4ed6","text":""},{"location":"api/jax/#ml_networks.jax.hypernetworks.HyperNet","title":"HyperNet","text":"<pre><code>HyperNet(input_dim, output_shapes, fc_cfg=None, encoding=None, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code>, <code>HyperNetMixin</code></p> <p>A hypernetwork that generates weights for a target network.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of the input.</p> required <code>output_shapes</code> <code>dict[str, Shape]</code> <p>Shapes of the primary network weights being predicted.</p> required <code>fc_cfg</code> <code>MLPConfig | None</code> <p>Configuration for the MLP backbone.</p> <code>None</code> <code>encoding</code> <code>InputMode</code> <p>The input encoding mode. Default is None.</p> <code>None</code> <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required Source code in <code>src/ml_networks/jax/hypernetworks.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    output_shapes: dict[str, Shape],\n    fc_cfg: MLPConfig | None = None,\n    encoding: InputMode = None,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    self.input_dim = input_dim\n    self.output_shapes = output_shapes\n    self.encoding = encoding\n\n    self._output_offsets = self.output_offsets()\n\n    self.backbone: nnx.Module\n    if fc_cfg is not None:\n        self.backbone = MLPLayer(\n            self.input_dim,\n            self.flat_output_size(),\n            fc_cfg,\n            rngs=rngs,\n        )\n    else:\n        self.backbone = nnx.Linear(\n            self.input_dim,\n            self.flat_output_size(),\n            rngs=rngs,\n        )\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.hypernetworks.HyperNet-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.hypernetworks.HyperNet.backbone","title":"backbone  <code>instance-attribute</code>","text":"<pre><code>backbone\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.hypernetworks.HyperNet.encoding","title":"encoding  <code>instance-attribute</code>","text":"<pre><code>encoding = encoding\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.hypernetworks.HyperNet.input_dim","title":"input_dim  <code>instance-attribute</code>","text":"<pre><code>input_dim = input_dim\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.hypernetworks.HyperNet.output_shapes","title":"output_shapes  <code>instance-attribute</code>","text":"<pre><code>output_shapes = output_shapes\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.hypernetworks.HyperNet-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.hypernetworks.HyperNet.__call__","title":"__call__","text":"<pre><code>__call__(inputs)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Array</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>dict[str, Array]</code> <p>Dictionary of output tensors.</p> Source code in <code>src/ml_networks/jax/hypernetworks.py</code> <pre><code>def __call__(self, inputs: jax.Array) -&gt; dict[str, jax.Array]:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    inputs : jax.Array\n        Input tensor.\n\n    Returns\n    -------\n    dict[str, jax.Array]\n        Dictionary of output tensors.\n    \"\"\"\n    if self.encoding is not None:\n        inputs = encode_input(inputs, self.encoding)\n\n    flat_output = self.backbone(inputs)\n    return self.unflatten_output(flat_output)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.contrastive.ContrastiveLearningLoss","title":"ContrastiveLearningLoss","text":"<pre><code>ContrastiveLearningLoss(dim_input1, dim_input2, cfg, *, rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Contrastive learning module.</p> <p>Parameters:</p> Name Type Description Default <code>dim_input1</code> <code>int</code> <p>Dimension of first input.</p> required <code>dim_input2</code> <code>int</code> <p>Dimension of second input.</p> required <code>cfg</code> <code>ContrastiveLearningConfig</code> <p>Configuration for contrastive learning.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required Source code in <code>src/ml_networks/jax/contrastive.py</code> <pre><code>def __init__(\n    self,\n    dim_input1: int,\n    dim_input2: int,\n    cfg: ContrastiveLearningConfig,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; None:\n    self.cfg = cfg\n    self.dim_feature = cfg.dim_feature\n    self.dim_input1 = dim_input1\n    self.dim_input2 = dim_input2\n    self.is_ce_like = cfg.cross_entropy_like\n\n    self.eval_func = MLPLayer(dim_input1, cfg.dim_feature, cfg.eval_func, rngs=rngs)\n    if self.dim_input1 != self.dim_input2:\n        self.eval_func2 = MLPLayer(dim_input2, cfg.dim_feature, cfg.eval_func, rngs=rngs)\n    else:\n        self.eval_func2 = self.eval_func\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.contrastive.ContrastiveLearningLoss-attributes","title":"Attributes","text":""},{"location":"api/jax/#ml_networks.jax.contrastive.ContrastiveLearningLoss.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.contrastive.ContrastiveLearningLoss.dim_feature","title":"dim_feature  <code>instance-attribute</code>","text":"<pre><code>dim_feature = dim_feature\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.contrastive.ContrastiveLearningLoss.dim_input1","title":"dim_input1  <code>instance-attribute</code>","text":"<pre><code>dim_input1 = dim_input1\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.contrastive.ContrastiveLearningLoss.dim_input2","title":"dim_input2  <code>instance-attribute</code>","text":"<pre><code>dim_input2 = dim_input2\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.contrastive.ContrastiveLearningLoss.eval_func","title":"eval_func  <code>instance-attribute</code>","text":"<pre><code>eval_func = MLPLayer(dim_input1, dim_feature, eval_func, rngs=rngs)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.contrastive.ContrastiveLearningLoss.eval_func2","title":"eval_func2  <code>instance-attribute</code>","text":"<pre><code>eval_func2 = MLPLayer(dim_input2, dim_feature, eval_func, rngs=rngs)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.contrastive.ContrastiveLearningLoss.is_ce_like","title":"is_ce_like  <code>instance-attribute</code>","text":"<pre><code>is_ce_like = cross_entropy_like\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.contrastive.ContrastiveLearningLoss-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.contrastive.ContrastiveLearningLoss.calc_nce","title":"calc_nce","text":"<pre><code>calc_nce(feature1, feature2, return_emb=False)\n</code></pre> <p>Calculate the Noise Contrastive Estimation (NCE) loss.</p> <p>Parameters:</p> Name Type Description Default <code>feature1</code> <code>Array</code> <p>First input tensor of shape (*, dim_input1)</p> required <code>feature2</code> <code>Array</code> <p>Second input tensor of shape (*, dim_input2)</p> required <code>return_emb</code> <code>bool</code> <p>Whether to return embeddings. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict or tuple</code> <p>Loss dictionary, optionally with embeddings.</p> Source code in <code>src/ml_networks/jax/contrastive.py</code> <pre><code>def calc_nce(\n    self,\n    feature1: jax.Array,\n    feature2: jax.Array,\n    return_emb: bool = False,\n) -&gt; dict[str, jax.Array] | tuple[dict[str, jax.Array], tuple[jax.Array, jax.Array]]:\n    \"\"\"\n    Calculate the Noise Contrastive Estimation (NCE) loss.\n\n    Parameters\n    ----------\n    feature1 : jax.Array\n        First input tensor of shape (*, dim_input1)\n    feature2 : jax.Array\n        Second input tensor of shape (*, dim_input2)\n    return_emb : bool\n        Whether to return embeddings. Default is False.\n\n    Returns\n    -------\n    dict or tuple\n        Loss dictionary, optionally with embeddings.\n    \"\"\"\n    loss_dict: dict[str, jax.Array] = {}\n    batch_shape = feature1.shape[:-1]\n    emb_1 = self.eval_func(feature1.reshape(-1, self.dim_input1))\n    emb_2 = self.eval_func2(feature2.reshape(-1, self.dim_input2))\n\n    if self.is_ce_like:\n        labels = jnp.arange(len(emb_1))\n        sim_matrix = emb_1 @ emb_2.T\n        # Cross entropy loss\n        log_softmax = jax.nn.log_softmax(sim_matrix, axis=-1)\n        nce_loss = -log_softmax[jnp.arange(len(labels)), labels] - np.log(len(sim_matrix))\n        loss_dict[\"nce\"] = nce_loss\n    else:\n        positive = jnp.sum(emb_1 * emb_2, axis=-1)\n        loss_dict[\"positive\"] = jax.lax.stop_gradient(positive).mean()\n\n        sim_matrix = emb_1 @ emb_2.T\n        negative = jax.nn.logsumexp(sim_matrix, axis=-1) - np.log(len(sim_matrix))\n        loss_dict[\"negative\"] = jax.lax.stop_gradient(negative).mean()\n\n        nce_loss = -positive + negative\n        loss_dict[\"nce\"] = nce_loss.reshape(batch_shape)\n\n    if return_emb:\n        return loss_dict, (emb_1, emb_2)\n    return loss_dict\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.contrastive.ContrastiveLearningLoss.calc_sigmoid","title":"calc_sigmoid","text":"<pre><code>calc_sigmoid(feature1, feature2, return_emb=False, temperature=0.1, bias=0.0)\n</code></pre> <p>Calculate the Sigmoid loss for contrastive learning.</p> <p>Parameters:</p> Name Type Description Default <code>feature1</code> <code>Array</code> <p>First input tensor of shape (*, dim_input1)</p> required <code>feature2</code> <code>Array</code> <p>Second input tensor of shape (*, dim_input2)</p> required <code>return_emb</code> <code>bool</code> <p>Whether to return embeddings. Default is False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Temperature. Default is 0.1.</p> <code>0.1</code> <code>bias</code> <code>float</code> <p>Bias. Default is 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>dict or tuple</code> <p>Loss dictionary, optionally with embeddings.</p> Source code in <code>src/ml_networks/jax/contrastive.py</code> <pre><code>def calc_sigmoid(\n    self,\n    feature1: jax.Array,\n    feature2: jax.Array,\n    return_emb: bool = False,\n    temperature: float | jax.Array = 0.1,\n    bias: float | jax.Array = 0.0,\n) -&gt; dict[str, jax.Array] | tuple[dict[str, jax.Array], tuple[jax.Array, jax.Array]]:\n    \"\"\"\n    Calculate the Sigmoid loss for contrastive learning.\n\n    Parameters\n    ----------\n    feature1 : jax.Array\n        First input tensor of shape (*, dim_input1)\n    feature2 : jax.Array\n        Second input tensor of shape (*, dim_input2)\n    return_emb : bool\n        Whether to return embeddings. Default is False.\n    temperature : float\n        Temperature. Default is 0.1.\n    bias : float\n        Bias. Default is 0.0.\n\n    Returns\n    -------\n    dict or tuple\n        Loss dictionary, optionally with embeddings.\n    \"\"\"\n    loss_dict: dict[str, jax.Array] = {}\n    batch_shape = feature1.shape[:-1]\n    emb_1 = self.eval_func(feature1.reshape(-1, self.dim_input1))\n    emb_2 = self.eval_func2(feature2.reshape(-1, self.dim_input2))\n\n    logits = emb_1 @ emb_2.T * temperature + bias\n    labels = jnp.eye(len(logits)) * 2 - 1\n    loss = -jax.nn.log_sigmoid(logits * labels).sum(axis=-1)\n    loss_dict[\"sigmoid\"] = loss.reshape(batch_shape)\n    if return_emb:\n        return loss_dict, (emb_1, emb_2)\n    return loss_dict\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.contrastive.ContrastiveLearningLoss.calc_timeseries_nce","title":"calc_timeseries_nce","text":"<pre><code>calc_timeseries_nce(feature1, feature2, positive_range_self=0, positive_range_tgt=0, return_emb=False)\n</code></pre> <p>Calculate the NCE loss for time series data.</p> <p>Parameters:</p> Name Type Description Default <code>feature1</code> <code>Array</code> <p>First input tensor of shape (*batch, length, dim_input1)</p> required <code>feature2</code> <code>Array</code> <p>Second input tensor of shape (*batch, length, dim_input2)</p> required <code>positive_range_self</code> <code>int</code> <p>Range for self-positive samples. Default is 0.</p> <code>0</code> <code>positive_range_tgt</code> <code>int</code> <p>Range for target-positive samples. Default is 0.</p> <code>0</code> <code>return_emb</code> <code>bool</code> <p>Whether to return embeddings. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict or tuple</code> <p>Loss dictionary, optionally with embeddings.</p> Source code in <code>src/ml_networks/jax/contrastive.py</code> <pre><code>def calc_timeseries_nce(\n    self,\n    feature1: jax.Array,\n    feature2: jax.Array,\n    positive_range_self: int = 0,\n    positive_range_tgt: int = 0,\n    return_emb: bool = False,\n) -&gt; dict[str, jax.Array] | tuple[dict[str, jax.Array], tuple[jax.Array, jax.Array]]:\n    \"\"\"\n    Calculate the NCE loss for time series data.\n\n    Parameters\n    ----------\n    feature1 : jax.Array\n        First input tensor of shape (*batch, length, dim_input1)\n    feature2 : jax.Array\n        Second input tensor of shape (*batch, length, dim_input2)\n    positive_range_self : int\n        Range for self-positive samples. Default is 0.\n    positive_range_tgt : int\n        Range for target-positive samples. Default is 0.\n    return_emb : bool\n        Whether to return embeddings. Default is False.\n\n    Returns\n    -------\n    dict or tuple\n        Loss dictionary, optionally with embeddings.\n    \"\"\"\n    if not positive_range_self and not positive_range_tgt:\n        return self.calc_nce(feature1, feature2, return_emb)\n\n    feature1 = feature1.reshape(-1, feature1.shape[-2], self.dim_input1)\n    feature2 = feature2.reshape(-1, feature2.shape[-2], self.dim_input2)\n    batch, length, _ = feature1.shape\n\n    emb_1 = self.eval_func(feature1)\n    emb_2 = self.eval_func2(feature2)\n\n    loss_dict: dict[str, jax.Array] = {}\n\n    positive = jnp.sum(\n        emb_1.reshape(-1, emb_1.shape[-1]) * emb_2.reshape(-1, emb_2.shape[-1]),\n        axis=-1,\n    )\n    loss_dict[\"positive\"] = jax.lax.stop_gradient(positive).mean()\n\n    if positive_range_self &gt; 0:\n        self_positive_1, self_positive_2 = self._calculate_self_positive_pairs(\n            emb_1,\n            emb_2,\n            batch,\n            length,\n            positive_range_self,\n        )\n        positive = positive + self_positive_1.reshape(-1) + self_positive_2.reshape(-1)\n        loss_dict[\"self_positive_1\"] = jax.lax.stop_gradient(self_positive_1).mean()\n        loss_dict[\"self_positive_2\"] = jax.lax.stop_gradient(self_positive_2).mean()\n\n    if positive_range_tgt &gt; 0:\n        tgt_positive = self._calculate_target_positive_pairs(\n            emb_1,\n            emb_2,\n            batch,\n            length,\n            positive_range_tgt,\n        )\n        positive = positive + tgt_positive.reshape(-1)\n        loss_dict[\"tgt_positive\"] = jax.lax.stop_gradient(tgt_positive).mean()\n\n    flat_emb1 = emb_1.reshape(-1, emb_1.shape[-1])\n    flat_emb2 = emb_2.reshape(-1, emb_2.shape[-1])\n    sim_matrix = flat_emb1 @ flat_emb2.T\n    negative = jax.nn.logsumexp(sim_matrix, axis=-1) - np.log(len(sim_matrix))\n    loss_dict[\"negative\"] = jax.lax.stop_gradient(negative).mean()\n\n    nce_loss = -positive + negative\n    nce_loss = nce_loss.mean()\n    loss_dict[\"nce\"] = nce_loss\n\n    if return_emb:\n        return loss_dict, (emb_1, emb_2)\n    return loss_dict\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.base.BaseModule","title":"BaseModule","text":"<p>               Bases: <code>Module</code></p> <p>Base module for JAX/Flax NNX.</p>"},{"location":"api/jax/#ml_networks.jax.base.BaseModule-functions","title":"Functions","text":""},{"location":"api/jax/#ml_networks.jax.base.BaseModule.freeze_biases","title":"freeze_biases","text":"<pre><code>freeze_biases()\n</code></pre> <p>Freeze all bias parameters.</p> Source code in <code>src/ml_networks/jax/base.py</code> <pre><code>def freeze_biases(self) -&gt; None:\n    \"\"\"Freeze all bias parameters.\"\"\"\n    _graph_def, state = nnx.split(self)\n    flat_state = state.flat_state()\n    for key, value in flat_state.items():\n        if \"bias\" in key and isinstance(value, nnx.VariableState):\n            value.type = nnx.VariableState  # type: ignore[assignment]\n    state = nnx.State.from_flat_path(flat_state)\n    nnx.update(self, state)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.base.BaseModule.freeze_weights","title":"freeze_weights","text":"<pre><code>freeze_weights()\n</code></pre> <p>Freeze all weight parameters (kernel in Flax).</p> Source code in <code>src/ml_networks/jax/base.py</code> <pre><code>def freeze_weights(self) -&gt; None:\n    \"\"\"Freeze all weight parameters (kernel in Flax).\"\"\"\n    _graph_def, state = nnx.split(self)\n    flat_state = state.flat_state()\n    for key, value in flat_state.items():\n        if \"kernel\" in key and isinstance(value, nnx.VariableState):\n            value.type = nnx.VariableState  # type: ignore[assignment]\n    state = nnx.State.from_flat_path(flat_state)\n    nnx.update(self, state)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.base.BaseModule.unfreeze_biases","title":"unfreeze_biases","text":"<pre><code>unfreeze_biases()\n</code></pre> <p>Unfreeze all bias parameters.</p> Source code in <code>src/ml_networks/jax/base.py</code> <pre><code>def unfreeze_biases(self) -&gt; None:\n    \"\"\"Unfreeze all bias parameters.\"\"\"\n    _graph_def, state = nnx.split(self)\n    flat_state = state.flat_state()\n    for key, value in flat_state.items():\n        if \"bias\" in key and isinstance(value, nnx.VariableState):\n            value.type = nnx.Param  # type: ignore[assignment]\n    state = nnx.State.from_flat_path(flat_state)\n    nnx.update(self, state)\n</code></pre>"},{"location":"api/jax/#ml_networks.jax.base.BaseModule.unfreeze_weights","title":"unfreeze_weights","text":"<pre><code>unfreeze_weights()\n</code></pre> <p>Unfreeze all weight parameters (kernel in Flax).</p> Source code in <code>src/ml_networks/jax/base.py</code> <pre><code>def unfreeze_weights(self) -&gt; None:\n    \"\"\"Unfreeze all weight parameters (kernel in Flax).\"\"\"\n    _graph_def, state = nnx.split(self)\n    flat_state = state.flat_state()\n    for key, value in flat_state.items():\n        if \"kernel\" in key and isinstance(value, nnx.VariableState):\n            value.type = nnx.Param  # type: ignore[assignment]\n    state = nnx.State.from_flat_path(flat_state)\n    nnx.update(self, state)\n</code></pre>"},{"location":"api/layers/","title":"\u30ec\u30a4\u30e4\u30fc","text":"<p>\u57fa\u672c\u7684\u306a\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30ec\u30a4\u30e4\u30fc\u3092\u63d0\u4f9b\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3059\u3002</p> <p><code>ml_networks.torch.layers</code>\uff08PyTorch\uff09\u3068<code>ml_networks.jax.layers</code>\uff08JAX\uff09\u306e\u4e21\u65b9\u3067\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"api/layers/#mlplayer","title":"MLPLayer","text":""},{"location":"api/layers/#ml_networks.torch.layers.MLPLayer","title":"MLPLayer","text":"<pre><code>MLPLayer(input_dim, output_dim, cfg)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>Multi-layer perceptron layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input dimension.</p> required <code>output_dim</code> <code>int</code> <p>Output dimension.</p> required <code>cfg</code> <code>MLPConfig</code> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = MLPConfig(\n...     hidden_dim=16,\n...     n_layers=3,\n...     output_activation=\"ReLU\",\n...     linear_cfg=LinearConfig(\n...         activation=\"ReLU\",\n...         norm=\"layer\",\n...         norm_cfg={\"eps\": 1e-05, \"elementwise_affine\": True, \"bias\": True},\n...         dropout=0.1,\n...         norm_first=False,\n...         bias=True\n...     )\n... )\n&gt;&gt;&gt; mlp = MLPLayer(32, 16, cfg)\n&gt;&gt;&gt; x = torch.randn(1, 32)\n&gt;&gt;&gt; output = mlp(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    output_dim: int,\n    cfg: MLPConfig,\n) -&gt; None:\n    super().__init__()\n    self.cfg = deepcopy(cfg)\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.hidden_dim = cfg.hidden_dim\n    self.n_layers = cfg.n_layers\n    self.dense = self._build_dense()\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.MLPLayer-attributes","title":"Attributes","text":""},{"location":"api/layers/#ml_networks.torch.layers.MLPLayer.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = deepcopy(cfg)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.MLPLayer.dense","title":"dense  <code>instance-attribute</code>","text":"<pre><code>dense = _build_dense()\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.MLPLayer.hidden_dim","title":"hidden_dim  <code>instance-attribute</code>","text":"<pre><code>hidden_dim = hidden_dim\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.MLPLayer.input_dim","title":"input_dim  <code>instance-attribute</code>","text":"<pre><code>input_dim = input_dim\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.MLPLayer.n_layers","title":"n_layers  <code>instance-attribute</code>","text":"<pre><code>n_layers = n_layers\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.MLPLayer.output_dim","title":"output_dim  <code>instance-attribute</code>","text":"<pre><code>output_dim = output_dim\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.MLPLayer-functions","title":"Functions","text":""},{"location":"api/layers/#ml_networks.torch.layers.MLPLayer.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (*, input_dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (*, output_dim)</p> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (*, input_dim)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (*, output_dim)\n\n    \"\"\"\n    return self.dense(x)\n</code></pre>"},{"location":"api/layers/#linearnormactivation","title":"LinearNormActivation","text":""},{"location":"api/layers/#ml_networks.torch.layers.LinearNormActivation","title":"LinearNormActivation","text":"<pre><code>LinearNormActivation(input_dim, output_dim, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Linear layer with normalization and activation, and dropouts.</p> References <p>LayerNorm: https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html RMSNorm: https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html Dropout: https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input dimension.</p> required <code>output_dim</code> <code>int</code> <p>Output dimension.</p> required <code>cfg</code> <code>LinearConfig</code> <p>Linear layer configuration.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = LinearConfig(\n...     activation=\"ReLU\",\n...     norm=\"layer\",\n...     norm_cfg={\"eps\": 1e-05, \"elementwise_affine\": True, \"bias\": True},\n...     dropout=0.1,\n...     norm_first=False,\n...     bias=True\n... )\n&gt;&gt;&gt; linear = LinearNormActivation(32, 16, cfg)\n&gt;&gt;&gt; linear\nLinearNormActivation(\n  (linear): Linear(in_features=32, out_features=16, bias=True)\n  (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n  (activation): Activation(\n    (activation): ReLU()\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n)\n&gt;&gt;&gt; x = torch.randn(1, 32)\n&gt;&gt;&gt; output = linear(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16])\n</code></pre> <pre><code>&gt;&gt;&gt; cfg = LinearConfig(\n...     activation=\"SiGLU\",\n...     norm=\"none\",\n...     norm_cfg={},\n...     dropout=0.0,\n...     norm_first=True,\n...     bias=True\n... )\n&gt;&gt;&gt; linear = LinearNormActivation(32, 16, cfg)\n&gt;&gt;&gt; # If activation includes \"glu\", linear output_dim is doubled to adjust actual output_dim.\n&gt;&gt;&gt; linear\nLinearNormActivation(\n  (linear): Linear(in_features=32, out_features=32, bias=True)\n  (norm): Identity()\n  (activation): Activation(\n    (activation): SiGLU()\n  )\n  (dropout): Identity()\n)\n&gt;&gt;&gt; x = torch.randn(1, 32)\n&gt;&gt;&gt; output = linear(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    output_dim: int,\n    cfg: LinearConfig,\n) -&gt; None:\n    super().__init__()\n    self.linear = nn.Linear(\n        input_dim,\n        output_dim * 2 if \"glu\" in cfg.activation.lower() else output_dim,\n        bias=cfg.bias,\n    )\n    if cfg.norm_first:\n        normalized_shape = input_dim\n    else:\n        normalized_shape = output_dim * 2 if \"glu\" in cfg.activation.lower() else output_dim\n\n    cfg.norm_cfg[\"normalized_shape\"] = normalized_shape\n    self.norm = get_norm(cfg.norm, **cfg.norm_cfg)\n    self.activation = Activation(cfg.activation)\n    self.dropout: nn.Module\n    if cfg.dropout &gt; 0:\n        self.dropout = nn.Dropout(cfg.dropout)\n    else:\n        self.dropout = nn.Identity()\n    self.norm_first = cfg.norm_first\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.LinearNormActivation-attributes","title":"Attributes","text":""},{"location":"api/layers/#ml_networks.torch.layers.LinearNormActivation.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = Activation(activation)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.LinearNormActivation.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.LinearNormActivation.linear","title":"linear  <code>instance-attribute</code>","text":"<pre><code>linear = Linear(input_dim, output_dim * 2 if 'glu' in lower() else output_dim, bias=bias)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.LinearNormActivation.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = get_norm(norm, **(norm_cfg))\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.LinearNormActivation.norm_first","title":"norm_first  <code>instance-attribute</code>","text":"<pre><code>norm_first = norm_first\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.LinearNormActivation-functions","title":"Functions","text":""},{"location":"api/layers/#ml_networks.torch.layers.LinearNormActivation.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (*, input_dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (*, output_dim)</p> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (*, input_dim)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (*, output_dim)\n    \"\"\"\n    if self.norm_first:\n        x = self.norm(x)\n        x = self.linear(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n    else:\n        x = self.linear(x)\n        x = self.norm(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n    return x\n</code></pre>"},{"location":"api/layers/#convnormactivation","title":"ConvNormActivation","text":""},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation","title":"ConvNormActivation","text":"<pre><code>ConvNormActivation(in_channels, out_channels, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Convolutional layer with normalization and activation, and dropouts.</p> References <p>PixelShuffle: https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html PixelUnshuffle: https://pytorch.org/docs/stable/generated/torch.nn.PixelUnshuffle.html BatchNorm2d: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html GroupNorm: https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html LayerNorm: https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html InstanceNorm2d: https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html Conv2d: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html Dropout: https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Input channels.</p> required <code>out_channels</code> <code>int</code> <p>Output channels.</p> required <code>cfg</code> <code>ConvConfig</code> <p>Convolutional layer configuration.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = ConvConfig(\n...     activation=\"ReLU\",\n...     kernel_size=3,\n...     stride=1,\n...     padding=1,\n...     dilation=1,\n...     groups=1,\n...     bias=True,\n...     dropout=0.1,\n...     norm=\"batch\",\n...     norm_cfg={\"affine\": True, \"track_running_stats\": True},\n...     scale_factor=0\n... )\n&gt;&gt;&gt; conv = ConvNormActivation(3, 16, cfg)\n&gt;&gt;&gt; x = torch.randn(1, 3, 32, 32)\n&gt;&gt;&gt; output = conv(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16, 32, 32])\n</code></pre> <pre><code>&gt;&gt;&gt; cfg = ConvConfig(\n...     activation=\"SiGLU\",\n...     kernel_size=3,\n...     stride=1,\n...     padding=1,\n...     dilation=1,\n...     groups=1,\n...     bias=True,\n...     dropout=0.0,\n...     norm=\"none\",\n...     norm_cfg={},\n...     scale_factor=2\n... )\n&gt;&gt;&gt; conv = ConvNormActivation(3, 16, cfg)\n&gt;&gt;&gt; x = torch.randn(1, 3, 32, 32)\n&gt;&gt;&gt; output = conv(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16, 64, 64])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    cfg: ConvConfig,\n) -&gt; None:\n    super().__init__()\n\n    out_channels_ = out_channels\n    if \"glu\" in cfg.activation.lower():\n        out_channels_ *= 2\n    if cfg.scale_factor &gt; 0:\n        out_channels_ *= abs(cfg.scale_factor) ** 2\n    elif cfg.scale_factor &lt; 0:\n        out_channels_ //= abs(cfg.scale_factor) ** 2\n    self.conv = nn.Conv2d(\n        in_channels=in_channels,\n        out_channels=out_channels_,\n        kernel_size=cfg.kernel_size,\n        stride=cfg.stride,\n        padding=cfg.padding,\n        dilation=cfg.dilation,\n        groups=cfg.groups,\n        bias=cfg.bias,\n        padding_mode=cfg.padding_mode,\n    )\n    if cfg.norm != \"none\" and cfg.norm != \"group\":\n        cfg.norm_cfg[\"num_features\"] = out_channels_\n    elif cfg.norm == \"group\":\n        cfg.norm_cfg[\"num_channels\"] = in_channels if cfg.norm_first else out_channels_\n\n    norm_type: Literal[\"layer\", \"rms\", \"group\", \"batch2d\", \"batch1d\", \"none\"] = (\n        \"batch2d\" if cfg.norm == \"batch\" else cfg.norm\n    )  # type: ignore[assignment]\n    self.norm = get_norm(norm_type, **cfg.norm_cfg)\n    self.pixel_shuffle: nn.Module\n    if cfg.scale_factor &gt; 0:\n        self.pixel_shuffle = nn.PixelShuffle(cfg.scale_factor)\n    elif cfg.scale_factor &lt; 0:\n        self.pixel_shuffle = nn.PixelUnshuffle(abs(cfg.scale_factor))\n    else:\n        self.pixel_shuffle = nn.Identity()\n    self.activation = Activation(cfg.activation, dim=-3)\n    self.dropout: nn.Module = nn.Dropout(cfg.dropout) if cfg.dropout &gt; 0 else nn.Identity()\n    self.norm_first = cfg.norm_first\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation-attributes","title":"Attributes","text":""},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = Activation(activation, dim=-3)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = Conv2d(in_channels=in_channels, out_channels=out_channels_, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias, padding_mode=padding_mode)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = Dropout(dropout) if dropout &gt; 0 else Identity()\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = get_norm(norm_type, **(norm_cfg))\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation.norm_first","title":"norm_first  <code>instance-attribute</code>","text":"<pre><code>norm_first = norm_first\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation.pixel_shuffle","title":"pixel_shuffle  <code>instance-attribute</code>","text":"<pre><code>pixel_shuffle\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation-functions","title":"Functions","text":""},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (B, in_channels, H, W) or (in_channels, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (B, out_channels, H', W') or (out_channels, H', W')</p> <code>H' and W' are calculated as follows:</code> <code>H' = (H + 2*padding - dilation * (kernel_size - 1) - 1) // stride + 1</code> <code>H' = H' * scale_factor if scale_factor &gt; 0 else H' // abs(scale_factor) if scale_factor &lt; 0 else H'</code> <code>W' = (W + 2*padding - dilation * (kernel_size - 1) - 1) // stride + 1</code> <code>W' = W' * scale_factor if scale_factor &gt; 0 else W' // abs(scale_factor) if scale_factor &lt; 0 else W'</code> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (B, in_channels, H, W) or (in_channels, H, W)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (B, out_channels, H', W') or (out_channels, H', W')\n    H' and W' are calculated as follows:\n    H' = (H + 2*padding - dilation * (kernel_size - 1) - 1) // stride + 1\n    H' = H' * scale_factor if scale_factor &gt; 0 else H' // abs(scale_factor) if scale_factor &lt; 0 else H'\n    W' = (W + 2*padding - dilation * (kernel_size - 1) - 1) // stride + 1\n    W' = W' * scale_factor if scale_factor &gt; 0 else W' // abs(scale_factor) if scale_factor &lt; 0 else W'\n\n    \"\"\"\n    if self.norm_first:\n        x = self.norm(x)\n        x = self.conv(x)\n        x = self.pixel_shuffle(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n    else:\n        x = self.conv(x)\n        x = self.norm(x)\n        x = self.pixel_shuffle(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n    return x\n</code></pre>"},{"location":"api/layers/#convnormactivation1d","title":"ConvNormActivation1d","text":""},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation1d","title":"ConvNormActivation1d","text":"<pre><code>ConvNormActivation1d(in_channels, out_channels, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>1D Convolutional layer with normalization and activation, and dropouts.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Input channels.</p> required <code>out_channels</code> <code>int</code> <p>Output channels.</p> required <code>cfg</code> <code>ConvConfig</code> <p>Convolutional layer configuration.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = ConvConfig(\n...     activation=\"ReLU\",\n...     kernel_size=3,\n...     stride=1,\n...     padding=1,\n...     dilation=1,\n...     groups=1,\n...     bias=True,\n...     dropout=0.1,\n...     norm=\"batch\",\n...     norm_cfg={\"affine\": True, \"track_running_stats\": True},\n...     padding_mode=\"zeros\"\n... )\n&gt;&gt;&gt; conv = ConvNormActivation1d(3, 16, cfg)\n&gt;&gt;&gt; x = torch.randn(1, 3, 32)\n&gt;&gt;&gt; output = conv(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16, 32])\n&gt;&gt;&gt; cfg = ConvConfig(\n...     activation=\"SiGLU\",\n...     kernel_size=3,\n...     stride=1,\n...     padding=1,\n...     dilation=1,\n...     groups=1,\n...     bias=True,\n...     dropout=0.0,\n...     norm=\"none\",\n...     norm_cfg={},\n...     padding_mode=\"zeros\"\n... )\n&gt;&gt;&gt; conv = ConvNormActivation1d(3, 16, cfg)\n&gt;&gt;&gt; x = torch.randn(1, 3, 32)\n&gt;&gt;&gt; output = conv(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16, 32])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    cfg: ConvConfig,\n) -&gt; None:\n    super().__init__()\n\n    out_channels_ = out_channels\n    if \"glu\" in cfg.activation.lower():\n        out_channels_ *= 2\n    if cfg.scale_factor &gt; 0:\n        out_channels_ *= abs(cfg.scale_factor)\n    elif cfg.scale_factor &lt; 0:\n        out_channels_ //= abs(cfg.scale_factor)\n    self.conv = nn.Conv1d(\n        in_channels=in_channels,\n        out_channels=out_channels_,\n        kernel_size=cfg.kernel_size,\n        stride=cfg.stride,\n        padding=cfg.padding,\n        dilation=cfg.dilation,\n        groups=cfg.groups,\n        bias=cfg.bias,\n        padding_mode=cfg.padding_mode,\n    )\n    if cfg.norm != \"none\" and cfg.norm != \"group\":\n        cfg.norm_cfg[\"num_features\"] = out_channels_\n    elif cfg.norm == \"group\":\n        cfg.norm_cfg[\"num_channels\"] = in_channels if cfg.norm_first else out_channels_\n\n    if cfg.scale_factor &gt; 0:\n        self.horizontal_shuffle: nn.Module = HorizonShuffle(cfg.scale_factor)\n    elif cfg.scale_factor &lt; 0:\n        self.horizontal_shuffle = HorizonUnShuffle(abs(cfg.scale_factor))\n    else:\n        self.horizontal_shuffle = nn.Identity()\n\n    norm_type: Literal[\"layer\", \"rms\", \"group\", \"batch2d\", \"batch1d\", \"none\"] = (\n        \"batch1d\" if cfg.norm == \"batch\" else cfg.norm\n    )  # type: ignore[assignment]\n    self.norm = get_norm(norm_type, **cfg.norm_cfg)\n    self.activation = Activation(cfg.activation, dim=-2)\n    self.dropout: nn.Module = nn.Dropout(cfg.dropout) if cfg.dropout &gt; 0 else nn.Identity()\n    self.norm_first = cfg.norm_first\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation1d-attributes","title":"Attributes","text":""},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation1d.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = Activation(activation, dim=-2)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation1d.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = Conv1d(in_channels=in_channels, out_channels=out_channels_, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias, padding_mode=padding_mode)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation1d.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = Dropout(dropout) if dropout &gt; 0 else Identity()\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation1d.horizontal_shuffle","title":"horizontal_shuffle  <code>instance-attribute</code>","text":"<pre><code>horizontal_shuffle = HorizonShuffle(scale_factor)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation1d.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = get_norm(norm_type, **(norm_cfg))\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation1d.norm_first","title":"norm_first  <code>instance-attribute</code>","text":"<pre><code>norm_first = norm_first\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation1d-functions","title":"Functions","text":""},{"location":"api/layers/#ml_networks.torch.layers.ConvNormActivation1d.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (B, in_channels, L) or (in_channels, L)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (B, out_channels, L') or (out_channels, L')</p> <code>L' is calculated as follows:</code> <code>L' = (L + 2*padding - dilation * (kernel_size - 1) - 1) // stride + 1</code> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (B, in_channels, L) or (in_channels, L)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (B, out_channels, L') or (out_channels, L')\n    L' is calculated as follows:\n    L' = (L + 2*padding - dilation * (kernel_size - 1) - 1) // stride + 1\n\n    \"\"\"\n    if self.norm_first:\n        x = self.norm(x)\n        x = self.conv(x)\n        x = self.horizontal_shuffle(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n    else:\n        x = self.conv(x)\n        x = self.horizontal_shuffle(x)\n        x = self.norm(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n    return x\n</code></pre>"},{"location":"api/layers/#convtransposenormactivation","title":"ConvTransposeNormActivation","text":""},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation","title":"ConvTransposeNormActivation","text":"<pre><code>ConvTransposeNormActivation(in_channels, out_channels, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Transposed convolutional layer with normalization and activation, and dropouts.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Input channels.</p> required <code>out_channels</code> <code>int</code> <p>Output channels.</p> required <code>cfg</code> <code>ConvConfig</code> <p>Convolutional layer configuration.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = ConvConfig(\n...     activation=\"ReLU\",\n...     kernel_size=3,\n...     stride=1,\n...     padding=1,\n...     output_padding=0,\n...     dilation=1,\n...     groups=1,\n...     bias=True,\n...     dropout=0.1,\n...     norm=\"batch\",\n...     norm_cfg={\"affine\": True, \"track_running_stats\": True}\n... )\n&gt;&gt;&gt; conv = ConvTransposeNormActivation(3, 16, cfg)\n&gt;&gt;&gt; x = torch.randn(1, 3, 32, 32)\n&gt;&gt;&gt; output = conv(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16, 32, 32])\n</code></pre> <pre><code>&gt;&gt;&gt; cfg = ConvConfig(\n...     activation=\"SiGLU\",\n...     kernel_size=3,\n...     stride=1,\n...     padding=1,\n...     output_padding=0,\n...     dilation=1,\n...     groups=1,\n...     bias=True,\n...     dropout=0.0,\n...     norm=\"none\",\n...     norm_cfg={}\n... )\n&gt;&gt;&gt; conv = ConvTransposeNormActivation(3, 16, cfg)\n&gt;&gt;&gt; x = torch.randn(1, 3, 32, 32)\n&gt;&gt;&gt; output = conv(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16, 32, 32])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    cfg: ConvConfig,\n) -&gt; None:\n    super().__init__()\n\n    self.conv = nn.ConvTranspose2d(\n        in_channels,\n        out_channels * 2 if \"glu\" in cfg.activation.lower() else out_channels,\n        cfg.kernel_size,\n        cfg.stride,\n        cfg.padding,\n        cfg.output_padding,\n        cfg.groups,\n        bias=cfg.bias,\n        dilation=cfg.dilation,\n    )\n    if cfg.norm not in {\"none\", \"group\"}:\n        cfg.norm_cfg[\"num_features\"] = out_channels * 2 if \"glu\" in cfg.activation.lower() else out_channels\n    elif cfg.norm == \"group\":\n        cfg.norm_cfg[\"num_channels\"] = out_channels * 2 if \"glu\" in cfg.activation.lower() else out_channels\n    norm_type: Literal[\"layer\", \"rms\", \"group\", \"batch2d\", \"batch1d\", \"none\"] = (\n        \"batch2d\" if cfg.norm == \"batch\" else cfg.norm\n    )  # type: ignore[assignment]\n    self.norm = get_norm(norm_type, **cfg.norm_cfg)\n    self.activation = Activation(cfg.activation, dim=-3)\n    self.dropout: nn.Module = nn.Dropout(cfg.dropout) if cfg.dropout &gt; 0 else nn.Identity()\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation-attributes","title":"Attributes","text":""},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = Activation(activation, dim=-3)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = ConvTranspose2d(in_channels, out_channels * 2 if 'glu' in lower() else out_channels, kernel_size, stride, padding, output_padding, groups, bias=bias, dilation=dilation)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = Dropout(dropout) if dropout &gt; 0 else Identity()\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = get_norm(norm_type, **(norm_cfg))\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation-functions","title":"Functions","text":""},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (B, in_channels, H, W) or (in_channels, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (B, out_channels, H', W') or (out_channels, H', W')</p> <code>H' and W' are calculated as follows:</code> <code>H' = (H - 1) * stride - 2 * padding + kernel_size + output_padding</code> <code>W' = (W - 1) * stride - 2 * padding + kernel_size + output_padding</code> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (B, in_channels, H, W) or (in_channels, H, W)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (B, out_channels, H', W') or (out_channels, H', W')\n    H' and W' are calculated as follows:\n    H' = (H - 1) * stride - 2 * padding + kernel_size + output_padding\n    W' = (W - 1) * stride - 2 * padding + kernel_size + output_padding\n    \"\"\"\n    x = self.conv(x)\n    x = self.norm(x)\n    x = self.activation(x)\n    return self.dropout(x)\n</code></pre>"},{"location":"api/layers/#convtransposenormactivation1d","title":"ConvTransposeNormActivation1d","text":""},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation1d","title":"ConvTransposeNormActivation1d","text":"<pre><code>ConvTransposeNormActivation1d(in_channels, out_channels, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>1D Transposed convolutional layer with normalization and activation, and dropouts.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Input channels.</p> required <code>out_channels</code> <code>int</code> <p>Output channels.</p> required <code>cfg</code> <code>ConvConfig</code> <p>Convolutional layer configuration.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = ConvConfig(\n...     activation=\"ReLU\",\n...     kernel_size=3,\n...     stride=1,\n...     padding=1,\n...     output_padding=0,\n...     dilation=1,\n...     groups=1,\n...     bias=True,\n...     dropout=0.1,\n...     norm=\"batch\",\n... )\n&gt;&gt;&gt; conv = ConvTransposeNormActivation1d(3, 16, cfg)\n&gt;&gt;&gt; x = torch.randn(1, 3, 32)\n&gt;&gt;&gt; output = conv(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16, 32])\n</code></pre> <pre><code>&gt;&gt;&gt; cfg = ConvConfig(\n...     activation=\"SiGLU\",\n...     kernel_size=3,\n...     stride=1,\n...     padding=1,\n...     output_padding=0,\n...     dilation=1,\n...     groups=1,\n...     bias=True,\n...     dropout=0.0,\n...     norm=\"none\",\n...     norm_cfg={}\n... )\n&gt;&gt;&gt; conv = ConvTransposeNormActivation1d(3, 16, cfg)\n&gt;&gt;&gt; x = torch.randn(1, 3, 32)\n&gt;&gt;&gt; output = conv(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16, 32])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    cfg: ConvConfig,\n) -&gt; None:\n    super().__init__()\n\n    self.conv = nn.ConvTranspose1d(\n        in_channels=in_channels,\n        out_channels=out_channels * 2 if \"glu\" in cfg.activation.lower() else out_channels,\n        kernel_size=cfg.kernel_size,\n        stride=cfg.stride,\n        padding=cfg.padding,\n        output_padding=cfg.output_padding,\n        groups=cfg.groups,\n        bias=cfg.bias,\n        dilation=cfg.dilation,\n    )\n    if cfg.norm not in {\"none\", \"group\"}:\n        cfg.norm_cfg[\"num_features\"] = out_channels * 2 if \"glu\" in cfg.activation.lower() else out_channels\n    elif cfg.norm == \"group\":\n        cfg.norm_cfg[\"num_channels\"] = out_channels * 2 if \"glu\" in cfg.activation.lower() else out_channels\n    norm_type: Literal[\"layer\", \"rms\", \"group\", \"batch2d\", \"batch1d\", \"none\"] = (\n        \"batch1d\" if cfg.norm == \"batch\" else cfg.norm\n    )  # type: ignore[assignment]\n    self.norm = get_norm(norm_type, **cfg.norm_cfg)\n    self.activation = Activation(cfg.activation, dim=-2)\n    self.dropout = nn.Dropout(cfg.dropout) if cfg.dropout &gt; 0 else nn.Identity()\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation1d-attributes","title":"Attributes","text":""},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation1d.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = Activation(activation, dim=-2)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation1d.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = ConvTranspose1d(in_channels=in_channels, out_channels=out_channels * 2 if 'glu' in lower() else out_channels, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=output_padding, groups=groups, bias=bias, dilation=dilation)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation1d.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = Dropout(dropout) if dropout &gt; 0 else Identity()\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation1d.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = get_norm(norm_type, **(norm_cfg))\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation1d-functions","title":"Functions","text":""},{"location":"api/layers/#ml_networks.torch.layers.ConvTransposeNormActivation1d.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (B, in_channels, L) or (in_channels, L)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (B, out_channels, L') or (out_channels, L')</p> <code>L' is calculated as follows:</code> <code>L' = (L - 1) * stride - 2 * padding + kernel_size + output_padding</code> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (B, in_channels, L) or (in_channels, L)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (B, out_channels, L') or (out_channels, L')\n    L' is calculated as follows:\n    L' = (L - 1) * stride - 2 * padding + kernel_size + output_padding\n    \"\"\"\n    x = self.conv(x)\n    x = self.norm(x)\n    x = self.activation(x)\n    return self.dropout(x)\n</code></pre>"},{"location":"api/layers/#residualblock","title":"ResidualBlock","text":""},{"location":"api/layers/#ml_networks.torch.layers.ResidualBlock","title":"ResidualBlock","text":"<pre><code>ResidualBlock(in_features, kernel_size, activation='ReLU', norm='none', norm_cfg=None, dropout=0.0, padding_mode='zeros')\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Residual block.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Input features.</p> required <code>kernel_size</code> <code>int</code> <p>Kernel size.</p> required <code>activation</code> <code>str</code> <p>Activation function.</p> <code>'ReLU'</code> <code>norm</code> <code>Literal['batch', 'group', 'none']</code> <p>Normalization layer. If it's set to \"none\", normalization is not applied. Default is \"none\".</p> <code>'none'</code> <code>norm_cfg</code> <code>dictConfig</code> <p>Normalization layer configuration. Default is {}.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout rate. If it's set to 0.0, dropout is not applied. Default is 0.0.</p> <code>0.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; resblock = ResidualBlock(16, 3, \"ReLU\", \"batch\", {}, 0.1)\n&gt;&gt;&gt; x = torch.randn(1, 16, 32, 32)\n&gt;&gt;&gt; output = resblock(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 16, 32, 32])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    kernel_size: int,\n    activation: str = \"ReLU\",\n    norm: Literal[\"batch\", \"group\", \"none\"] = \"none\",\n    norm_cfg: dict[str, Any] | None = None,\n    dropout: float = 0.0,\n    padding_mode: Literal[\"zeros\", \"reflect\", \"replicate\", \"circular\"] = \"zeros\",\n) -&gt; None:\n    super().__init__()\n    first_cfg = ConvConfig(\n        activation=activation,\n        kernel_size=kernel_size,\n        stride=1,\n        padding=(kernel_size - 1) // 2,\n        dilation=1,\n        groups=1,\n        bias=True,\n        dropout=dropout,\n        norm=norm,\n        norm_cfg=norm_cfg or {},\n        padding_mode=padding_mode,\n    )\n    second_cfg = ConvConfig(\n        activation=\"Identity\",\n        kernel_size=kernel_size,\n        stride=1,\n        padding=(kernel_size - 1) // 2,\n        dilation=1,\n        groups=1,\n        bias=True,\n        dropout=dropout,\n        norm=norm,\n        norm_cfg=norm_cfg or {},\n        padding_mode=padding_mode,\n    )\n    self.conv_block = nn.Sequential(\n        ConvNormActivation(\n            in_features,\n            in_features,\n            first_cfg,\n        ),\n        ConvNormActivation(\n            in_features,\n            in_features * 2 if \"glu\" in activation.lower() else in_features,\n            second_cfg,\n        ),\n    )\n    self.activation = Activation(activation, dim=-3)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ResidualBlock-attributes","title":"Attributes","text":""},{"location":"api/layers/#ml_networks.torch.layers.ResidualBlock.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = Activation(activation, dim=-3)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ResidualBlock.conv_block","title":"conv_block  <code>instance-attribute</code>","text":"<pre><code>conv_block = Sequential(ConvNormActivation(in_features, in_features, first_cfg), ConvNormActivation(in_features, in_features * 2 if 'glu' in lower() else in_features, second_cfg))\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.ResidualBlock-functions","title":"Functions","text":""},{"location":"api/layers/#ml_networks.torch.layers.ResidualBlock.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (B, in_features, H, W) or (in_features, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (B, in_features, H, W) or (in_features, H, W)</p> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (B, in_features, H, W) or (in_features, H, W)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (B, in_features, H, W) or (in_features, H, W)\n    \"\"\"\n    return self.activation(x + self.conv_block(x))\n</code></pre>"},{"location":"api/layers/#attention1d","title":"Attention1d","text":""},{"location":"api/layers/#ml_networks.torch.layers.Attention1d","title":"Attention1d","text":"<pre><code>Attention1d(channels, nhead=None, patch_size=1, attn_cfg=None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>1d\u81ea\u5df1\u6ce8\u610f\u6a5f\u69cb.</p> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(\n    self,\n    channels: int,\n    nhead: int | None = None,\n    patch_size: int = 1,\n    attn_cfg: AttentionConfig | None = None,\n) -&gt; None:\n    super().__init__()\n    self.channels = channels\n    if nhead is None:\n        assert attn_cfg is not None, \"Either nhead or attn_cfg must be provided\"\n        self.n_heads = attn_cfg.nhead\n        self.patch_size = attn_cfg.patch_size\n    else:\n        self.n_heads = nhead\n        self.patch_size = patch_size\n    assert channels % self.n_heads == 0, f\"q,k,v channels {channels} is not divisible by num_head_channels {nhead}\"\n    cfg = ConvConfig(\n        kernel_size=1,\n        padding=0,\n        stride=1,\n        activation=\"Identity\",\n        dropout=0.0,\n    )\n    first_cfg = cfg\n    first_cfg.norm_first = True\n    self.qkv = ConvNormActivation1d(channels, channels * 3, first_cfg)\n\n    self.proj_out = ConvNormActivation1d(\n        channels,\n        channels,\n        cfg,\n    )\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.Attention1d-attributes","title":"Attributes","text":""},{"location":"api/layers/#ml_networks.torch.layers.Attention1d.channels","title":"channels  <code>instance-attribute</code>","text":"<pre><code>channels = channels\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.Attention1d.n_heads","title":"n_heads  <code>instance-attribute</code>","text":"<pre><code>n_heads = nhead\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.Attention1d.patch_size","title":"patch_size  <code>instance-attribute</code>","text":"<pre><code>patch_size = patch_size\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.Attention1d.proj_out","title":"proj_out  <code>instance-attribute</code>","text":"<pre><code>proj_out = ConvNormActivation1d(channels, channels, cfg)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.Attention1d.qkv","title":"qkv  <code>instance-attribute</code>","text":"<pre><code>qkv = ConvNormActivation1d(channels, channels * 3, first_cfg)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.Attention1d-functions","title":"Functions","text":""},{"location":"api/layers/#ml_networks.torch.layers.Attention1d.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    b, c, *spatial = x.shape\n    x = x.reshape(b, c, -1)\n    qkv = self.qkv(x)\n    qkv = rearrange(qkv, \"b c (t p) -&gt; b (c p) t\", p=self.patch_size)\n    h = self.qkv_attention(qkv)\n    h = rearrange(h, \"b (c p) t -&gt; b c (t p)\", p=self.patch_size)\n    h = self.proj_out(h)\n    return (x + h).reshape(b, c, *spatial)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.Attention1d.qkv_attention","title":"qkv_attention","text":"<pre><code>qkv_attention(qkv)\n</code></pre> <p>Apply QKV attention.</p> <p>Parameters:</p> Name Type Description Default <code>qkv</code> <code>Tensor</code> <p>An [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>An [N x (H * C) x T] tensor after attention.</p> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def qkv_attention(self, qkv: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply QKV attention.\n\n    Parameters\n    ----------\n    qkv : torch.Tensor\n        An [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n\n    Returns\n    -------\n    torch.Tensor\n        An [N x (H * C) x T] tensor after attention.\n    \"\"\"\n    bs, width, length = qkv.shape\n    assert width % (3 * self.n_heads) == 0\n    ch = width // (3 * self.n_heads)\n    q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n    scale = 1 / np.sqrt(np.sqrt(ch))\n    weight = torch.einsum(\n        \"bct,bcs-&gt;bts\",\n        q * scale,\n        k * scale,\n    )  # More stable with f16 than dividing afterwards\n    weight = torch.softmax(weight - torch.max(weight, dim=-1, keepdim=True)[0], dim=-1)\n    a = torch.einsum(\"bts,bcs-&gt;bct\", weight, v)\n    return a.reshape(bs, -1, length)\n</code></pre>"},{"location":"api/layers/#attention2d","title":"Attention2d","text":""},{"location":"api/layers/#ml_networks.torch.layers.Attention2d","title":"Attention2d","text":"<pre><code>Attention2d(channels, nhead=None, patch_size=1, attn_cfg=None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>2d\u81ea\u5df1\u6ce8\u610f\u6a5f\u69cb.</p> <p>Args:     channels (int): \u5165\u529b\u30fb\u51fa\u529b\u30c1\u30e3\u30f3\u30cd\u30eb\u6570     nhead (int): \u6ce8\u610f\u30d8\u30c3\u30c9\u306e\u6570</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; attn = Attention2d(channels=64, nhead=8)\n&gt;&gt;&gt; x = torch.randn(2, 64, 32, 32)\n&gt;&gt;&gt; out = attn(x)\n&gt;&gt;&gt; out.shape\ntorch.Size([2, 64, 32, 32])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(\n    self,\n    channels: int,\n    nhead: int | None = None,\n    patch_size: int = 1,\n    attn_cfg: AttentionConfig | None = None,\n) -&gt; None:\n    super().__init__()\n    self.channels = channels\n    if nhead is None or patch_size is None:\n        assert attn_cfg is not None, \"Either nhead and patch_size or attn_cfg must be provided\"\n        self.n_heads = attn_cfg.nhead\n        self.patch_size = attn_cfg.patch_size\n    else:\n        self.n_heads = nhead\n        self.patch_size = patch_size\n    assert channels % self.n_heads == 0, f\"q,k,v channels {channels} is not divisible by num_head_channels {nhead}\"\n    cfg = ConvConfig(\n        kernel_size=1,\n        padding=0,\n        stride=1,\n        activation=\"Identity\",\n        dropout=0.0,\n    )\n    first_cfg = cfg\n    first_cfg.norm_first = True\n    self.qkv = ConvNormActivation(channels, channels * 3, first_cfg)\n\n    self.proj_out = ConvNormActivation(\n        channels,\n        channels,\n        cfg,\n    )\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.Attention2d-attributes","title":"Attributes","text":""},{"location":"api/layers/#ml_networks.torch.layers.Attention2d.channels","title":"channels  <code>instance-attribute</code>","text":"<pre><code>channels = channels\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.Attention2d.n_heads","title":"n_heads  <code>instance-attribute</code>","text":"<pre><code>n_heads = nhead\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.Attention2d.patch_size","title":"patch_size  <code>instance-attribute</code>","text":"<pre><code>patch_size = patch_size\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.Attention2d.proj_out","title":"proj_out  <code>instance-attribute</code>","text":"<pre><code>proj_out = ConvNormActivation(channels, channels, cfg)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.Attention2d.qkv","title":"qkv  <code>instance-attribute</code>","text":"<pre><code>qkv = ConvNormActivation(channels, channels * 3, first_cfg)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.Attention2d-functions","title":"Functions","text":""},{"location":"api/layers/#ml_networks.torch.layers.Attention2d.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    b, c, *spatial = x.shape\n    qkv = self.qkv(x)\n    qkv = rearrange(qkv, \"b c (h p1) (w p2) -&gt; b (c p1 p2) h w\", p1=self.patch_size, p2=self.patch_size)\n    h = self.qkv_attn(qkv)\n    h = rearrange(h, \"b (c p1 p2) h w -&gt; b c (h p1) (w p2)\", p1=self.patch_size, p2=self.patch_size)\n    h = self.proj_out(h)\n    return (x + h).reshape(b, c, *spatial)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.Attention2d.qkv_attn","title":"qkv_attn","text":"<pre><code>qkv_attn(qkv)\n</code></pre> <p>Apply QKV attention.</p> <p>Parameters:</p> Name Type Description Default <code>qkv</code> <code>Tensor</code> <p>An [N x (Heads * 3 * C) x H x W] tensor of query, key, value.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>An [N x (C * Head) x H x W] tensor of attended values.</p> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def qkv_attn(self, qkv: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply QKV attention.\n\n    Parameters\n    ----------\n    qkv : torch.Tensor\n        An [N x (Heads * 3 * C) x H x W] tensor of query, key, value.\n\n    Returns\n    -------\n    torch.Tensor\n        An [N x (C * Head) x H x W] tensor of attended values.\n    \"\"\"\n    bs, channels, height, width = qkv.shape\n    assert channels % (3 * self.n_heads) == 0\n    ch = channels // (3 * self.n_heads)\n    q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, height * width).split(ch, dim=1)\n    scale = 1 / np.sqrt(np.sqrt(ch))\n    weight = torch.einsum(\n        \"bct,bcs-&gt;bts\",\n        q * scale,\n        k * scale,\n    )  # More stable with f16 than dividing afterwards\n    weight = torch.softmax(weight - torch.max(weight, dim=-1, keepdim=True)[0], dim=-1)\n    a = torch.einsum(\"bts,bcs-&gt;bct\", weight, v)\n    return a.reshape(bs, -1, height, width)\n</code></pre>"},{"location":"api/layers/#transformerlayer","title":"TransformerLayer","text":""},{"location":"api/layers/#ml_networks.torch.layers.TransformerLayer","title":"TransformerLayer","text":"<pre><code>TransformerLayer(input_dim, output_dim, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Transformer layer.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input dimension.</p> required <code>output_dim</code> <code>int</code> <p>Output dimension.</p> required <code>cfg</code> <code>TransformerConfig</code> <p>Transformer layer configuration.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = TransformerConfig(\n...     d_model=16,\n...     nhead=4,\n...     dim_ff=64,\n...     n_layers=3,\n...     dropout=0.1,\n...     hidden_activation=\"ReLU\",\n...     output_activation=\"ReLU\"\n... )\n&gt;&gt;&gt; transformer = TransformerLayer(32, 16, cfg)\n&gt;&gt;&gt; x = torch.randn(1, 8, 32)\n&gt;&gt;&gt; output = transformer(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 8, 16])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    output_dim: int,\n    cfg: TransformerConfig,\n) -&gt; None:\n    super().__init__()\n    self.d_model = cfg.d_model\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.nhead = cfg.nhead\n    self.dim_feedforward = cfg.dim_ff\n    self.n_layers = cfg.n_layers\n    self.in_proj = (\n        nn.Sequential(\n            nn.Linear(input_dim, cfg.d_model),\n            nn.LayerNorm(cfg.d_model),\n            Activation(cfg.hidden_activation),\n        )\n        if input_dim != cfg.d_model\n        else nn.Identity()\n    )\n    self.out_proj = (\n        nn.Sequential(\n            nn.Linear(cfg.d_model, output_dim),\n            Activation(cfg.output_activation),\n        )\n        if output_dim\n        else Activation(cfg.output_activation)\n    )\n\n    self.transformer_layer = nn.TransformerEncoderLayer(\n        d_model=self.d_model,\n        nhead=self.nhead,\n        dim_feedforward=self.dim_feedforward,\n        activation=cfg.hidden_activation.lower(),\n        dropout=cfg.dropout,\n        batch_first=True,\n    )\n\n    self.transformer = nn.TransformerEncoder(\n        self.transformer_layer,\n        num_layers=self.n_layers,\n        enable_nested_tensor=True,\n    )\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.TransformerLayer-attributes","title":"Attributes","text":""},{"location":"api/layers/#ml_networks.torch.layers.TransformerLayer.d_model","title":"d_model  <code>instance-attribute</code>","text":"<pre><code>d_model = d_model\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.TransformerLayer.dim_feedforward","title":"dim_feedforward  <code>instance-attribute</code>","text":"<pre><code>dim_feedforward = dim_ff\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.TransformerLayer.in_proj","title":"in_proj  <code>instance-attribute</code>","text":"<pre><code>in_proj = Sequential(Linear(input_dim, d_model), LayerNorm(d_model), Activation(hidden_activation)) if input_dim != d_model else Identity()\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.TransformerLayer.input_dim","title":"input_dim  <code>instance-attribute</code>","text":"<pre><code>input_dim = input_dim\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.TransformerLayer.n_layers","title":"n_layers  <code>instance-attribute</code>","text":"<pre><code>n_layers = n_layers\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.TransformerLayer.nhead","title":"nhead  <code>instance-attribute</code>","text":"<pre><code>nhead = nhead\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.TransformerLayer.out_proj","title":"out_proj  <code>instance-attribute</code>","text":"<pre><code>out_proj = Sequential(Linear(d_model, output_dim), Activation(output_activation)) if output_dim else Activation(output_activation)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.TransformerLayer.output_dim","title":"output_dim  <code>instance-attribute</code>","text":"<pre><code>output_dim = output_dim\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.TransformerLayer.transformer","title":"transformer  <code>instance-attribute</code>","text":"<pre><code>transformer = TransformerEncoder(transformer_layer, num_layers=n_layers, enable_nested_tensor=True)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.TransformerLayer.transformer_layer","title":"transformer_layer  <code>instance-attribute</code>","text":"<pre><code>transformer_layer = TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, activation=lower(), dropout=dropout, batch_first=True)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.TransformerLayer-functions","title":"Functions","text":""},{"location":"api/layers/#ml_networks.torch.layers.TransformerLayer.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (B, L, input_dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (B, L, output_dim)</p> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (B, L, input_dim)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (B, L, output_dim)\n    \"\"\"\n    x = self.in_proj(x)\n    x = self.transformer(x)\n    return self.out_proj(x)\n</code></pre>"},{"location":"api/layers/#spatialsoftmax","title":"SpatialSoftmax","text":""},{"location":"api/layers/#ml_networks.torch.layers.SpatialSoftmax","title":"SpatialSoftmax","text":"<pre><code>SpatialSoftmax(cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Spatial Softmax and Flatten layer.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>SpatialSoftmaxConfig</code> <p>Spatial softmax configuration.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = SpatialSoftmaxConfig(temperature=1.0, is_argmax=True)\n&gt;&gt;&gt; spatial_softmax = SpatialSoftmax(cfg)\n&gt;&gt;&gt; x = torch.randn(1, 64, 16, 16)\n&gt;&gt;&gt; output = spatial_softmax(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 64, 2])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(self, cfg: SpatialSoftmaxConfig) -&gt; None:\n    super().__init__()\n    self.temperature = cfg.temperature\n    self.eps = cfg.eps\n    assert self.temperature &gt; 0.0, \"temperature must be non-negative\"\n    if cfg.is_argmax:\n        self.spatial_softmax = self.spatial_argmax2d\n    elif cfg.is_straight_through:\n        self.spatial_softmax = self.spatial_softmax_straight_through\n    else:\n        self.spatial_softmax = spatial_soft_argmax2d\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.SpatialSoftmax-attributes","title":"Attributes","text":""},{"location":"api/layers/#ml_networks.torch.layers.SpatialSoftmax.eps","title":"eps  <code>instance-attribute</code>","text":"<pre><code>eps = eps\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.SpatialSoftmax.spatial_softmax","title":"spatial_softmax  <code>instance-attribute</code>","text":"<pre><code>spatial_softmax = spatial_argmax2d\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.SpatialSoftmax.temperature","title":"temperature  <code>instance-attribute</code>","text":"<pre><code>temperature = temperature\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.SpatialSoftmax-functions","title":"Functions","text":""},{"location":"api/layers/#ml_networks.torch.layers.SpatialSoftmax.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>\u9806\u4f1d\u64ad.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>B: \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3001N: \u30c8\u30fc\u30af\u30f3\u6570\u3001H: \u9ad8\u3055\u3001W: \u5e45</p> required <p>Returns:</p> Type Description <code>    Spatial Softmax\u3092\u9069\u7528\u3057\u305f\u7279\u5fb4\u91cf\u3002\u5f62\u72b6\u306f\u3001(B, N, D)\u3002</code> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    \u9806\u4f1d\u64ad.\n\n    Parameters\n    ----------\n    x: \u5165\u529b\u7279\u5fb4\u91cf\u3002\u5f62\u72b6\u306f\u3001(B, N, H, W)\n        B: \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3001N: \u30c8\u30fc\u30af\u30f3\u6570\u3001H: \u9ad8\u3055\u3001W: \u5e45\n\n    Returns\n    -------\n        Spatial Softmax\u3092\u9069\u7528\u3057\u305f\u7279\u5fb4\u91cf\u3002\u5f62\u72b6\u306f\u3001(B, N, D)\u3002\n    \"\"\"\n    return self.spatial_softmax(x / self.temperature)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.SpatialSoftmax.spatial_argmax2d","title":"spatial_argmax2d","text":"<pre><code>spatial_argmax2d(x)\n</code></pre> <p>Spatial Softmax and Argmax layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>\u5165\u529b\u7279\u5fb4\u91cf\u3002\u5f62\u72b6\u306f\u3001(B, N, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Spatial Softmax\u3092\u9069\u7528\u3057\u305f\u7279\u5fb4\u91cf\u3002\u5f62\u72b6\u306f\u3001(B, N, 2)\u3002</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input is not a torch.Tensor.</p> <code>ValueError</code> <p>If input shape is not 4D.</p> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def spatial_argmax2d(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Spatial Softmax and Argmax layer.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        \u5165\u529b\u7279\u5fb4\u91cf\u3002\u5f62\u72b6\u306f\u3001(B, N, H, W)\n\n    Returns\n    -------\n    torch.Tensor\n        Spatial Softmax\u3092\u9069\u7528\u3057\u305f\u7279\u5fb4\u91cf\u3002\u5f62\u72b6\u306f\u3001(B, N, 2)\u3002\n\n    Raises\n    ------\n    TypeError\n        If input is not a torch.Tensor.\n    ValueError\n        If input shape is not 4D.\n    \"\"\"\n    if not torch.is_tensor(x):\n        msg = f\"Input input type is not a torch.Tensor. Got {type(x)}\"\n        raise TypeError(msg)\n    if len(x.shape) != 4:\n        msg = f\"Invalid input shape, we expect BxCxHxW. Got: {x.shape}\"\n        raise ValueError(msg)\n    # unpack shapes and create view from input tensor\n    batch_size, channels, _height, _width = x.shape\n    pos_y, pos_x = create_meshgrid(x, normalized_coordinates=True)\n    x = x.view(batch_size, channels, -1)\n\n    # compute softmax with max subtraction  trick\n    exp_x = torch.exp(x - torch.max(x, dim=-1, keepdim=True)[0])\n    exp_x_sum = 1.0 / (exp_x.sum(dim=-1, keepdim=True) + self.eps)\n    softmax_x = exp_x * exp_x_sum\n\n    # straight-through trick\n    argmax_x = torch.argmax(x, dim=-1, keepdim=True)\n    argmax_x = F.one_hot(argmax_x, num_classes=x.shape[-1]).squeeze(2)\n    argmax_x = argmax_x + softmax_x - softmax_x.detach()\n\n    # create coordinates grid\n    pos_x = pos_x.reshape(-1)\n    pos_y = pos_y.reshape(-1)\n\n    # compute the expected coordinates\n    expected_y = torch.sum(pos_y * argmax_x, dim=-1, keepdim=True)\n    expected_x = torch.sum(pos_x * argmax_x, dim=-1, keepdim=True)\n    output = torch.cat([expected_x, expected_y], dim=-1)\n    return output.view(batch_size, channels, 2)  # BxNx2\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.SpatialSoftmax.spatial_softmax_straight_through","title":"spatial_softmax_straight_through","text":"<pre><code>spatial_softmax_straight_through(x)\n</code></pre> <p>Spatial Softmax and Argmax layer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>\u5165\u529b\u7279\u5fb4\u91cf\u3002\u5f62\u72b6\u306f\u3001(B, N, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Spatial Softmax\u3092\u9069\u7528\u3057\u305f\u7279\u5fb4\u91cf\u3002\u5f62\u72b6\u306f\u3001(B, N, 2)\u3002</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If input is not a torch.Tensor.</p> <code>ValueError</code> <p>If input shape is not 4D.</p> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def spatial_softmax_straight_through(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Spatial Softmax and Argmax layer.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        \u5165\u529b\u7279\u5fb4\u91cf\u3002\u5f62\u72b6\u306f\u3001(B, N, H, W)\n\n    Returns\n    -------\n    torch.Tensor\n        Spatial Softmax\u3092\u9069\u7528\u3057\u305f\u7279\u5fb4\u91cf\u3002\u5f62\u72b6\u306f\u3001(B, N, 2)\u3002\n\n    Raises\n    ------\n    TypeError\n        If input is not a torch.Tensor.\n    ValueError\n        If input shape is not 4D.\n    \"\"\"\n    if not torch.is_tensor(x):\n        msg = f\"Input input type is not a torch.Tensor. Got {type(x)}\"\n        raise TypeError(msg)\n    if len(x.shape) != 4:\n        msg = f\"Invalid input shape, we expect BxCxHxW. Got: {x.shape}\"\n        raise ValueError(msg)\n    # unpack shapes and create view from input tensor\n    batch_size, channels, _height, _width = x.shape\n    pos_y, pos_x = create_meshgrid(x, normalized_coordinates=True)\n    x = x.view(batch_size, channels, -1)\n\n    # compute softmax with max subtraction  trick\n    exp_x = torch.exp(x - torch.max(x, dim=-1, keepdim=True)[0])\n    exp_x_sum = 1.0 / (exp_x.sum(dim=-1, keepdim=True) + self.eps)\n    softmax_x = exp_x * exp_x_sum\n\n    # straight-through trick\n    softmax_x = softmax_x + x - x.detach()\n\n    # create coordinates grid\n    pos_x = pos_x.reshape(-1)\n    pos_y = pos_y.reshape(-1)\n\n    # compute the expected coordinates\n    expected_y = torch.sum(pos_y * softmax_x, dim=-1, keepdim=True)\n    expected_x = torch.sum(pos_x * softmax_x, dim=-1, keepdim=True)\n    output = torch.cat([expected_x, expected_y], dim=-1)\n    return output.view(batch_size, channels, 2)  # BxNx2\n</code></pre>"},{"location":"api/layers/#patchembed","title":"PatchEmbed","text":""},{"location":"api/layers/#ml_networks.torch.layers.PatchEmbed","title":"PatchEmbed","text":"<pre><code>PatchEmbed(emb_dim, patch_size, obs_shape)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Patch embedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>emb_dim</code> <code>int</code> <p>Embedding dimension.</p> required <code>patch_size</code> <code>int</code> <p>Patch size.</p> required <code>obs_shape</code> <code>Tuple[int, int, int]</code> <p>Observation shape.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; patch_embed = PatchEmbed(16, 4, (3, 32, 32))\n&gt;&gt;&gt; x = torch.randn(1, 3, 32, 32)\n&gt;&gt;&gt; output = patch_embed(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 64, 16])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(\n    self,\n    emb_dim: int,\n    patch_size: int,\n    obs_shape: tuple[int, int, int],\n) -&gt; None:\n    super().__init__()\n    self.emb_dim = emb_dim\n    self.obs_shape = obs_shape\n\n    # \u30d1\u30c3\u30c1\u306e\u5927\u304d\u3055\n    self.patch_size = patch_size\n    self.patch_num = (obs_shape[1] // patch_size) * (obs_shape[2] // patch_size)\n    assert self.patch_size * self.patch_size * self.patch_num == self.obs_shape[1] * self.obs_shape[2], (\n        \"patch_num is not correct\"\n    )\n\n    # \u5165\u529b\u753b\u50cf\u306e\u30d1\u30c3\u30c1\u3078\u306e\u5206\u5272 &amp; \u30d1\u30c3\u30c1\u306e\u57cb\u3081\u8fbc\u307f\u3092\u4e00\u6c17\u306b\u884c\u3046\u5c64\n    self.patch_emb_layer = nn.Conv2d(\n        in_channels=self.obs_shape[0],\n        out_channels=self.emb_dim,\n        kernel_size=self.patch_size,\n        stride=self.patch_size,\n    )\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.PatchEmbed-attributes","title":"Attributes","text":""},{"location":"api/layers/#ml_networks.torch.layers.PatchEmbed.emb_dim","title":"emb_dim  <code>instance-attribute</code>","text":"<pre><code>emb_dim = emb_dim\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.PatchEmbed.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.PatchEmbed.patch_emb_layer","title":"patch_emb_layer  <code>instance-attribute</code>","text":"<pre><code>patch_emb_layer = Conv2d(in_channels=obs_shape[0], out_channels=emb_dim, kernel_size=patch_size, stride=patch_size)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.PatchEmbed.patch_num","title":"patch_num  <code>instance-attribute</code>","text":"<pre><code>patch_num = obs_shape[1] // patch_size * (obs_shape[2] // patch_size)\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.PatchEmbed.patch_size","title":"patch_size  <code>instance-attribute</code>","text":"<pre><code>patch_size = patch_size\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.PatchEmbed-functions","title":"Functions","text":""},{"location":"api/layers/#ml_networks.torch.layers.PatchEmbed.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (B, C, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (B, Np, D)</p> <code>Np is the number of patches.</code> <p>Np = H*W/P^2</p> <code>D is the embedding dimension.</code> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (B, C, H, W)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (B, Np, D)\n\n    Np is the number of patches.\n        Np = H*W/P^2\n    D is the embedding dimension.\n\n    \"\"\"\n    # \u30d1\u30c3\u30c1\u306e\u57cb\u3081\u8fbc\u307f &amp;&amp; flatten[\u5f0f(3)]\n    # \u30d1\u30c3\u30c1\u306e\u57cb\u3081\u8fbc\u307f (B, C, H, W) -&gt; (B, D, H/P, W/P)\n    # \u3053\u3053\u3067\u3001P\u306f\u30d1\u30c3\u30c11\u8fba\u306e\u5927\u304d\u3055\n    x = self.patch_emb_layer(x)\n\n    # \u30d1\u30c3\u30c1\u306eflatten (B, D, H/P, W/P) -&gt; (B, D, Np)\n    # \u3053\u3053\u3067\u3001Np \u306f\u30d1\u30c3\u30c1\u306e\u6570( = H*W/P^2)\n    x = x.flatten(2)\n\n    # \u8ef8\u306e\u5165\u308c\u66ff\u3048 (B, D, Np) -&gt; (B, Np, D)\n    return x.transpose(1, 2)\n</code></pre>"},{"location":"api/layers/#positionalencoding","title":"PositionalEncoding","text":""},{"location":"api/layers/#ml_networks.torch.layers.PositionalEncoding","title":"PositionalEncoding","text":"<pre><code>PositionalEncoding(d_model, dropout, max_len)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Positional encoding.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Dimension of model.</p> required <code>dropout</code> <code>float</code> <p>Dropout rate. If it's set to 0.0, dropout is not applied.</p> required <code>max_len</code> <code>int</code> <p>Maximum length.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; pos_enc = PositionalEncoding(16, 0.1, 100)\n&gt;&gt;&gt; x = torch.randn(1, 8, 16)\n&gt;&gt;&gt; output = pos_enc(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 8, 16])\n</code></pre> <pre><code>&gt;&gt;&gt; x = torch.randn(1, 100, 16)\n&gt;&gt;&gt; output = pos_enc(x)\n&gt;&gt;&gt; output.shape\ntorch.Size([1, 100, 16])\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def __init__(self, d_model: int, dropout: float, max_len: int) -&gt; None:\n    super().__init__()\n\n    self.dropout = nn.Dropout(p=dropout) if dropout &gt; 0 else nn.Identity()\n\n    position = torch.arange(max_len).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n    pe = torch.zeros(1, max_len, d_model)\n    pe[0, :, 0::2] = torch.sin(position * div_term)\n    pe[0, :, 1::2] = torch.cos(position * div_term)\n    self.register_buffer(\"pe\", pe)\n    self.pe: torch.Tensor = pe\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.PositionalEncoding-attributes","title":"Attributes","text":""},{"location":"api/layers/#ml_networks.torch.layers.PositionalEncoding.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = Dropout(p=dropout) if dropout &gt; 0 else Identity()\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.PositionalEncoding.pe","title":"pe  <code>instance-attribute</code>","text":"<pre><code>pe = pe\n</code></pre>"},{"location":"api/layers/#ml_networks.torch.layers.PositionalEncoding-functions","title":"Functions","text":""},{"location":"api/layers/#ml_networks.torch.layers.PositionalEncoding.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (B, L, D)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (B, L, D)</p> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (B, L, D)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (B, L, D)\n    \"\"\"\n    x = x + self.pe[:, : x.size(1)]\n    return self.dropout(x)\n</code></pre>"},{"location":"api/layers/#get_norm","title":"get_norm","text":""},{"location":"api/layers/#ml_networks.torch.layers.get_norm","title":"get_norm","text":"<pre><code>get_norm(norm, **kwargs)\n</code></pre> <p>Get normalization layer.</p> <p>Parameters:</p> Name Type Description Default <code>norm</code> <code>Literal['layer', 'rms', 'group', 'batch', 'none']</code> <p>Normalization layer. If it's set to \"none\", normalization is not applied.</p> required <code>kwargs</code> <code>dict</code> <p>Normalization layer configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>Normalization layer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cfg = {\"normalized_shape\": 1, \"eps\": 1e-05, \"elementwise_affine\": True, \"bias\": True}\n&gt;&gt;&gt; norm = get_norm(\"layer\", **cfg)\n&gt;&gt;&gt; norm\nLayerNorm((1,), eps=1e-05, elementwise_affine=True)\n</code></pre> <pre><code>&gt;&gt;&gt; cfg = {\"normalized_shape\": 1, \"eps\": 1e-05, \"elementwise_affine\": True}\n&gt;&gt;&gt; norm = get_norm(\"rms\", **cfg)\n&gt;&gt;&gt; norm\nRMSNorm((1,), eps=1e-05, elementwise_affine=True)\n</code></pre> <pre><code>&gt;&gt;&gt; cfg = {\"num_groups\": 1, \"num_channels\": 12, \"eps\": 1e-05, \"affine\": True}\n&gt;&gt;&gt; norm = get_norm(\"group\", **cfg)\n&gt;&gt;&gt; norm\nGroupNorm(1, 12, eps=1e-05, affine=True)\n</code></pre> <pre><code>&gt;&gt;&gt; cfg = {\"num_features\": 1, \"eps\": 1e-05, \"momentum\": 0.1, \"affine\": True, \"track_running_stats\": True}\n&gt;&gt;&gt; norm = get_norm(\"batch2d\", **cfg)\n&gt;&gt;&gt; norm\nBatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n</code></pre> <pre><code>&gt;&gt;&gt; cfg = {\"num_features\": 1, \"eps\": 1e-05, \"momentum\": 0.1, \"affine\": True, \"track_running_stats\": True}\n&gt;&gt;&gt; norm = get_norm(\"batch1d\", **cfg)\n&gt;&gt;&gt; norm\nBatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n</code></pre> <pre><code>&gt;&gt;&gt; norm = get_norm(\"none\")\n&gt;&gt;&gt; norm\nIdentity()\n</code></pre> Source code in <code>src/ml_networks/torch/layers.py</code> <pre><code>def get_norm(\n    norm: Literal[\"layer\", \"rms\", \"group\", \"batch2d\", \"batch1d\", \"none\"],\n    **kwargs: Any,\n) -&gt; nn.Module:\n    \"\"\"\n    Get normalization layer.\n\n    Parameters\n    ----------\n    norm : Literal[\"layer\", \"rms\", \"group\", \"batch\", \"none\"]\n        Normalization layer. If it's set to \"none\", normalization is not applied.\n    kwargs : dict\n        Normalization layer configuration.\n\n    Returns\n    -------\n    nn.Module\n        Normalization layer.\n\n    Examples\n    --------\n    &gt;&gt;&gt; cfg = {\"normalized_shape\": 1, \"eps\": 1e-05, \"elementwise_affine\": True, \"bias\": True}\n    &gt;&gt;&gt; norm = get_norm(\"layer\", **cfg)\n    &gt;&gt;&gt; norm\n    LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n\n    &gt;&gt;&gt; cfg = {\"normalized_shape\": 1, \"eps\": 1e-05, \"elementwise_affine\": True}\n    &gt;&gt;&gt; norm = get_norm(\"rms\", **cfg)\n    &gt;&gt;&gt; norm\n    RMSNorm((1,), eps=1e-05, elementwise_affine=True)\n\n    &gt;&gt;&gt; cfg = {\"num_groups\": 1, \"num_channels\": 12, \"eps\": 1e-05, \"affine\": True}\n    &gt;&gt;&gt; norm = get_norm(\"group\", **cfg)\n    &gt;&gt;&gt; norm\n    GroupNorm(1, 12, eps=1e-05, affine=True)\n\n    &gt;&gt;&gt; cfg = {\"num_features\": 1, \"eps\": 1e-05, \"momentum\": 0.1, \"affine\": True, \"track_running_stats\": True}\n    &gt;&gt;&gt; norm = get_norm(\"batch2d\", **cfg)\n    &gt;&gt;&gt; norm\n    BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n    &gt;&gt;&gt; cfg = {\"num_features\": 1, \"eps\": 1e-05, \"momentum\": 0.1, \"affine\": True, \"track_running_stats\": True}\n    &gt;&gt;&gt; norm = get_norm(\"batch1d\", **cfg)\n    &gt;&gt;&gt; norm\n    BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n    &gt;&gt;&gt; norm = get_norm(\"none\")\n    &gt;&gt;&gt; norm\n    Identity()\n\n    \"\"\"\n    if norm == \"layer\":\n        return nn.LayerNorm(**kwargs)\n    if norm == \"rms\":\n        return nn.RMSNorm(**kwargs)\n    if norm == \"group\":\n        return nn.GroupNorm(**kwargs)\n    if norm == \"batch2d\":\n        return nn.BatchNorm2d(**kwargs)\n    if norm == \"batch1d\":\n        return nn.BatchNorm1d(**kwargs)\n    return nn.Identity()\n</code></pre>"},{"location":"api/loss/","title":"\u640d\u5931\u95a2\u6570","text":"<p>\u640d\u5931\u95a2\u6570\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p> <p><code>ml_networks.torch.loss</code>\uff08PyTorch\uff09\u3068<code>ml_networks.jax.loss</code>\uff08JAX\uff09\u306e\u4e21\u65b9\u3067\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"api/loss/#focal_loss","title":"focal_loss","text":""},{"location":"api/loss/#ml_networks.torch.loss.focal_loss","title":"focal_loss","text":"<pre><code>focal_loss(prediction, target, gamma=2.0, sum_dim=-1)\n</code></pre> <p>Focal loss function. Mainly for multi-class classification.</p> Reference <p>Focal Loss for Dense Object Detection https://arxiv.org/abs/1708.02002</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted tensor. This should be before softmax.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor.</p> required <code>gamma</code> <code>float</code> <p>The gamma parameter. Default is 2.0.</p> <code>2.0</code> <code>sum_dim</code> <code>int</code> <p>The dimension to sum the loss. Default is -1.</p> <code>-1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The focal loss.</p> Source code in <code>src/ml_networks/torch/loss.py</code> <pre><code>def focal_loss(\n    prediction: torch.Tensor,\n    target: torch.Tensor,\n    gamma: float = 2.0,\n    sum_dim: int = -1,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Focal loss function. Mainly for multi-class classification.\n\n    Reference\n    ---------\n    Focal Loss for Dense Object Detection\n    https://arxiv.org/abs/1708.02002\n\n    Parameters\n    ----------\n    prediction : torch.Tensor\n        The predicted tensor. This should be before softmax.\n    target : torch.Tensor\n        The target tensor.\n    gamma : float\n        The gamma parameter. Default is 2.0.\n    sum_dim : int\n        The dimension to sum the loss. Default is -1.\n\n    Returns\n    -------\n    torch.Tensor\n        The focal loss.\n\n    \"\"\"\n    prediction = prediction.unsqueeze(1).transpose(sum_dim, 1).squeeze(-1)\n    if gamma:\n        log_prob = F.log_softmax(prediction, dim=1)\n        prob = torch.exp(log_prob)\n        loss = F.nll_loss((1 - prob) ** gamma * log_prob, target, reduction=\"none\")\n    else:\n        loss = F.cross_entropy(prediction, target, reduction=\"none\")\n    return loss.mean(0).sum()\n</code></pre>"},{"location":"api/loss/#binary_focal_loss","title":"binary_focal_loss","text":""},{"location":"api/loss/#ml_networks.torch.loss.binary_focal_loss","title":"binary_focal_loss","text":"<pre><code>binary_focal_loss(prediction, target, gamma=2.0, sum_dim=-1)\n</code></pre> <p>Binary focal loss function. Mainly for binary classification.</p> Reference <p>Focal Loss for Dense Object Detection     https://arxiv.org/abs/1708.02002</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted tensor. This should be before sigmoid.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor.</p> required <code>gamma</code> <code>float</code> <p>The gamma parameter. Default is 2.0.</p> <code>2.0</code> <code>sum_dim</code> <code>int</code> <p>The dimension to sum the loss. Default is -1.</p> <code>-1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The binary focal loss.</p> Source code in <code>src/ml_networks/torch/loss.py</code> <pre><code>def binary_focal_loss(\n    prediction: torch.Tensor,\n    target: torch.Tensor,\n    gamma: float = 2.0,\n    sum_dim: int = -1,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Binary focal loss function. Mainly for binary classification.\n\n    Reference\n    ---------\n    Focal Loss for Dense Object Detection\n        https://arxiv.org/abs/1708.02002\n\n    Parameters\n    ----------\n    prediction : torch.Tensor\n        The predicted tensor. This should be before sigmoid.\n    target : torch.Tensor\n        The target tensor.\n    gamma : float\n        The gamma parameter. Default is 2.0.\n    sum_dim : int\n        The dimension to sum the loss. Default is -1.\n\n    Returns\n    -------\n    torch.Tensor\n        The binary focal loss.\n\n    \"\"\"\n    if gamma:\n        log_probs = F.logsigmoid(prediction)\n        neg_log_probs = F.logsigmoid(-prediction)\n        probs = torch.sigmoid(prediction)\n        focal_weight = torch.where(target == 1, (1 - probs) ** gamma, probs**gamma)\n        loss = torch.where(target == 1, -log_probs, -neg_log_probs)\n        loss = focal_weight * loss\n    else:\n        loss = F.binary_cross_entropy_with_logits(prediction, target, reduction=\"none\")\n    return loss.sum(sum_dim)\n</code></pre>"},{"location":"api/loss/#charbonnier","title":"charbonnier","text":""},{"location":"api/loss/#ml_networks.torch.loss.charbonnier","title":"charbonnier","text":"<pre><code>charbonnier(prediction, target, epsilon=0.001, alpha=1, sum_dim=None)\n</code></pre> <p>Charbonnier loss function.</p> Reference <p>A General and Adaptive Robust Loss Function http://arxiv.org/abs/1701.03077</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted tensor.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor.</p> required <code>epsilon</code> <code>float</code> <p>A small value to avoid division by zero. Default is 1e-3.</p> <code>0.001</code> <code>alpha</code> <code>float</code> <p>The alpha parameter. Default is 1.</p> <code>1</code> <code>sum_dim</code> <code>int | list[int] | tuple[int, ...] | None</code> <p>The dimension to sum the loss. Default is None (sums over [-1, -2, -3]).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The Charbonnier loss.</p> Source code in <code>src/ml_networks/torch/loss.py</code> <pre><code>def charbonnier(\n    prediction: torch.Tensor,\n    target: torch.Tensor,\n    epsilon: float = 1e-3,\n    alpha: float = 1,\n    sum_dim: int | list[int] | tuple[int, ...] | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Charbonnier loss function.\n\n    Reference\n    ---------\n    A General and Adaptive Robust Loss Function\n    http://arxiv.org/abs/1701.03077\n\n    Parameters\n    ----------\n    prediction : torch.Tensor\n        The predicted tensor.\n    target : torch.Tensor\n        The target tensor.\n    epsilon : float\n        A small value to avoid division by zero. Default is 1e-3.\n    alpha : float\n        The alpha parameter. Default is 1.\n    sum_dim : int | list[int] | tuple[int, ...] | None\n        The dimension to sum the loss. Default is None (sums over [-1, -2, -3]).\n\n    Returns\n    -------\n    torch.Tensor\n        The Charbonnier loss.\n\n    \"\"\"\n    if sum_dim is None:\n        sum_dim = [-1, -2, -3]\n    x = prediction - target\n    loss = (x**2 + epsilon**2) ** (alpha / 2)\n    return torch.sum(loss, dim=sum_dim)\n</code></pre>"},{"location":"api/loss/#focalfrequencyloss","title":"FocalFrequencyLoss","text":""},{"location":"api/loss/#ml_networks.torch.loss.FocalFrequencyLoss","title":"FocalFrequencyLoss","text":"<pre><code>FocalFrequencyLoss(loss_weight=1.0, alpha=1.0, patch_factor=1, ave_spectrum=False, log_matrix=False, batch_matrix=False)\n</code></pre> <p>The torch.nn.Module class that implements focal frequency loss.</p> <p>A frequency domain loss function for optimizing generative models.</p> Reference <p>Focal Frequency Loss for Image Reconstruction and Synthesis. In ICCV 2021. https://arxiv.org/pdf/2012.12821.pdf</p> <p>Parameters:</p> Name Type Description Default <code>loss_weight</code> <code>float</code> <p>weight for focal frequency loss. Default: 1.0</p> <code>1.0</code> <code>alpha</code> <code>float</code> <p>the scaling factor alpha of the spectrum weight matrix for flexibility. Default: 1.0</p> <code>1.0</code> <code>patch_factor</code> <code>int</code> <p>the factor to crop image patches for patch-based focal frequency loss. Default: 1</p> <code>1</code> <code>ave_spectrum</code> <code>bool</code> <p>whether to use minibatch average spectrum. Default: False</p> <code>False</code> <code>log_matrix</code> <code>bool</code> <p>whether to adjust the spectrum weight matrix by logarithm. Default: False</p> <code>False</code> <code>batch_matrix</code> <code>bool</code> <p>whether to calculate the spectrum weight matrix using batch-based statistics. Default: False</p> <code>False</code> Source code in <code>src/ml_networks/torch/loss.py</code> <pre><code>def __init__(\n    self,\n    loss_weight: float = 1.0,\n    alpha: float = 1.0,\n    patch_factor: int = 1,\n    ave_spectrum: bool = False,\n    log_matrix: bool = False,\n    batch_matrix: bool = False,\n) -&gt; None:\n    self.loss_weight = loss_weight\n    self.alpha = alpha\n    self.patch_factor = patch_factor\n    self.ave_spectrum = ave_spectrum\n    self.log_matrix = log_matrix\n    self.batch_matrix = batch_matrix\n</code></pre>"},{"location":"api/loss/#ml_networks.torch.loss.FocalFrequencyLoss-attributes","title":"Attributes","text":""},{"location":"api/loss/#ml_networks.torch.loss.FocalFrequencyLoss.alpha","title":"alpha  <code>instance-attribute</code>","text":"<pre><code>alpha = alpha\n</code></pre>"},{"location":"api/loss/#ml_networks.torch.loss.FocalFrequencyLoss.ave_spectrum","title":"ave_spectrum  <code>instance-attribute</code>","text":"<pre><code>ave_spectrum = ave_spectrum\n</code></pre>"},{"location":"api/loss/#ml_networks.torch.loss.FocalFrequencyLoss.batch_matrix","title":"batch_matrix  <code>instance-attribute</code>","text":"<pre><code>batch_matrix = batch_matrix\n</code></pre>"},{"location":"api/loss/#ml_networks.torch.loss.FocalFrequencyLoss.log_matrix","title":"log_matrix  <code>instance-attribute</code>","text":"<pre><code>log_matrix = log_matrix\n</code></pre>"},{"location":"api/loss/#ml_networks.torch.loss.FocalFrequencyLoss.loss_weight","title":"loss_weight  <code>instance-attribute</code>","text":"<pre><code>loss_weight = loss_weight\n</code></pre>"},{"location":"api/loss/#ml_networks.torch.loss.FocalFrequencyLoss.patch_factor","title":"patch_factor  <code>instance-attribute</code>","text":"<pre><code>patch_factor = patch_factor\n</code></pre>"},{"location":"api/loss/#ml_networks.torch.loss.FocalFrequencyLoss-functions","title":"Functions","text":""},{"location":"api/loss/#ml_networks.torch.loss.FocalFrequencyLoss.__call__","title":"__call__","text":"<pre><code>__call__(pred, target, matrix=None, mean_batch=True)\n</code></pre> <p>Forward function to calculate focal frequency loss.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>of shape (N, C, H, W). Predicted tensor.</p> required <code>target</code> <code>Tensor</code> <p>of shape (N, C, H, W). Target tensor.</p> required <code>matrix</code> <code>Tensor | None</code> <p>Default: None (If set to None: calculated online, dynamic).</p> <code>None</code> <code>mean_batch</code> <code>bool</code> <p>Whether to average over batch dimension.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The focal frequency loss.</p> Source code in <code>src/ml_networks/torch/loss.py</code> <pre><code>def __call__(\n    self,\n    pred: torch.Tensor,\n    target: torch.Tensor,\n    matrix: torch.Tensor | None = None,\n    mean_batch: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"Forward function to calculate focal frequency loss.\n\n    Parameters\n    ----------\n    pred: torch.Tensor\n        of shape (N, C, H, W). Predicted tensor.\n    target: torch.Tensor\n        of shape (N, C, H, W). Target tensor.\n    matrix: torch.Tensor | None\n        Default: None (If set to None: calculated online, dynamic).\n    mean_batch: bool\n        Whether to average over batch dimension.\n\n    Returns\n    -------\n    torch.Tensor\n        The focal frequency loss.\n    \"\"\"\n    if target.shape != pred.shape:\n        target = target.expand_as(pred)\n    if pred.ndim == 5:\n        batch_shape = pred.shape[:2]\n        pred = pred.flatten(0, 1)\n        target = target.flatten(0, 1)\n        flattened = True\n    else:\n        flattened = False\n\n    pred_freq = self.tensor2freq(pred)\n    target_freq = self.tensor2freq(target)\n\n    # whether to use minibatch average spectrum\n    if self.ave_spectrum:\n        pred_freq = torch.mean(pred_freq, 0, keepdim=True)\n        target_freq = torch.mean(target_freq, 0, keepdim=True)\n\n    # calculate focal frequency loss\n    loss = self.loss_formulation(pred_freq, target_freq, matrix, mean_batch) * self.loss_weight\n    if flattened and not mean_batch:\n        loss = loss.reshape(batch_shape)\n    return loss\n</code></pre>"},{"location":"api/loss/#ml_networks.torch.loss.FocalFrequencyLoss.loss_formulation","title":"loss_formulation","text":"<pre><code>loss_formulation(recon_freq, real_freq, matrix=None, mean_batch=True)\n</code></pre> Source code in <code>src/ml_networks/torch/loss.py</code> <pre><code>def loss_formulation(\n    self,\n    recon_freq: torch.Tensor,\n    real_freq: torch.Tensor,\n    matrix: torch.Tensor | None = None,\n    mean_batch: bool = True,\n) -&gt; torch.Tensor:\n    # spectrum weight matrix\n    if matrix is not None:\n        # if the matrix is predefined\n        weight_matrix = matrix.detach()\n    else:\n        # if the matrix is calculated online: continuous, dynamic, based on current Euclidean distance\n        matrix_tmp = (recon_freq - real_freq) ** 2\n        matrix_tmp = torch.sqrt(matrix_tmp[..., 0] + matrix_tmp[..., 1]) ** self.alpha\n\n        # whether to adjust the spectrum weight matrix by logarithm\n        if self.log_matrix:\n            matrix_tmp = torch.log(matrix_tmp + 1.0)\n\n        # whether to calculate the spectrum weight matrix using batch-based statistics\n        if self.batch_matrix:\n            matrix_tmp = matrix_tmp / matrix_tmp.max()\n        else:\n            matrix_tmp = matrix_tmp / matrix_tmp.max(-1).values.max(-1).values[:, :, :, None, None]\n\n        matrix_tmp[torch.isnan(matrix_tmp)] = 0.0\n        matrix_tmp = torch.clamp(matrix_tmp, min=0.0, max=1.0)\n        weight_matrix = matrix_tmp.clone().detach()\n\n    min_val = weight_matrix.min().item()\n    max_val = weight_matrix.max().item()\n    assert min_val &gt;= 0, f\"The values of spectrum weight matrix should be &gt;= 0, but got Min: {min_val:.10f}\"\n    assert max_val &lt;= 1, f\"The values of spectrum weight matrix should be &lt;= 1, but got Max: {max_val:.10f}\"\n\n    # frequency distance using (squared) Euclidean distance\n    tmp = (recon_freq - real_freq) ** 2\n    freq_distance = tmp[..., 0] + tmp[..., 1]\n\n    # dynamic spectrum weighting (Hadamard product)\n    loss = weight_matrix * freq_distance\n    loss = loss.sum(dim=[-1, -2, -3])\n    if mean_batch:\n        loss = loss.mean()\n    return loss\n</code></pre>"},{"location":"api/loss/#ml_networks.torch.loss.FocalFrequencyLoss.tensor2freq","title":"tensor2freq","text":"<pre><code>tensor2freq(x)\n</code></pre> Source code in <code>src/ml_networks/torch/loss.py</code> <pre><code>def tensor2freq(self, x: torch.Tensor) -&gt; torch.Tensor:\n    # crop image patches\n    patch_factor = self.patch_factor\n    _, _, h, w = x.shape\n    assert h % patch_factor == 0, \"Patch factor should be divisible by image height\"\n    assert w % patch_factor == 0, \"Patch factor should be divisible by image width\"\n    patch_h = h // patch_factor\n    patch_w = w // patch_factor\n    patch_list: list[torch.Tensor] = [\n        x[:, :, i * patch_h : (i + 1) * patch_h, j * patch_w : (j + 1) * patch_w]\n        for i in range(patch_factor)\n        for j in range(patch_factor)\n    ]\n\n    # stack to patch tensor\n    y = torch.stack(patch_list, 1)\n\n    # perform 2D DFT (real-to-complex, orthonormalization)\n    if IS_HIGH_VERSION:\n        freq = torch.fft.fft2(y, norm=\"ortho\")\n        freq = torch.stack([freq.real, freq.imag], -1)\n    else:\n        freq = torch.rfft(y, 2, onesided=False, normalized=True)  # type: ignore[attr-defined]\n    return freq\n</code></pre>"},{"location":"api/loss/#kl_divergence","title":"kl_divergence","text":""},{"location":"api/loss/#ml_networks.torch.loss.kl_divergence","title":"kl_divergence","text":"<pre><code>kl_divergence(posterior, prior)\n</code></pre> <p>KL divergence between two distributions for StochState in ml-networks.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>StochState</code> <p>The posterior distribution.</p> required <code>prior</code> <code>StochState</code> <p>The prior distribution.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The KL divergence between the two distributions.</p> Source code in <code>src/ml_networks/torch/loss.py</code> <pre><code>def kl_divergence(posterior: StochState, prior: StochState) -&gt; torch.Tensor:\n    \"\"\"KL divergence between two distributions for StochState in ml-networks.\n\n    Parameters\n    ----------\n    posterior : StochState\n        The posterior distribution.\n    prior : StochState\n        The prior distribution.\n\n    Returns\n    -------\n    torch.Tensor\n        The KL divergence between the two distributions.\n\n    \"\"\"\n    return D.kl_divergence(posterior.get_distribution(), prior.get_distribution())\n</code></pre>"},{"location":"api/loss/#kl_balancing","title":"kl_balancing","text":""},{"location":"api/loss/#ml_networks.torch.loss.kl_balancing","title":"kl_balancing","text":"<pre><code>kl_balancing(posterior, prior, weight=0.8)\n</code></pre> <p>KL balancing loss function for StochState in ml-networks.</p> Reference <p>Mastering Atari with Discrete World Models. In NeurIPS 2020. https://arxiv.org/abs/2010.02193</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>StochState</code> <p>The posterior distribution.</p> required <code>prior</code> <code>StochState</code> <p>The prior distribution.</p> required <code>weight</code> <code>float</code> <p>The weight of prior gradient for the balancing. Default is 0.8.</p> <code>0.8</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The KL balancing loss.</p> Source code in <code>src/ml_networks/torch/loss.py</code> <pre><code>def kl_balancing(posterior: StochState, prior: StochState, weight: float = 0.8) -&gt; torch.Tensor:\n    \"\"\"\n    KL balancing loss function for StochState in ml-networks.\n\n    Reference\n    ---------\n    Mastering Atari with Discrete World Models. In NeurIPS 2020.\n    https://arxiv.org/abs/2010.02193\n\n    Parameters\n    ----------\n    posterior : StochState\n        The posterior distribution.\n    prior : StochState\n        The prior distribution.\n    weight : float\n        The weight of prior gradient for the balancing. Default is 0.8.\n\n    Returns\n    -------\n    torch.Tensor\n        The KL balancing loss.\n\n    \"\"\"\n    assert 0 &lt;= weight &lt;= 1, \"weight should be in the range [0, 1]\"\n    kld_prior = weight * D.kl_divergence(\n        posterior.detach().get_distribution(),\n        prior.get_distribution(),\n    )\n\n    kld_posterior = (1 - weight) * D.kl_divergence(\n        posterior.get_distribution(),\n        prior.detach().get_distribution(),\n    )\n\n    return kld_prior + kld_posterior\n</code></pre>"},{"location":"api/others/","title":"\u305d\u306e\u4ed6","text":"<p>\u305d\u306e\u4ed6\u306e\u30af\u30e9\u30b9\u3068\u95a2\u6570\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p>"},{"location":"api/others/#basemodule","title":"BaseModule","text":"<p>PyTorch Lightning\u306e<code>LightningModule</code>\u3092\u62e1\u5f35\u3057\u305f\u57fa\u5e95\u30af\u30e9\u30b9\u3067\u3059\u3002</p>"},{"location":"api/others/#ml_networks.torch.base.BaseModule","title":"BaseModule","text":"<p>               Bases: <code>LightningModule</code></p> <p>Base module for PyTorch Lightning.</p>"},{"location":"api/others/#ml_networks.torch.base.BaseModule-functions","title":"Functions","text":""},{"location":"api/others/#ml_networks.torch.base.BaseModule.freeze_biases","title":"freeze_biases","text":"<pre><code>freeze_biases()\n</code></pre> <p>Freeze all bias parameters.</p> Source code in <code>src/ml_networks/torch/base.py</code> <pre><code>def freeze_biases(self) -&gt; None:\n    \"\"\"Freeze all bias parameters.\"\"\"\n    for name, param in self.named_parameters():\n        if \"bias\" in name:\n            param.requires_grad = False\n</code></pre>"},{"location":"api/others/#ml_networks.torch.base.BaseModule.freeze_weights","title":"freeze_weights","text":"<pre><code>freeze_weights()\n</code></pre> <p>Freeze all weight parameters.</p> Source code in <code>src/ml_networks/torch/base.py</code> <pre><code>def freeze_weights(self) -&gt; None:\n    \"\"\"Freeze all weight parameters.\"\"\"\n    for name, param in self.named_parameters():\n        if \"weight\" in name:\n            param.requires_grad = False\n</code></pre>"},{"location":"api/others/#ml_networks.torch.base.BaseModule.unfreeze_biases","title":"unfreeze_biases","text":"<pre><code>unfreeze_biases()\n</code></pre> <p>Unfreeze all bias parameters.</p> Source code in <code>src/ml_networks/torch/base.py</code> <pre><code>def unfreeze_biases(self) -&gt; None:\n    \"\"\"Unfreeze all bias parameters.\"\"\"\n    for name, param in self.named_parameters():\n        if \"bias\" in name:\n            param.requires_grad = True\n</code></pre>"},{"location":"api/others/#ml_networks.torch.base.BaseModule.unfreeze_weights","title":"unfreeze_weights","text":"<pre><code>unfreeze_weights()\n</code></pre> <p>Unfreeze all weight parameters.</p> Source code in <code>src/ml_networks/torch/base.py</code> <pre><code>def unfreeze_weights(self) -&gt; None:\n    \"\"\"Unfreeze all weight parameters.\"\"\"\n    for name, param in self.named_parameters():\n        if \"weight\" in name:\n            param.requires_grad = True\n</code></pre>"},{"location":"api/others/#hypernet","title":"HyperNet","text":"<p>\u3042\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08HyperNet\uff09\u304c\u5225\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08TargetNet\uff09\u306e\u91cd\u307f\u3092\u52d5\u7684\u306b\u751f\u6210\u3059\u308b\u30e1\u30bf\u5b66\u7fd2\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3059\u3002</p>"},{"location":"api/others/#ml_networks.torch.hypernetworks.HyperNet","title":"HyperNet","text":"<pre><code>HyperNet(input_dim, output_params, fc_cfg=None, encoding=None)\n</code></pre> <p>               Bases: <code>LightningModule</code>, <code>HyperNetMixin</code></p> <p>A hypernetwork that generates weights for a target network. Shape is dict[str, Tuple[int, ...]].</p> <p>Args:     input_dim: The dimension of the input.     output_shapes: The shapes of the primary network weights being predicted.     mlp_cfg: Configuration for the MLP backbone.     encoding: The input encoding mode. Defaults to None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ml_networks.config import MLPConfig, LinearConfig\n&gt;&gt;&gt; input_dim = 10\n&gt;&gt;&gt; cond_dim = 128\n&gt;&gt;&gt; target_net = nn.Linear(10, 5)\n&gt;&gt;&gt; output_params = target_net.state_dict()\n&gt;&gt;&gt; mlp_cfg = MLPConfig(\n...     hidden_dim=64,\n...     n_layers=2,\n...     output_activation=\"ReLU\",\n...     linear_cfg=LinearConfig(\n...         activation=\"ReLU\",\n...     )\n... )\n&gt;&gt;&gt; net = HyperNet(cond_dim, output_params, mlp_cfg)\n&gt;&gt;&gt; condition = torch.randn(2, cond_dim)\n&gt;&gt;&gt; x = torch.randn(2, input_dim)\n&gt;&gt;&gt; param = net(condition)\n&gt;&gt;&gt; outputs = torch.func.functional_call(target_net, param, x)\n&gt;&gt;&gt; outputs.shape\ntorch.Size([2, 5])\n</code></pre> Source code in <code>src/ml_networks/torch/hypernetworks.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    output_params: dict[str, Shape],\n    fc_cfg: MLPConfig | None = None,\n    encoding: InputMode = None,\n) -&gt; None:\n    super().__init__()\n\n    self.input_dim = input_dim\n    self.output_shapes = output_params\n    self.encoding = encoding\n\n    # Cache this property to avoid recomputation\n    self._output_offsets = self.output_offsets()\n\n    self.backbone: nn.Module\n    if fc_cfg is not None:\n        self.backbone = MLPLayer(\n            self.input_dim,\n            self.flat_output_size(),\n            fc_cfg,\n        )\n    else:\n        self.backbone = nn.Linear(\n            self.input_dim,\n            self.flat_output_size(),\n        )\n</code></pre>"},{"location":"api/others/#ml_networks.torch.hypernetworks.HyperNet-attributes","title":"Attributes","text":""},{"location":"api/others/#ml_networks.torch.hypernetworks.HyperNet.backbone","title":"backbone  <code>instance-attribute</code>","text":"<pre><code>backbone\n</code></pre>"},{"location":"api/others/#ml_networks.torch.hypernetworks.HyperNet.encoding","title":"encoding  <code>instance-attribute</code>","text":"<pre><code>encoding = encoding\n</code></pre>"},{"location":"api/others/#ml_networks.torch.hypernetworks.HyperNet.input_dim","title":"input_dim  <code>instance-attribute</code>","text":"<pre><code>input_dim = input_dim\n</code></pre>"},{"location":"api/others/#ml_networks.torch.hypernetworks.HyperNet.output_shapes","title":"output_shapes  <code>instance-attribute</code>","text":"<pre><code>output_shapes = output_params\n</code></pre>"},{"location":"api/others/#ml_networks.torch.hypernetworks.HyperNet-functions","title":"Functions","text":""},{"location":"api/others/#ml_networks.torch.hypernetworks.HyperNet.forward","title":"forward","text":"<pre><code>forward(inputs)\n</code></pre> <p>Perform a forward pass of the neural network.</p> <p>Args:     inputs: The input tensors.</p> <p>Returns:</p> Type Description <code>    A dictionary of output tensors.</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ml_networks.config import MLPConfig, LinearConfig\n&gt;&gt;&gt; input_dim = 10\n&gt;&gt;&gt; output_shapes = {\"weight\": (5, 10), \"bias\": (5,)}\n&gt;&gt;&gt; mlp_cfg = MLPConfig(\n...     hidden_dim=64,\n...     n_layers=2,\n...     output_activation=\"ReLU\",\n...     linear_cfg=LinearConfig(\n...         activation=\"ReLU\",\n...         norm=\"none\",\n...         dropout=0.0,\n...         bias=True\n...     )\n... )\n&gt;&gt;&gt; net = HyperNet(input_dim, output_shapes, mlp_cfg)\n&gt;&gt;&gt; x = torch.randn(2, input_dim)\n&gt;&gt;&gt; outputs = net(x)\n&gt;&gt;&gt; outputs[\"weight\"].shape\ntorch.Size([2, 5, 10])\n&gt;&gt;&gt; outputs[\"bias\"].shape\ntorch.Size([2, 5])\n</code></pre> Source code in <code>src/ml_networks/torch/hypernetworks.py</code> <pre><code>def forward(self, inputs: torch.Tensor) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Perform a forward pass of the neural network.\n\n    Args:\n        inputs: The input tensors.\n\n    Returns\n    -------\n        A dictionary of output tensors.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from ml_networks.config import MLPConfig, LinearConfig\n    &gt;&gt;&gt; input_dim = 10\n    &gt;&gt;&gt; output_shapes = {\"weight\": (5, 10), \"bias\": (5,)}\n    &gt;&gt;&gt; mlp_cfg = MLPConfig(\n    ...     hidden_dim=64,\n    ...     n_layers=2,\n    ...     output_activation=\"ReLU\",\n    ...     linear_cfg=LinearConfig(\n    ...         activation=\"ReLU\",\n    ...         norm=\"none\",\n    ...         dropout=0.0,\n    ...         bias=True\n    ...     )\n    ... )\n    &gt;&gt;&gt; net = HyperNet(input_dim, output_shapes, mlp_cfg)\n    &gt;&gt;&gt; x = torch.randn(2, input_dim)\n    &gt;&gt;&gt; outputs = net(x)\n    &gt;&gt;&gt; outputs[\"weight\"].shape\n    torch.Size([2, 5, 10])\n    &gt;&gt;&gt; outputs[\"bias\"].shape\n    torch.Size([2, 5])\n    \"\"\"\n    if self.encoding is not None:\n        inputs = encode_input(inputs, self.encoding)\n\n    flat_output = self.backbone(inputs)\n    return self.unflatten_output(flat_output)\n</code></pre>"},{"location":"api/others/#contrastivelearningloss","title":"ContrastiveLearningLoss","text":"<p>\u5bfe\u7167\u5b66\u7fd2\uff08Contrastive Learning\uff09\u306e\u305f\u3081\u306e\u640d\u5931\u95a2\u6570\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3059\u3002</p>"},{"location":"api/others/#ml_networks.torch.contrastive.ContrastiveLearningLoss","title":"ContrastiveLearningLoss","text":"<pre><code>ContrastiveLearningLoss(dim_input1, dim_input2, cfg)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>Contrastive learning module.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>ContrastiveLearningConfig</code> <p>Configuration for contrastive learning.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ml_networks.config import ContrastiveLearningConfig, MLPConfig, LinearConfig\n&gt;&gt;&gt; cfg = ContrastiveLearningConfig(\n...     dim_feature=128,\n...     dim_input1=256,\n...     dim_input2=256,\n...     eval_func=MLPConfig(\n...         hidden_dim=256,\n...         n_layers=2,\n...         output_activation=\"ReLU\",\n...         linear_cfg=LinearConfig(\n...             activation=\"ReLU\",\n...             norm=\"layer\",\n...             norm_cfg={\"eps\": 1e-05, \"elementwise_affine\": True, \"bias\": True},\n...             dropout=0.1,\n...             norm_first=False,\n...             bias=True\n...         )\n...     ),\n...     cross_entropy_like=False\n... )\n&gt;&gt;&gt; model = ContrastiveLearningLoss(cfg)\n&gt;&gt;&gt; x1 = torch.randn(2, 256)\n&gt;&gt;&gt; x2 = torch.randn(2, 256)\n&gt;&gt;&gt; output = model.calc_nce(x1, x2)\n&gt;&gt;&gt; output[\"nce\"].shape\ntorch.Size([])\n&gt;&gt;&gt; output, embeddings = model.calc_nce(x1, x2, return_emb=True)\n&gt;&gt;&gt; embeddings[0].shape, embeddings[1].shape\n(torch.Size([2, 128]), torch.Size([2, 128]))\n</code></pre> Source code in <code>src/ml_networks/torch/contrastive.py</code> <pre><code>def __init__(\n    self,\n    dim_input1: int,\n    dim_input2: int,\n    cfg: ContrastiveLearningConfig,\n) -&gt; None:\n    super().__init__()\n    self.cfg = cfg\n    self.dim_feature = cfg.dim_feature\n    self.dim_input1 = dim_input1\n    self.dim_input2 = dim_input2\n    self.is_ce_like = cfg.cross_entropy_like\n\n    self.eval_func = MLPLayer(dim_input1, cfg.dim_feature, cfg.eval_func)\n    if self.dim_input1 != self.dim_input2:\n        self.eval_func2 = MLPLayer(dim_input2, cfg.dim_feature, cfg.eval_func)\n    else:\n        self.eval_func2 = self.eval_func\n</code></pre>"},{"location":"api/others/#ml_networks.torch.contrastive.ContrastiveLearningLoss-attributes","title":"Attributes","text":""},{"location":"api/others/#ml_networks.torch.contrastive.ContrastiveLearningLoss.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/others/#ml_networks.torch.contrastive.ContrastiveLearningLoss.dim_feature","title":"dim_feature  <code>instance-attribute</code>","text":"<pre><code>dim_feature = dim_feature\n</code></pre>"},{"location":"api/others/#ml_networks.torch.contrastive.ContrastiveLearningLoss.dim_input1","title":"dim_input1  <code>instance-attribute</code>","text":"<pre><code>dim_input1 = dim_input1\n</code></pre>"},{"location":"api/others/#ml_networks.torch.contrastive.ContrastiveLearningLoss.dim_input2","title":"dim_input2  <code>instance-attribute</code>","text":"<pre><code>dim_input2 = dim_input2\n</code></pre>"},{"location":"api/others/#ml_networks.torch.contrastive.ContrastiveLearningLoss.eval_func","title":"eval_func  <code>instance-attribute</code>","text":"<pre><code>eval_func = MLPLayer(dim_input1, dim_feature, eval_func)\n</code></pre>"},{"location":"api/others/#ml_networks.torch.contrastive.ContrastiveLearningLoss.eval_func2","title":"eval_func2  <code>instance-attribute</code>","text":"<pre><code>eval_func2 = MLPLayer(dim_input2, dim_feature, eval_func)\n</code></pre>"},{"location":"api/others/#ml_networks.torch.contrastive.ContrastiveLearningLoss.is_ce_like","title":"is_ce_like  <code>instance-attribute</code>","text":"<pre><code>is_ce_like = cross_entropy_like\n</code></pre>"},{"location":"api/others/#ml_networks.torch.contrastive.ContrastiveLearningLoss-functions","title":"Functions","text":""},{"location":"api/others/#ml_networks.torch.contrastive.ContrastiveLearningLoss.calc_nce","title":"calc_nce","text":"<pre><code>calc_nce(feature1, feature2, return_emb=False)\n</code></pre> <p>Calculate the Noise Contrastive Estimation (NCE) loss.</p> <p>Parameters:</p> Name Type Description Default <code>feature1</code> <code>Tensor</code> <p>First input tensor of shape (*, dim_input1)</p> required <code>feature2</code> <code>Tensor</code> <p>Second input tensor of shape (*, dim_input2)</p> required <code>return_emb</code> <code>bool</code> <p>Whether to return embeddings, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[dict[str, Tensor], Tuple[dict[str, Tensor], Tuple[Tensor, Tensor]]]</code> <p>If return_emb is False, returns loss dictionary. If return_emb is True, returns (loss dictionary, (embeddings1, embeddings2))</p> Source code in <code>src/ml_networks/torch/contrastive.py</code> <pre><code>def calc_nce(\n    self,\n    feature1: torch.Tensor,\n    feature2: torch.Tensor,\n    return_emb: bool = False,\n) -&gt; dict[str, torch.Tensor] | tuple[dict[str, torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n    \"\"\"\n    Calculate the Noise Contrastive Estimation (NCE) loss.\n\n    Parameters\n    ----------\n    feature1 : torch.Tensor\n        First input tensor of shape (*, dim_input1)\n    feature2 : torch.Tensor\n        Second input tensor of shape (*, dim_input2)\n    return_emb : bool, optional\n        Whether to return embeddings, by default False\n\n    Returns\n    -------\n    Union[dict[str, torch.Tensor], Tuple[dict[str, torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]]\n        If return_emb is False, returns loss dictionary.\n        If return_emb is True, returns (loss dictionary, (embeddings1, embeddings2))\n    \"\"\"\n    loss_dict: dict[str, torch.Tensor] = {}\n    batch_shape = feature1.shape[:-1]\n    emb_1 = self.eval_func(feature1.reshape(-1, self.dim_input1))\n    emb_2 = self.eval_func2(feature2.reshape(-1, self.dim_input2))\n\n    if self.is_ce_like:\n        labels = torch.arange(len(emb_1), device=emb_1.device)\n        sim_matrix = torch.mm(emb_1, emb_2.T)\n        nce_loss = F.cross_entropy(sim_matrix, labels, reduction=\"none\") - np.log(len(sim_matrix))\n        loss_dict[\"nce\"] = nce_loss\n    else:\n        positive = torch.sum(emb_1 * emb_2, dim=-1)\n        loss_dict[\"positive\"] = positive.detach().clone().mean()\n\n        sim_matrix = torch.mm(emb_1, emb_2.T)\n        negative = torch.logsumexp(sim_matrix, dim=-1) - np.log(len(sim_matrix))\n        loss_dict[\"negative\"] = negative.detach().clone().mean()\n\n        nce_loss = -positive + negative\n        loss_dict[\"nce\"] = nce_loss.reshape(batch_shape)\n\n    if return_emb:\n        return loss_dict, (emb_1, emb_2)\n    return loss_dict\n</code></pre>"},{"location":"api/others/#ml_networks.torch.contrastive.ContrastiveLearningLoss.calc_sigmoid","title":"calc_sigmoid","text":"<pre><code>calc_sigmoid(feature1, feature2, return_emb=False, temperature=0.1, bias=0.0)\n</code></pre> <p>Calculate the Sigmoid loss for contrastive learning.</p> <p>Parameters:</p> Name Type Description Default <code>feature1</code> <code>Tensor</code> <p>First input tensor of shape (*, dim_input1)</p> required <code>feature2</code> <code>Tensor</code> <p>Second input tensor of shape (*, dim_input2)</p> required <code>return_emb</code> <code>bool</code> <p>Whether to return embeddings, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[dict[str, Tensor], Tuple[dict[str, Tensor], Tuple[Tensor, Tensor]]]</code> <p>If return_emb is False, returns loss dictionary. If return_emb is True, returns (loss dictionary, (embeddings1, embeddings2))</p> Source code in <code>src/ml_networks/torch/contrastive.py</code> <pre><code>def calc_sigmoid(\n    self,\n    feature1: torch.Tensor,\n    feature2: torch.Tensor,\n    return_emb: bool = False,\n    temperature: float | torch.Tensor = 0.1,\n    bias: float | torch.Tensor = 0.0,\n) -&gt; dict[str, torch.Tensor] | tuple[dict[str, torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n    \"\"\"\n    Calculate the Sigmoid loss for contrastive learning.\n\n    Parameters\n    ----------\n    feature1 : torch.Tensor\n        First input tensor of shape (*, dim_input1)\n    feature2 : torch.Tensor\n        Second input tensor of shape (*, dim_input2)\n    return_emb : bool, optional\n        Whether to return embeddings, by default False\n\n    Returns\n    -------\n    Union[dict[str, torch.Tensor], Tuple[dict[str, torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]]\n        If return_emb is False, returns loss dictionary.\n        If return_emb is True, returns (loss dictionary, (embeddings1, embeddings2))\n    \"\"\"\n    loss_dict: dict[str, torch.Tensor] = {}\n    batch_shape = feature1.shape[:-1]\n    emb_1 = self.eval_func(feature1.reshape(-1, self.dim_input1))\n    emb_2 = self.eval_func2(feature2.reshape(-1, self.dim_input2))\n\n    logits = torch.matmul(emb_1, emb_2.T) * temperature + bias\n    labels = torch.eye(len(logits), device=logits.device) * 2 - 1\n    loss = -F.logsigmoid(logits * labels).sum(-1)\n    loss_dict[\"sigmoid\"] = loss.reshape(batch_shape)\n    if return_emb:\n        return loss_dict, (emb_1, emb_2)\n    return loss_dict\n</code></pre>"},{"location":"api/others/#ml_networks.torch.contrastive.ContrastiveLearningLoss.calc_timeseries_nce","title":"calc_timeseries_nce","text":"<pre><code>calc_timeseries_nce(feature1, feature2, positive_range_self=0, positive_range_tgt=0, return_emb=False)\n</code></pre> <p>Calculate the Noise Contrastive Estimation (NCE) loss for time series data.</p> <p>Parameters:</p> Name Type Description Default <code>feature1</code> <code>Tensor</code> <p>First input tensor of shape (*batch, length, dim_input1)</p> required <code>feature2</code> <code>Tensor</code> <p>Second input tensor of shape (*batch, length, dim_input2)</p> required <code>positive_range_self</code> <code>int</code> <p>Range for self-positive samples, by default 0</p> <code>0</code> <code>positive_range_tgt</code> <code>int</code> <p>Range for target-positive samples, by default 0</p> <code>0</code> <code>return_emb</code> <code>bool</code> <p>Whether to return embeddings, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[dict[str, Tensor], Tuple[dict[str, Tensor], Tuple[Tensor, Tensor]]]</code> <p>If return_emb is False, returns loss dictionary. If return_emb is True, returns (loss dictionary, (embeddings1, embeddings2))</p> Source code in <code>src/ml_networks/torch/contrastive.py</code> <pre><code>def calc_timeseries_nce(\n    self,\n    feature1: torch.Tensor,\n    feature2: torch.Tensor,\n    positive_range_self: int = 0,\n    positive_range_tgt: int = 0,\n    return_emb: bool = False,\n) -&gt; dict[str, torch.Tensor] | tuple[dict[str, torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n    \"\"\"\n    Calculate the Noise Contrastive Estimation (NCE) loss for time series data.\n\n    Parameters\n    ----------\n    feature1 : torch.Tensor\n        First input tensor of shape (*batch, length, dim_input1)\n    feature2 : torch.Tensor\n        Second input tensor of shape (*batch, length, dim_input2)\n    positive_range_self : int, optional\n        Range for self-positive samples, by default 0\n    positive_range_tgt : int, optional\n        Range for target-positive samples, by default 0\n    return_emb : bool, optional\n        Whether to return embeddings, by default False\n\n    Returns\n    -------\n    Union[dict[str, torch.Tensor], Tuple[dict[str, torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]]\n        If return_emb is False, returns loss dictionary.\n        If return_emb is True, returns (loss dictionary, (embeddings1, embeddings2))\n    \"\"\"\n    if not positive_range_self and not positive_range_tgt:\n        return self.calc_nce(feature1, feature2, return_emb)\n\n    # Reshape inputs\n    feature1 = feature1.reshape(-1, feature1.shape[-2], self.dim_input1)\n    feature2 = feature2.reshape(-1, feature2.shape[-2], self.dim_input2)\n    batch, length, _ = feature1.shape\n\n    # Calculate embeddings\n    emb_1 = self.eval_func(feature1)\n    emb_2 = self.eval_func2(feature2)\n\n    # Initialize loss dictionary\n    loss_dict: dict[str, torch.Tensor] = {}\n\n    # Calculate positive pairs\n    positive = torch.sum(emb_1.flatten(0, 1) * emb_2.flatten(0, 1), dim=-1)  # (batch*length)\n    loss_dict[\"positive\"] = positive.detach().clone().mean()\n\n    # Calculate self-positive pairs if needed\n    if positive_range_self &gt; 0:\n        self_positive_1, self_positive_2 = self._calculate_self_positive_pairs(\n            emb_1,\n            emb_2,\n            batch,\n            length,\n            positive_range_self,\n        )\n        positive += self_positive_1.flatten(0, 1) + self_positive_2.flatten(0, 1)\n        loss_dict[\"self_positive_1\"] = self_positive_1.detach().clone().mean()\n        loss_dict[\"self_positive_2\"] = self_positive_2.detach().clone().mean()\n\n    # Calculate target-positive pairs if needed\n    if positive_range_tgt &gt; 0:\n        tgt_positive = self._calculate_target_positive_pairs(\n            emb_1,\n            emb_2,\n            batch,\n            length,\n            positive_range_tgt,\n        )\n        positive += tgt_positive.flatten(0, 1)\n        loss_dict[\"tgt_positive\"] = tgt_positive.detach().clone().mean()\n\n    # Calculate negative pairs\n    sim_matrix = torch.mm(emb_1.flatten(0, 1), emb_2.flatten(0, 1).T)\n    negative = torch.logsumexp(sim_matrix, dim=-1) - np.log(len(sim_matrix))\n    loss_dict[\"negative\"] = negative.detach().clone().mean()\n\n    # Calculate final loss\n    nce_loss = -positive + negative\n    nce_loss = nce_loss.mean()\n    loss_dict[\"nce\"] = nce_loss\n\n    if return_emb:\n        return loss_dict, (emb_1, emb_2)\n    return loss_dict\n</code></pre>"},{"location":"api/others/#progressbarcallback","title":"ProgressBarCallback","text":"<p>PyTorch Lightning\u306eRich\u30d7\u30ed\u30b0\u30ec\u30b9\u30d0\u30fc\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u3067\u3059\u3002</p>"},{"location":"api/others/#ml_networks.callbacks.ProgressBarCallback","title":"ProgressBarCallback","text":"<pre><code>ProgressBarCallback()\n</code></pre> <p>               Bases: <code>RichProgressBar</code></p> <p>Make the progress bar richer.</p> References <ul> <li>https://qiita.com/akihironitta/items/edfd6b29dfb67b17fb00</li> </ul> <p>Rich progress bar with custom theme.</p> Source code in <code>src/ml_networks/callbacks.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Rich progress bar with custom theme.\"\"\"\n    theme = RichProgressBarTheme(\n        description=\"green_yellow\",\n        progress_bar=\"green1\",\n        progress_bar_finished=\"green1\",\n        batch_progress=\"green_yellow\",\n        time=\"grey82\",\n        processing_speed=\"grey82\",\n        metrics=\"grey82\",\n    )\n    super().__init__(theme=theme)\n</code></pre>"},{"location":"api/others/#ml_networks.callbacks.ProgressBarCallback-functions","title":"Functions","text":""},{"location":"api/unet/","title":"UNet","text":"<p>\u6761\u4ef6\u4ed8\u304dUNet\u95a2\u9023\u306e\u30af\u30e9\u30b9\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p> <p><code>ml_networks.torch.unet</code>\uff08PyTorch\uff09\u3068<code>ml_networks.jax.unet</code>\uff08JAX\uff09\u306e\u4e21\u65b9\u3067\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"api/unet/#conditionalunet2d","title":"ConditionalUnet2d","text":"<p>2D\u753b\u50cf\u30c7\u30fc\u30bf\u7528\u306e\u6761\u4ef6\u4ed8\u304dUNet\u3002Diffusion Model\u306e\u30ce\u30a4\u30ba\u4e88\u6e2c\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3068\u3057\u3066\u5178\u578b\u7684\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002</p>"},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet2d","title":"ConditionalUnet2d","text":"<pre><code>ConditionalUnet2d(feature_dim, obs_shape, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>\u6761\u4ef6\u4ed8\u304dUNet\u30e2\u30c7\u30eb.</p> <p>Args:     feature_dim (int): \u6761\u4ef6\u4ed8\u304d\u7279\u5fb4\u91cf\u306e\u6b21\u5143\u6570     obs_shape (tuple[int, int, int]): \u89b3\u6e2c\u30c7\u30fc\u30bf\u306e\u5f62\u72b6 (\u30c1\u30e3\u30f3\u30cd\u30eb\u6570, \u9ad8\u3055, \u5e45)     cfg (UNetConfig): UNet\u306e\u8a2d\u5b9a</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ml_networks.config import UNetConfig, ConvConfig, MLPConfig, LinearConfig\n&gt;&gt;&gt; cfg = UNetConfig(\n...     channels=[64, 128, 256],\n...     conv_cfg=ConvConfig(\n...         kernel_size=3,\n...         padding=1,\n...         stride=1,\n...         groups=1,\n...         activation=\"ReLU\",\n...         dropout=0.0\n...     ),\n...     has_attn=True,\n...     nhead=8,\n...     cond_pred_scale=True\n... )\n&gt;&gt;&gt; net = ConditionalUnet2d(feature_dim=32, obs_shape=(3, 64, 64), cfg=cfg)\n&gt;&gt;&gt; x = torch.randn(2, 3, 64, 64)\n&gt;&gt;&gt; cond = torch.randn(2, 32)\n&gt;&gt;&gt; out = net(x, cond)\n&gt;&gt;&gt; out.shape\ntorch.Size([2, 3, 64, 64])\n</code></pre> Source code in <code>src/ml_networks/torch/unet.py</code> <pre><code>def __init__(\n    self,\n    feature_dim: int,\n    obs_shape: tuple[int, int, int],\n    cfg: UNetConfig,\n) -&gt; None:\n    super().__init__()\n    all_dims = [obs_shape[0], *list(cfg.channels)]\n    start_dim = cfg.channels[0]\n    self.obs_shape = obs_shape\n\n    in_out = list(pairwise(all_dims))\n\n    mid_dim = all_dims[-1]\n    self.mid_modules = nn.ModuleList([\n        ConditionalResidualBlock2d(\n            mid_dim,\n            mid_dim,\n            cond_dim=feature_dim,\n            conv_cfg=cfg.conv_cfg,\n            cond_predict_scale=cfg.cond_pred_scale,\n        ),\n        Attention2d(mid_dim, cfg.nhead) if cfg.has_attn and cfg.nhead is not None else nn.Identity(),\n        ConditionalResidualBlock2d(\n            mid_dim,\n            mid_dim,\n            cond_dim=feature_dim,\n            conv_cfg=cfg.conv_cfg,\n            cond_predict_scale=cfg.cond_pred_scale,\n        ),\n    ])\n\n    down_modules = nn.ModuleList([])\n    for ind, (dim_in, dim_out) in enumerate(in_out):\n        is_last = ind &gt;= (len(in_out) - 1)\n        down_modules.append(\n            nn.ModuleList([\n                ConditionalResidualBlock2d(\n                    dim_in,\n                    dim_out,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                ),\n                Attention2d(dim_out, cfg.nhead) if cfg.has_attn and cfg.nhead is not None else nn.Identity(),\n                ConditionalResidualBlock2d(\n                    dim_out,\n                    dim_out,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                ),\n                Downsample2d(dim_out, cfg.use_shuffle) if not is_last else nn.Identity(),\n            ]),\n        )\n\n    up_modules = nn.ModuleList([])\n    for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n        is_last = ind &gt;= (len(in_out) - 1)\n        up_modules.append(\n            nn.ModuleList([\n                ConditionalResidualBlock2d(\n                    dim_out * 2,\n                    dim_in,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                ),\n                Attention2d(dim_in, cfg.nhead) if cfg.has_attn and cfg.nhead is not None else nn.Identity(),\n                ConditionalResidualBlock2d(\n                    dim_in,\n                    dim_in,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                ),\n                Upsample2d(dim_in, cfg.use_shuffle) if not is_last else nn.Identity(),\n            ]),\n        )\n\n    final_conv = nn.Sequential(\n        ConvNormActivation(start_dim, start_dim, cfg.conv_cfg),\n        nn.Conv2d(start_dim, obs_shape[0], 1),\n    )\n\n    # ModuleList \u306f\u578b\u60c5\u5831\u3092\u6301\u305f\u306a\u3044\u306e\u3067\u3001mypy \u306b\u5bfe\u3057\u3066\u306f\u3088\u308a\u5177\u4f53\u7684\u306a\n    # list[DownBlock] \u3068\u3057\u3066\u6271\u3046\u3088\u3046\u306b cast \u3059\u308b\n    self.up_modules = cast(\"list[DownBlock]\", up_modules)\n    self.down_modules = cast(\"list[DownBlock]\", down_modules)\n    self.final_conv = final_conv\n</code></pre>"},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet2d-attributes","title":"Attributes","text":""},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet2d.down_modules","title":"down_modules  <code>instance-attribute</code>","text":"<pre><code>down_modules = cast('list[DownBlock]', down_modules)\n</code></pre>"},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet2d.final_conv","title":"final_conv  <code>instance-attribute</code>","text":"<pre><code>final_conv = final_conv\n</code></pre>"},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet2d.mid_modules","title":"mid_modules  <code>instance-attribute</code>","text":"<pre><code>mid_modules = ModuleList([ConditionalResidualBlock2d(mid_dim, mid_dim, cond_dim=feature_dim, conv_cfg=conv_cfg, cond_predict_scale=cond_pred_scale), Attention2d(mid_dim, nhead) if has_attn and nhead is not None else Identity(), ConditionalResidualBlock2d(mid_dim, mid_dim, cond_dim=feature_dim, conv_cfg=conv_cfg, cond_predict_scale=cond_pred_scale)])\n</code></pre>"},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet2d.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet2d.up_modules","title":"up_modules  <code>instance-attribute</code>","text":"<pre><code>up_modules = cast('list[DownBlock]', up_modules)\n</code></pre>"},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet2d-functions","title":"Functions","text":""},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet2d.forward","title":"forward","text":"<pre><code>forward(base, cond)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>Tensor</code> <p>Input tensor of shape (B, T, input_dim).</p> required <code>cond</code> <code>Tensor</code> <p>Conditional tensor of shape (B, cond_dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (B, T, input_dim).</p> Source code in <code>src/ml_networks/torch/unet.py</code> <pre><code>def forward(\n    self,\n    base: torch.Tensor,\n    cond: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    base : torch.Tensor\n        Input tensor of shape (B, T, input_dim).\n    cond : torch.Tensor\n        Conditional tensor of shape (B, cond_dim).\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (B, T, input_dim).\n    \"\"\"\n    batch_shape = base.shape[:-3]\n    assert base.shape[-3:] == self.obs_shape, (\n        f\"Input shape {base.shape[-3:]} does not match expected shape {self.obs_shape}\"\n    )\n    base = base.reshape(-1, *self.obs_shape)\n\n    global_feature = cond.reshape(-1, cond.shape[-1])\n\n    x = base\n    h: list[torch.Tensor] = []\n    for modules in self.down_modules:\n        resnet, attn, resnet2, downsample = modules\n        x = resnet(x, global_feature)\n        x = attn(x)\n        x = resnet2(x, global_feature)\n        h.append(x)\n        x = downsample(x)\n\n    for mid_module in self.mid_modules:\n        x = mid_module(x, global_feature)\n\n    for modules in self.up_modules:\n        resnet, attn, resnet2, upsample = modules\n        x = torch.cat((x, h.pop()), dim=1)\n        x = resnet(x, global_feature)\n        x = attn(x)\n        x = resnet2(x, global_feature)\n        x = upsample(x)\n\n    x = self.final_conv(x)\n\n    return x.reshape(*batch_shape, *self.obs_shape)\n</code></pre>"},{"location":"api/unet/#conditionalunet1d","title":"ConditionalUnet1d","text":"<p>1D\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u7528\u306e\u6761\u4ef6\u4ed8\u304dUNet\u3002</p>"},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet1d","title":"ConditionalUnet1d","text":"<pre><code>ConditionalUnet1d(feature_dim, obs_shape, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>\u6761\u4ef6\u4ed8\u304d1D UNet\u30e2\u30c7\u30eb\u3002.</p> <p>Args:     feature_dim (int): \u6761\u4ef6\u4ed8\u304d\u7279\u5fb4\u91cf\u306e\u6b21\u5143\u6570     obs_shape (tuple[int, int]): \u89b3\u6e2c\u30c7\u30fc\u30bf\u306e\u5f62\u72b6 (\u30c1\u30e3\u30f3\u30cd\u30eb\u6570, \u9577\u3055)     cfg (UNetConfig): UNet\u306e\u8a2d\u5b9a</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ml_networks.config import UNetConfig, ConvConfig, MLPConfig, LinearConfig\n&gt;&gt;&gt; cfg = UNetConfig(\n...     channels=[64, 128, 256],\n...     conv_cfg=ConvConfig(\n...         kernel_size=3,\n...         padding=1,\n...         stride=1,\n...         groups=1,\n...         activation=\"ReLU\",\n...         dropout=0.0\n...     ),\n...     has_attn=True,\n...     nhead=8,\n...     cond_pred_scale=True\n... )\n&gt;&gt;&gt; net = ConditionalUnet1d(feature_dim=32, obs_shape=(3, 64), cfg=cfg)\n&gt;&gt;&gt; x = torch.randn(2, 3, 64)\n&gt;&gt;&gt; cond = torch.randn(2, 32)\n&gt;&gt;&gt; out = net(x, cond)\n&gt;&gt;&gt; out.shape\ntorch.Size([2, 3, 64])\n</code></pre> Source code in <code>src/ml_networks/torch/unet.py</code> <pre><code>def __init__(\n    self,\n    feature_dim: int,\n    obs_shape: tuple[int, int],\n    cfg: UNetConfig,\n) -&gt; None:\n    super().__init__()\n    all_dims = [obs_shape[0], *list(cfg.channels)]\n    start_dim = cfg.channels[0]\n    self.obs_shape = obs_shape\n\n    in_out = list(pairwise(all_dims))\n\n    mid_dim = all_dims[-1]\n    self.mid_modules = nn.ModuleList([\n        ConditionalResidualBlock1d(\n            mid_dim,\n            mid_dim,\n            cond_dim=feature_dim,\n            conv_cfg=cfg.conv_cfg,\n            cond_predict_scale=cfg.cond_pred_scale,\n        )\n        if not cfg.use_hypernet\n        else HyperConditionalResidualBlock1d(\n            mid_dim,\n            mid_dim,\n            cond_dim=feature_dim,\n            conv_cfg=cfg.conv_cfg,\n            hyper_mlp_cfg=cfg.hyper_mlp_cfg,\n        ),\n        Attention1d(mid_dim, cfg.nhead) if cfg.has_attn and cfg.nhead is not None else nn.Identity(),\n        ConditionalResidualBlock1d(\n            mid_dim,\n            mid_dim,\n            cond_dim=feature_dim,\n            conv_cfg=cfg.conv_cfg,\n            cond_predict_scale=cfg.cond_pred_scale,\n        )\n        if not cfg.use_hypernet\n        else HyperConditionalResidualBlock1d(\n            mid_dim,\n            mid_dim,\n            cond_dim=feature_dim,\n            conv_cfg=cfg.conv_cfg,\n            hyper_mlp_cfg=cfg.hyper_mlp_cfg,\n        ),\n    ])\n\n    down_modules = nn.ModuleList([])\n    for ind, (dim_in, dim_out) in enumerate(in_out):\n        is_last = ind &gt;= (len(in_out) - 1)\n        down_modules.append(\n            nn.ModuleList([\n                ConditionalResidualBlock1d(\n                    dim_in,\n                    dim_out,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                )\n                if not cfg.use_hypernet\n                else HyperConditionalResidualBlock1d(\n                    dim_in,\n                    dim_out,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    hyper_mlp_cfg=cfg.hyper_mlp_cfg,\n                ),\n                Attention1d(dim_out, cfg.nhead) if cfg.has_attn and cfg.nhead is not None else nn.Identity(),\n                ConditionalResidualBlock1d(\n                    dim_out,\n                    dim_out,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                )\n                if not cfg.use_hypernet\n                else HyperConditionalResidualBlock1d(\n                    dim_out,\n                    dim_out,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    hyper_mlp_cfg=cfg.hyper_mlp_cfg,\n                ),\n                Downsample1d(dim_out, cfg.use_shuffle) if not is_last else nn.Identity(),\n            ]),\n        )\n\n    up_modules = nn.ModuleList([])\n    for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n        is_last = ind &gt;= (len(in_out) - 1)\n        up_modules.append(\n            nn.ModuleList([\n                ConditionalResidualBlock1d(\n                    dim_out * 2,\n                    dim_in,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                )\n                if not cfg.use_hypernet\n                else HyperConditionalResidualBlock1d(\n                    dim_out * 2,\n                    dim_in,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    hyper_mlp_cfg=cfg.hyper_mlp_cfg,\n                ),\n                Attention1d(dim_in, cfg.nhead) if cfg.has_attn and cfg.nhead is not None else nn.Identity(),\n                ConditionalResidualBlock1d(\n                    dim_in,\n                    dim_in,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    cond_predict_scale=cfg.cond_pred_scale,\n                )\n                if not cfg.use_hypernet\n                else HyperConditionalResidualBlock1d(\n                    dim_in,\n                    dim_in,\n                    cond_dim=feature_dim,\n                    conv_cfg=cfg.conv_cfg,\n                    hyper_mlp_cfg=cfg.hyper_mlp_cfg,\n                ),\n                Upsample1d(dim_in, cfg.use_shuffle) if not is_last else nn.Identity(),\n            ]),\n        )\n    final_conv = nn.Sequential(\n        ConvNormActivation1d(start_dim, start_dim, cfg.conv_cfg),\n        nn.Conv1d(start_dim, obs_shape[0], 1),\n    )\n\n    # ModuleList \u306f\u578b\u60c5\u5831\u3092\u6301\u305f\u306a\u3044\u306e\u3067\u3001mypy \u306b\u5bfe\u3057\u3066\u306f\u3088\u308a\u5177\u4f53\u7684\u306a\n    # list[DownBlock] \u3068\u3057\u3066\u6271\u3046\u3088\u3046\u306b cast \u3059\u308b\n    self.up_modules = cast(\"list[DownBlock]\", up_modules)\n    self.down_modules = cast(\"list[DownBlock]\", down_modules)\n    self.final_conv = final_conv\n</code></pre>"},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet1d-attributes","title":"Attributes","text":""},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet1d.down_modules","title":"down_modules  <code>instance-attribute</code>","text":"<pre><code>down_modules = cast('list[DownBlock]', down_modules)\n</code></pre>"},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet1d.final_conv","title":"final_conv  <code>instance-attribute</code>","text":"<pre><code>final_conv = final_conv\n</code></pre>"},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet1d.mid_modules","title":"mid_modules  <code>instance-attribute</code>","text":"<pre><code>mid_modules = ModuleList([ConditionalResidualBlock1d(mid_dim, mid_dim, cond_dim=feature_dim, conv_cfg=conv_cfg, cond_predict_scale=cond_pred_scale) if not use_hypernet else HyperConditionalResidualBlock1d(mid_dim, mid_dim, cond_dim=feature_dim, conv_cfg=conv_cfg, hyper_mlp_cfg=hyper_mlp_cfg), Attention1d(mid_dim, nhead) if has_attn and nhead is not None else Identity(), ConditionalResidualBlock1d(mid_dim, mid_dim, cond_dim=feature_dim, conv_cfg=conv_cfg, cond_predict_scale=cond_pred_scale) if not use_hypernet else HyperConditionalResidualBlock1d(mid_dim, mid_dim, cond_dim=feature_dim, conv_cfg=conv_cfg, hyper_mlp_cfg=hyper_mlp_cfg)])\n</code></pre>"},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet1d.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet1d.up_modules","title":"up_modules  <code>instance-attribute</code>","text":"<pre><code>up_modules = cast('list[DownBlock]', up_modules)\n</code></pre>"},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet1d-functions","title":"Functions","text":""},{"location":"api/unet/#ml_networks.torch.unet.ConditionalUnet1d.forward","title":"forward","text":"<pre><code>forward(base, cond)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>Tensor</code> <p>Input tensor of shape (B, input_dim, T).</p> required <code>cond</code> <code>Tensor</code> <p>Conditional tensor of shape (B, cond_dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (B, T, input_dim).</p> Source code in <code>src/ml_networks/torch/unet.py</code> <pre><code>def forward(\n    self,\n    base: torch.Tensor,\n    cond: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    base : torch.Tensor\n        Input tensor of shape (B, input_dim, T).\n    cond : torch.Tensor\n        Conditional tensor of shape (B, cond_dim).\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (B, T, input_dim).\n    \"\"\"\n    batch_shape = base.shape[:-2]\n    assert base.shape[-2:] == self.obs_shape, (\n        f\"Input shape {base.shape[-2:]} does not match expected shape {self.obs_shape}\"\n    )\n    base = base.reshape(-1, *self.obs_shape)\n\n    global_feature = cond.reshape(-1, cond.shape[-1])\n\n    x = base\n    h: list[torch.Tensor] = []\n    for modules in self.down_modules:\n        resnet, attn, resnet2, downsample = modules\n        x = resnet(x, global_feature)\n        x = attn(x)\n        x = resnet2(x, global_feature)\n        h.append(x)\n        x = downsample(x)\n\n    for mid_module in self.mid_modules:\n        x = mid_module(x) if isinstance(mid_module, nn.Identity) else mid_module(x, global_feature)\n\n    for modules in self.up_modules:\n        resnet, attn, resnet2, upsample = modules\n        x = torch.cat((x, h.pop()), dim=1)\n        x = resnet(x, global_feature)\n        x = attn(x)\n        x = resnet2(x, global_feature)\n        x = upsample(x)\n\n    x = self.final_conv(x)\n\n    return x.reshape(*batch_shape, *self.obs_shape)\n</code></pre>"},{"location":"api/utils/","title":"\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3","text":"<p>\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\u95a2\u6570\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p>"},{"location":"api/utils/#ml_networksutils","title":"\u5171\u901a\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3 (<code>ml_networks.utils</code>)","text":""},{"location":"api/utils/#save_blosc2","title":"save_blosc2","text":""},{"location":"api/utils/#ml_networks.utils.save_blosc2","title":"save_blosc2","text":"<pre><code>save_blosc2(path, x)\n</code></pre> <p>Save numpy array with blosc2 compression.</p> Args: <p>path : str     Path to save. x : np.ndarray     Numpy array to save.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; save_blosc2(\"test.blosc2\", np.random.randn(10, 10))\n</code></pre> Source code in <code>src/ml_networks/utils.py</code> <pre><code>def save_blosc2(path: str, x: np.ndarray) -&gt; None:\n    \"\"\"Save numpy array with blosc2 compression.\n\n    Args:\n    -----\n    path : str\n        Path to save.\n    x : np.ndarray\n        Numpy array to save.\n\n    Examples\n    --------\n    &gt;&gt;&gt; save_blosc2(\"test.blosc2\", np.random.randn(10, 10))\n\n    \"\"\"\n    Path(path).write_bytes(blosc2.pack_array2(x))\n</code></pre>"},{"location":"api/utils/#load_blosc2","title":"load_blosc2","text":""},{"location":"api/utils/#ml_networks.utils.load_blosc2","title":"load_blosc2","text":"<pre><code>load_blosc2(path)\n</code></pre> <p>Load numpy array with blosc2 compression.</p> Args: <p>path : str     Path to load.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = load_blosc2(\"test.blosc2\")\n&gt;&gt;&gt; type(data)\n&lt;class 'numpy.ndarray'&gt;\n</code></pre> Source code in <code>src/ml_networks/utils.py</code> <pre><code>def load_blosc2(path: str) -&gt; np.ndarray:\n    \"\"\"Load numpy array with blosc2 compression.\n\n    Args:\n    -----\n    path : str\n        Path to load.\n\n    Returns\n    -------\n    np.ndarray\n        Numpy array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; data = load_blosc2(\"test.blosc2\")\n    &gt;&gt;&gt; type(data)\n    &lt;class 'numpy.ndarray'&gt;\n    \"\"\"\n    return blosc2.unpack_array2(Path(path).read_bytes())\n</code></pre>"},{"location":"api/utils/#determine_loader","title":"determine_loader","text":""},{"location":"api/utils/#ml_networks.utils.determine_loader","title":"determine_loader","text":"<pre><code>determine_loader(data, seed, batch_size, shuffle=True, collate_fn=None)\n</code></pre> <p>Determine DataLoader with fixed seed.</p> Args: <p>data : Dataset     Dataset to load. seed : int     Random seed. batch_size : int     Batch size. shuffle : bool     Whether to shuffle data. Default is True. collate_fn : callable     Collate function. Default is None.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>DataLoader with fixed seed.</p> Source code in <code>src/ml_networks/utils.py</code> <pre><code>def determine_loader(\n    data: Dataset,\n    seed: int,\n    batch_size: int,\n    shuffle: bool = True,\n    collate_fn: Callable | None = None,\n) -&gt; DataLoader:\n    \"\"\"\n    Determine DataLoader with fixed seed.\n\n    Args:\n    -----\n    data : Dataset\n        Dataset to load.\n    seed : int\n        Random seed.\n    batch_size : int\n        Batch size.\n    shuffle : bool\n        Whether to shuffle data. Default is True.\n    collate_fn : callable\n        Collate function. Default is None.\n\n    Returns\n    -------\n    DataLoader\n        DataLoader with fixed seed.\n    \"\"\"\n    g = torch.Generator()  # type: ignore[attr-defined]\n    g.manual_seed(seed)\n    return DataLoader(\n        data,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        worker_init_fn=seed_worker,\n        generator=g,\n        num_workers=2,\n        pin_memory=False,\n        collate_fn=collate_fn,\n    )\n</code></pre>"},{"location":"api/utils/#seed_worker","title":"seed_worker","text":""},{"location":"api/utils/#ml_networks.utils.seed_worker","title":"seed_worker","text":"<pre><code>seed_worker(_worker_id)\n</code></pre> <p>DataLoader\u306eworker\u306e\u56fa\u5b9a.</p> <p>Dataloader\u306e\u4e71\u6570\u56fa\u5b9a\u306b\u306fgenerator\u306e\u56fa\u5b9a\u3082\u5fc5\u8981\u3089\u3057\u3044</p> Source code in <code>src/ml_networks/utils.py</code> <pre><code>def seed_worker(_worker_id: int) -&gt; None:\n    \"\"\"\n    DataLoader\u306eworker\u306e\u56fa\u5b9a.\n\n    Dataloader\u306e\u4e71\u6570\u56fa\u5b9a\u306b\u306fgenerator\u306e\u56fa\u5b9a\u3082\u5fc5\u8981\u3089\u3057\u3044\n    \"\"\"\n    worker_seed = torch.initial_seed() % 2**32\n    pl.seed_everything(worker_seed)\n</code></pre>"},{"location":"api/utils/#conv_out","title":"conv_out","text":"<p>\u7573\u307f\u8fbc\u307f\u5c64\u306e\u51fa\u529b\u30b5\u30a4\u30ba\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p>"},{"location":"api/utils/#ml_networks.utils.conv_out","title":"conv_out","text":"<pre><code>conv_out(h_in, padding, kernel_size, stride, dilation=1)\n</code></pre> <p>Calculate the output size of convolutional layer.</p> Args: <p>h_in : int     Input size. padding : int     Padding size. kernel_size : int     Kernel size. stride : int     Stride size. dilation : int     Dilation size. Default is 1.</p> <p>Returns:</p> Type Description <code>int</code> <p>Output size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; conv_out(32, 1, 3, 1)\n32\n&gt;&gt;&gt; conv_out(32, 1, 3, 2)\n16\n&gt;&gt;&gt; conv_out(32, 1, 3, 1, 2)\n30\n</code></pre> Source code in <code>src/ml_networks/utils.py</code> <pre><code>def conv_out(h_in: int, padding: int, kernel_size: int, stride: int, dilation: int = 1) -&gt; int:\n    \"\"\"\n    Calculate the output size of convolutional layer.\n\n    Args:\n    -----\n    h_in : int\n        Input size.\n    padding : int\n        Padding size.\n    kernel_size : int\n        Kernel size.\n    stride : int\n        Stride size.\n    dilation : int\n        Dilation size. Default is 1.\n\n    Returns\n    -------\n    int\n        Output size.\n\n    Examples\n    --------\n    &gt;&gt;&gt; conv_out(32, 1, 3, 1)\n    32\n    &gt;&gt;&gt; conv_out(32, 1, 3, 2)\n    16\n    &gt;&gt;&gt; conv_out(32, 1, 3, 1, 2)\n    30\n    \"\"\"\n    return int((h_in + 2.0 * padding - dilation * (kernel_size - 1.0) - 1.0) / stride + 1.0)\n</code></pre>"},{"location":"api/utils/#conv_transpose_out","title":"conv_transpose_out","text":"<p>\u8ee2\u7f6e\u7573\u307f\u8fbc\u307f\u5c64\u306e\u51fa\u529b\u30b5\u30a4\u30ba\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p>"},{"location":"api/utils/#ml_networks.utils.conv_transpose_out","title":"conv_transpose_out","text":"<pre><code>conv_transpose_out(h_in, padding, kernel_size, stride, dilation=1, output_padding=0)\n</code></pre> <p>Calculate the output size of transposed convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>h_in</code> <code>int</code> <p>Input size.</p> required <code>padding</code> <code>int</code> <p>Padding size.</p> required <code>kernel_size</code> <code>int</code> <p>Kernel size.</p> required <code>stride</code> <code>int</code> <p>Stride size.</p> required <code>dilation</code> <code>int</code> <p>Dilation size. Default is 1.</p> <code>1</code> <code>output_padding</code> <code>int</code> <p>Output padding size. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>int</code> <p>Output size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; conv_transpose_out(32, 1, 3, 1)\n32\n&gt;&gt;&gt; conv_transpose_out(32, 1, 3, 2)\n63\n&gt;&gt;&gt; conv_transpose_out(32, 1, 3, 1, 2)\n34\n&gt;&gt;&gt; conv_transpose_out(32, 1, 3, 1, 1, 1)\n33\n</code></pre> Source code in <code>src/ml_networks/utils.py</code> <pre><code>def conv_transpose_out(\n    h_in: int,\n    padding: int,\n    kernel_size: int,\n    stride: int,\n    dilation: int = 1,\n    output_padding: int = 0,\n) -&gt; int:\n    \"\"\"\n    Calculate the output size of transposed convolutional layer.\n\n    Parameters\n    ----------\n    h_in : int\n        Input size.\n    padding : int\n        Padding size.\n    kernel_size : int\n        Kernel size.\n    stride : int\n        Stride size.\n    dilation : int\n        Dilation size. Default is 1.\n    output_padding : int\n        Output padding size. Default is 0.\n\n    Returns\n    -------\n    int\n        Output size.\n\n    Examples\n    --------\n    &gt;&gt;&gt; conv_transpose_out(32, 1, 3, 1)\n    32\n    &gt;&gt;&gt; conv_transpose_out(32, 1, 3, 2)\n    63\n    &gt;&gt;&gt; conv_transpose_out(32, 1, 3, 1, 2)\n    34\n    &gt;&gt;&gt; conv_transpose_out(32, 1, 3, 1, 1, 1)\n    33\n\n\n    \"\"\"\n    return (h_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n</code></pre>"},{"location":"api/utils/#output_padding","title":"output_padding","text":"<p>\u8ee2\u7f6e\u7573\u307f\u8fbc\u307f\u5c64\u306e\u51fa\u529b\u30d1\u30c7\u30a3\u30f3\u30b0\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p>"},{"location":"api/utils/#ml_networks.utils.output_padding","title":"output_padding","text":"<pre><code>output_padding(h_in, h_out, padding, kernel_size, stride, dilation=1)\n</code></pre> <p>Calculate the output padding size of transposed convolutional layer.</p> Args: <p>h_in : int     Input size. h_out : int     Output size. padding : int     Padding size. kernel_size : int     Kernel size. stride : int     Stride size. dilation : int     Dilation size. Default is 1.</p> <p>Returns:</p> Type Description <code>int</code> <p>Output padding size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; output_padding(32, 32, 1, 3, 1)\n0\n&gt;&gt;&gt; output_padding(32, 16, 1, 3, 2)\n1\n&gt;&gt;&gt; output_padding(32, 30, 1, 3, 1, 2)\n0\n</code></pre> Source code in <code>src/ml_networks/utils.py</code> <pre><code>def output_padding(h_in: int, h_out: int, padding: int, kernel_size: int, stride: int, dilation: int = 1) -&gt; int:\n    \"\"\"\n    Calculate the output padding size of transposed convolutional layer.\n\n    Args:\n    -----\n    h_in : int\n        Input size.\n    h_out : int\n        Output size.\n    padding : int\n        Padding size.\n    kernel_size : int\n        Kernel size.\n    stride : int\n        Stride size.\n    dilation : int\n        Dilation size. Default is 1.\n\n    Returns\n    -------\n    int\n        Output padding size.\n\n    Examples\n    --------\n    &gt;&gt;&gt; output_padding(32, 32, 1, 3, 1)\n    0\n    &gt;&gt;&gt; output_padding(32, 16, 1, 3, 2)\n    1\n    &gt;&gt;&gt; output_padding(32, 30, 1, 3, 1, 2)\n    0\n\n    \"\"\"\n    return h_in - (h_out - 1) * stride + 2 * padding - dilation * (kernel_size - 1) - 1\n</code></pre>"},{"location":"api/utils/#pytorch-ml_networkstorchtorch_utils","title":"PyTorch\u56fa\u6709\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3 (<code>ml_networks.torch.torch_utils</code>)","text":""},{"location":"api/utils/#get_optimizer","title":"get_optimizer","text":""},{"location":"api/utils/#ml_networks.torch.torch_utils.get_optimizer","title":"get_optimizer","text":"<pre><code>get_optimizer(param, name, **kwargs)\n</code></pre> <p>Get optimizer from torch.optim or pytorch_optimizer.</p> Args: <p>param : Iterator[nn.Parameter]     Parameters of models to optimize. name : str     Optimizer name. kwargs : dict     Optimizer arguments(settings).</p> <p>Returns:</p> Type Description <code>Optimizer</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_optimizer([nn.Parameter(torch.randn(1, 3))], \"Adam\", lr=0.01)\nAdam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.01\n    maximize: False\n    weight_decay: 0\n)\n</code></pre> Source code in <code>src/ml_networks/torch/torch_utils.py</code> <pre><code>def get_optimizer(\n    param: Iterator[nn.Parameter],\n    name: str,\n    **kwargs: float | str | bool,\n) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Get optimizer from torch.optim or pytorch_optimizer.\n\n    Args:\n    -----\n    param : Iterator[nn.Parameter]\n        Parameters of models to optimize.\n    name : str\n        Optimizer name.\n    kwargs : dict\n        Optimizer arguments(settings).\n\n    Returns\n    -------\n    torch.optim.Optimizer\n\n    Examples\n    --------\n    &gt;&gt;&gt; get_optimizer([nn.Parameter(torch.randn(1, 3))], \"Adam\", lr=0.01)\n    Adam (\n    Parameter Group 0\n        amsgrad: False\n        betas: (0.9, 0.999)\n        capturable: False\n        differentiable: False\n        eps: 1e-08\n        foreach: None\n        fused: None\n        lr: 0.01\n        maximize: False\n        weight_decay: 0\n    )\n    \"\"\"\n    if hasattr(schedulefree, name):\n        optimizer = getattr(schedulefree, name)\n    elif hasattr(torch.optim, name):\n        optimizer = getattr(torch.optim, name)\n    elif hasattr(pytorch_optimizer, name):\n        optimizer = getattr(pytorch_optimizer, name)\n    else:\n        msg = f\"Optimizer {name} is not implemented in torch.optim or pytorch_optimizer, schedulefree. \"\n        msg += \"Please check the name and capitalization.\"\n        raise NotImplementedError(msg)\n    return optimizer(param, **kwargs)\n</code></pre>"},{"location":"api/utils/#torch_fix_seed","title":"torch_fix_seed","text":""},{"location":"api/utils/#ml_networks.torch.torch_utils.torch_fix_seed","title":"torch_fix_seed","text":"<pre><code>torch_fix_seed(seed=42)\n</code></pre> <p>\u4e71\u6570\u3092\u56fa\u5b9a\u3059\u308b\u95a2\u6570.</p> References <ul> <li>https://qiita.com/north_redwing/items/1e153139125d37829d2d</li> </ul> Source code in <code>src/ml_networks/torch/torch_utils.py</code> <pre><code>def torch_fix_seed(seed: int = 42) -&gt; None:\n    \"\"\"\n    \u4e71\u6570\u3092\u56fa\u5b9a\u3059\u308b\u95a2\u6570.\n\n    References\n    ----------\n    - https://qiita.com/north_redwing/items/1e153139125d37829d2d\n    \"\"\"\n    random.seed(seed)\n    pl.seed_everything(seed, workers=True)\n    torch.set_float32_matmul_precision(\"medium\")\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n</code></pre>"},{"location":"api/utils/#gumbel_softmax","title":"gumbel_softmax","text":""},{"location":"api/utils/#ml_networks.torch.torch_utils.gumbel_softmax","title":"gumbel_softmax","text":"<pre><code>gumbel_softmax(inputs, dim, temperature=1.0)\n</code></pre> <p>Gumbel softmax function with temperature. This prevents overflow and underflow.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>Input tensor.</p> required <code>dim</code> <code>int</code> <p>Dimension to apply softmax.</p> required <code>temperature</code> <code>float</code> <p>Temperature. Default is 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Gumbel softmaxed tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the gumbel_softmax is inf or nan.</p> Source code in <code>src/ml_networks/torch/torch_utils.py</code> <pre><code>def gumbel_softmax(\n    inputs: torch.Tensor,\n    dim: int,\n    temperature: float = 1.0,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Gumbel softmax function with temperature. This prevents overflow and underflow.\n\n    Parameters\n    ----------\n    inputs : torch.Tensor\n        Input tensor.\n    dim : int\n        Dimension to apply softmax.\n    temperature : float\n        Temperature. Default is 1.0.\n\n    Returns\n    -------\n    torch.Tensor\n        Gumbel softmaxed tensor.\n\n    Raises\n    ------\n    ValueError\n        If the gumbel_softmax is inf or nan.\n    \"\"\"\n    x = inputs - torch.max(inputs.detach(), dim=-1, keepdim=True)[0]\n    x = F.gumbel_softmax(x, dim=dim, tau=temperature, hard=True)\n    if torch.isinf(x).any() or torch.isnan(x).any():\n        msg = \"gumbel_softmax is inf or nan\"\n        raise ValueError(msg)\n    return x\n</code></pre>"},{"location":"api/utils/#softmax","title":"softmax","text":""},{"location":"api/utils/#ml_networks.torch.torch_utils.softmax","title":"softmax","text":"<pre><code>softmax(inputs, dim, temperature=1.0)\n</code></pre> <p>Softmax function with temperature. This prevents overflow and underflow.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>Input tensor.</p> required <code>dim</code> <code>int</code> <p>Dimension to apply softmax.</p> required <code>temperature</code> <code>float</code> <p>Temperature. Default is 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Softmaxed tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the softmax is inf or nan.</p> Source code in <code>src/ml_networks/torch/torch_utils.py</code> <pre><code>def softmax(\n    inputs: torch.Tensor,\n    dim: int,\n    temperature: float = 1.0,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Softmax function with temperature. This prevents overflow and underflow.\n\n    Parameters\n    ----------\n    inputs : torch.Tensor\n        Input tensor.\n    dim : int\n        Dimension to apply softmax.\n    temperature : float\n        Temperature. Default is 1.0.\n\n    Returns\n    -------\n    torch.Tensor\n        Softmaxed tensor.\n\n    Raises\n    ------\n    ValueError\n        If the softmax is inf or nan.\n    \"\"\"\n    x = inputs / temperature\n    x = torch.exp(F.log_softmax(x, dim=dim))\n    if torch.isinf(x).any() or torch.isnan(x).any():\n        msg = \"softmax is inf or nan\"\n        raise ValueError(msg)\n    return x\n</code></pre>"},{"location":"api/utils/#minmaxnormalize","title":"MinMaxNormalize","text":""},{"location":"api/utils/#ml_networks.torch.torch_utils.MinMaxNormalize","title":"MinMaxNormalize","text":"<pre><code>MinMaxNormalize(min_val, max_val, old_min=0.0, old_max=1.0)\n</code></pre> <p>               Bases: <code>Normalize</code></p> <p>MinMax \u6b63\u898f\u5316\u5909\u63db.</p> <p>MinMaxNormalize \u306e\u521d\u671f\u5316.</p> Args: <p>min_val : float     \u6700\u5c0f\u5024. max_val : float     \u6700\u5927\u5024. old_min : float     \u5143\u306e\u6700\u5c0f\u5024. old_max : float     \u5143\u306e\u6700\u5927\u5024.</p> Source code in <code>src/ml_networks/torch/torch_utils.py</code> <pre><code>def __init__(self, min_val: float, max_val: float, old_min: float = 0.0, old_max: float = 1.0) -&gt; None:\n    \"\"\"\n    MinMaxNormalize \u306e\u521d\u671f\u5316.\n\n    Args:\n    -----\n    min_val : float\n        \u6700\u5c0f\u5024.\n    max_val : float\n        \u6700\u5927\u5024.\n    old_min : float\n        \u5143\u306e\u6700\u5c0f\u5024.\n    old_max : float\n        \u5143\u306e\u6700\u5927\u5024.\n\n\n    \"\"\"\n    scale = (max_val - min_val) / (old_max - old_min)\n    shift = min_val - old_min * scale  # new = scale\u00b7x + shift\n\n    #   Normalize does (x - mean)/std  =&gt;  x \u00b7 (1/std)  - mean/std\n    mean = -shift / scale  # invert the affine form\n    std = 1.0 / scale\n\n    super().__init__(mean=[mean] * 3, std=[std] * 3)\n    self.min = min_val\n    self.max = max_val\n</code></pre>"},{"location":"api/utils/#ml_networks.torch.torch_utils.MinMaxNormalize-attributes","title":"Attributes","text":""},{"location":"api/utils/#ml_networks.torch.torch_utils.MinMaxNormalize.max","title":"max  <code>instance-attribute</code>","text":"<pre><code>max = max_val\n</code></pre>"},{"location":"api/utils/#ml_networks.torch.torch_utils.MinMaxNormalize.min","title":"min  <code>instance-attribute</code>","text":"<pre><code>min = min_val\n</code></pre>"},{"location":"api/utils/#ml_networks.torch.torch_utils.MinMaxNormalize-functions","title":"Functions","text":""},{"location":"api/utils/#softmaxtransformation","title":"SoftmaxTransformation","text":""},{"location":"api/utils/#ml_networks.torch.torch_utils.SoftmaxTransformation","title":"SoftmaxTransformation","text":"<pre><code>SoftmaxTransformation(cfg)\n</code></pre> <p>Softmax \u5909\u63db\u30af\u30e9\u30b9.</p> <p>SoftmaxTransformation \u306e\u521d\u671f\u5316.</p> Args: <p>cfg : SoftmaxTransConfig     SoftmaxTransformation \u306e\u8a2d\u5b9a.</p> Source code in <code>src/ml_networks/torch/torch_utils.py</code> <pre><code>def __init__(\n    self,\n    cfg: SoftmaxTransConfig,\n) -&gt; None:\n    \"\"\"\n    SoftmaxTransformation \u306e\u521d\u671f\u5316.\n\n    Args:\n    -----\n    cfg : SoftmaxTransConfig\n        SoftmaxTransformation \u306e\u8a2d\u5b9a.\n    \"\"\"\n    super().__init__()\n    self.vector = cfg.vector\n    self.sigma = cfg.sigma\n    self.n_ignore = cfg.n_ignore\n    self.max = cfg.max\n    self.min = cfg.min\n    self.k = torch.linspace(self.min, self.max, self.vector)\n</code></pre>"},{"location":"api/utils/#ml_networks.torch.torch_utils.SoftmaxTransformation-attributes","title":"Attributes","text":""},{"location":"api/utils/#ml_networks.torch.torch_utils.SoftmaxTransformation.k","title":"k  <code>instance-attribute</code>","text":"<pre><code>k = linspace(min, max, vector)\n</code></pre>"},{"location":"api/utils/#ml_networks.torch.torch_utils.SoftmaxTransformation.max","title":"max  <code>instance-attribute</code>","text":"<pre><code>max = max\n</code></pre>"},{"location":"api/utils/#ml_networks.torch.torch_utils.SoftmaxTransformation.min","title":"min  <code>instance-attribute</code>","text":"<pre><code>min = min\n</code></pre>"},{"location":"api/utils/#ml_networks.torch.torch_utils.SoftmaxTransformation.n_ignore","title":"n_ignore  <code>instance-attribute</code>","text":"<pre><code>n_ignore = n_ignore\n</code></pre>"},{"location":"api/utils/#ml_networks.torch.torch_utils.SoftmaxTransformation.sigma","title":"sigma  <code>instance-attribute</code>","text":"<pre><code>sigma = sigma\n</code></pre>"},{"location":"api/utils/#ml_networks.torch.torch_utils.SoftmaxTransformation.vector","title":"vector  <code>instance-attribute</code>","text":"<pre><code>vector = vector\n</code></pre>"},{"location":"api/utils/#ml_networks.torch.torch_utils.SoftmaxTransformation-functions","title":"Functions","text":""},{"location":"api/utils/#ml_networks.torch.torch_utils.SoftmaxTransformation.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> Source code in <code>src/ml_networks/torch/torch_utils.py</code> <pre><code>def __call__(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return self.transform(x)\n</code></pre>"},{"location":"api/utils/#ml_networks.torch.torch_utils.SoftmaxTransformation.get_transformed_dim","title":"get_transformed_dim","text":"<pre><code>get_transformed_dim(dim)\n</code></pre> Source code in <code>src/ml_networks/torch/torch_utils.py</code> <pre><code>def get_transformed_dim(self, dim: int) -&gt; int:\n    return (dim - self.n_ignore) * self.vector + self.n_ignore\n</code></pre>"},{"location":"api/utils/#ml_networks.torch.torch_utils.SoftmaxTransformation.inverse","title":"inverse","text":"<pre><code>inverse(x)\n</code></pre> <p>SoftmaxTransformation \u306e\u9006\u5909\u63db.</p> Args: <p>x : torch.Tensor     \u5165\u529b\u30c6\u30f3\u30bd\u30eb.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>\u51fa\u529b\u30c6\u30f3\u30bd\u30eb.</p> Source code in <code>src/ml_networks/torch/torch_utils.py</code> <pre><code>def inverse(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    SoftmaxTransformation \u306e\u9006\u5909\u63db.\n\n    Args:\n    -----\n    x : torch.Tensor\n        \u5165\u529b\u30c6\u30f3\u30bd\u30eb.\n\n    Returns\n    -------\n    torch.Tensor\n        \u51fa\u529b\u30c6\u30f3\u30bd\u30eb.\n    \"\"\"\n    *batch, dim = x.shape\n    x = x.reshape(-1, dim)\n    if self.n_ignore:\n        data, ignored = x[:, : -self.n_ignore], x[:, -self.n_ignore :]\n    else:\n        data = x\n\n    data = data.reshape([len(data), -1, self.vector])\n\n    data = rearrange(data, \"b d v -&gt; v b d\")\n\n    data = torch.stack([data[v] * self.k[v] for v in range(self.vector)]).sum(dim=0)\n\n    data = torch.cat([data, ignored], dim=-1) if self.n_ignore else data\n    return data.reshape(*batch, -1)\n</code></pre>"},{"location":"api/utils/#ml_networks.torch.torch_utils.SoftmaxTransformation.transform","title":"transform","text":"<pre><code>transform(x)\n</code></pre> <p>SoftmaxTransformation \u306e\u5b9f\u884c.</p> Args: <p>x : torch.Tensor     \u5165\u529b\u30c6\u30f3\u30bd\u30eb.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>\u51fa\u529b\u30c6\u30f3\u30bd\u30eb.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; trans = SoftmaxTransformation(SoftmaxTransConfig(vector=16, sigma=0.01, n_ignore=1, min=-1.0, max=1.0))\n&gt;&gt;&gt; x = torch.randn(2, 3, 4)\n&gt;&gt;&gt; transformed = trans(x)\n&gt;&gt;&gt; transformed.shape\ntorch.Size([2, 3, 49])\n</code></pre> <pre><code>&gt;&gt;&gt; trans = SoftmaxTransformation(SoftmaxTransConfig(vector=11, sigma=0.05, n_ignore=0, min=-1.0, max=1.0))\n&gt;&gt;&gt; x = torch.randn(2, 3, 4)\n&gt;&gt;&gt; transformed = trans(x)\n&gt;&gt;&gt; transformed.shape\ntorch.Size([2, 3, 44])\n</code></pre> Source code in <code>src/ml_networks/torch/torch_utils.py</code> <pre><code>def transform(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    SoftmaxTransformation \u306e\u5b9f\u884c.\n\n    Args:\n    -----\n    x : torch.Tensor\n        \u5165\u529b\u30c6\u30f3\u30bd\u30eb.\n\n    Returns\n    -------\n    torch.Tensor\n        \u51fa\u529b\u30c6\u30f3\u30bd\u30eb.\n\n    Examples\n    --------\n    &gt;&gt;&gt; trans = SoftmaxTransformation(SoftmaxTransConfig(vector=16, sigma=0.01, n_ignore=1, min=-1.0, max=1.0))\n    &gt;&gt;&gt; x = torch.randn(2, 3, 4)\n    &gt;&gt;&gt; transformed = trans(x)\n    &gt;&gt;&gt; transformed.shape\n    torch.Size([2, 3, 49])\n\n    &gt;&gt;&gt; trans = SoftmaxTransformation(SoftmaxTransConfig(vector=11, sigma=0.05, n_ignore=0, min=-1.0, max=1.0))\n    &gt;&gt;&gt; x = torch.randn(2, 3, 4)\n    &gt;&gt;&gt; transformed = trans(x)\n    &gt;&gt;&gt; transformed.shape\n    torch.Size([2, 3, 44])\n\n    \"\"\"\n    *batch, dim = x.shape\n    x = x.reshape(-1, dim)\n    if self.n_ignore:\n        data, ignored = x[:, : -self.n_ignore], x[:, -self.n_ignore :]\n    else:\n        data = x\n\n    negative = torch.stack([torch.exp((-((data - self.k[v]) ** 2)) / self.sigma) for v in range(self.vector)])\n    negative_sum = negative.sum(dim=0)\n\n    transformed = negative / (negative_sum + 1e-8)\n    transformed = rearrange(transformed, \"v b d -&gt; b (d v)\")\n\n    transformed = torch.cat([transformed, ignored], dim=-1) if self.n_ignore else transformed\n    return transformed.reshape(*batch, self.get_transformed_dim(dim))\n</code></pre>"},{"location":"api/vision/","title":"\u30d3\u30b8\u30e7\u30f3","text":"<p>\u30d3\u30b8\u30e7\u30f3\u95a2\u9023\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\uff08Encoder\u3001Decoder\u306a\u3069\uff09\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p> <p><code>ml_networks.torch.vision</code>\uff08PyTorch\uff09\u3068<code>ml_networks.jax.vision</code>\uff08JAX\uff09\u306e\u4e21\u65b9\u3067\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"api/vision/#encoder","title":"Encoder","text":""},{"location":"api/vision/#ml_networks.torch.vision.Encoder","title":"Encoder","text":"<pre><code>Encoder(feature_dim, obs_shape, backbone_cfg, fc_cfg=None)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Encoder with various architectures.</p> <p>Parameters:</p> Name Type Description Default <code>feature_dim</code> <code>int | tuple[int, int, int]</code> <p>Dimension of the feature tensor. If int, Encoder includes full connection layer to downsample the feature tensor. Otherwise, Encoder does not include full connection layer and directly process with backbone network.</p> required <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>shape of the input tensor</p> required <code>backbone_cfg</code> <code>ViTConfig | ConvNetConfig | ResNetConfig</code> <p>configuration of the network</p> required <code>fc_cfg</code> <code>MLPConfig | LinearConfig | SpatialSoftmaxConfig | None</code> <p>configuration of the full connection layer. If feature_dim is tuple, fc_cfg is ignored. If feature_dim is int, fc_cfg must be provided. Default is None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; feature_dim = 128\n&gt;&gt;&gt; obs_shape = (3, 64, 64)\n&gt;&gt;&gt; cfg = ConvNetConfig(\n...     channels=[16, 32, 64],\n...     conv_cfgs=[\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...     ]\n... )\n&gt;&gt;&gt; fc_cfg = LinearConfig(\n...     activation=\"ReLU\",\n...     bias=True\n... )\n&gt;&gt;&gt; encoder = Encoder(feature_dim, obs_shape, cfg, fc_cfg)\n&gt;&gt;&gt; x = torch.randn(2, *obs_shape)\n&gt;&gt;&gt; y = encoder(x)\n&gt;&gt;&gt; y.shape\ntorch.Size([2, 128])\n</code></pre> <pre><code>&gt;&gt;&gt; encoder\nEncoder(\n  (encoder): ConvNet(\n    (conv): Sequential(\n      (0): ConvNormActivation(\n        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (pixel_shuffle): Identity()\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n      (1): ConvNormActivation(\n        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (pixel_shuffle): Identity()\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n      (2): ConvNormActivation(\n        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (pixel_shuffle): Identity()\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n    )\n  )\n  (fc): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): LinearNormActivation(\n      (linear): Linear(in_features=4096, out_features=128, bias=True)\n      (norm): Identity()\n      (activation): Activation(\n        (activation): ReLU()\n      )\n      (dropout): Identity()\n    )\n  )\n)\n</code></pre> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def __init__(\n    self,\n    feature_dim: int | tuple[int, int, int],\n    obs_shape: tuple[int, int, int],\n    backbone_cfg: ViTConfig | ConvNetConfig | ResNetConfig,\n    fc_cfg: MLPConfig | LinearConfig | SpatialSoftmaxConfig | None = None,\n) -&gt; None:\n    super().__init__()\n\n    self.obs_shape = obs_shape\n\n    self.encoder: nn.Module\n    if isinstance(backbone_cfg, ViTConfig):\n        self.encoder = ViT(obs_shape, backbone_cfg)\n    elif isinstance(backbone_cfg, ConvNetConfig):\n        self.encoder = ConvNet(obs_shape, backbone_cfg)\n    elif isinstance(backbone_cfg, ResNetConfig):\n        self.encoder = ResNetPixUnshuffle(obs_shape, backbone_cfg)\n    else:\n        msg = f\"{type(backbone_cfg)} is not implemented\"\n        raise NotImplementedError(msg)\n\n    self.feature_dim = feature_dim\n    # \u578b\u60c5\u5831\u3092\u88dc\u3046\u305f\u3081\u306b\u660e\u793a\u7684\u306b\u30ad\u30e3\u30b9\u30c8\n    self.conved_size = cast(\"int\", self.encoder.conved_size)\n    self.conved_shape = cast(\"tuple[int, int]\", self.encoder.conved_shape)\n    self.last_channel = cast(\"int\", self.encoder.last_channel)\n\n    if isinstance(feature_dim, int):\n        assert fc_cfg is not None, \"fc_cfg must be provided if feature_dim is provided\"\n    else:\n        assert feature_dim == (self.last_channel, *self.conved_shape), (\n            f\"{feature_dim} != {(self.last_channel, *self.conved_shape)}\"\n        )\n    self.fc: nn.Module\n    if isinstance(fc_cfg, MLPConfig):\n        assert isinstance(feature_dim, int), \"feature_dim must be int when using MLPConfig\"\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            MLPLayer(self.conved_size, feature_dim, fc_cfg),\n        )\n    elif isinstance(fc_cfg, LinearConfig):\n        assert isinstance(feature_dim, int), \"feature_dim must be int when using LinearConfig\"\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            LinearNormActivation(self.conved_size, feature_dim, fc_cfg),\n        )\n    elif isinstance(fc_cfg, AdaptiveAveragePoolingConfig):\n        assert isinstance(feature_dim, int), \"feature_dim must be int when using AdaptiveAveragePoolingConfig\"\n        self.fc = nn.Sequential(\n            nn.AdaptiveAvgPool2d(fc_cfg.output_size),\n            nn.Flatten(),\n            LinearNormActivation(\n                int(self.last_channel * np.prod(fc_cfg.output_size)),\n                feature_dim,\n                fc_cfg.additional_layer,\n            )\n            if isinstance(\n                fc_cfg.additional_layer,\n                LinearConfig,\n            )\n            else MLPLayer(\n                int(self.last_channel * np.prod(fc_cfg.output_size)),\n                feature_dim,\n                fc_cfg.additional_layer,\n            )\n            if isinstance(\n                fc_cfg.additional_layer,\n                MLPConfig,\n            )\n            else nn.Identity(),\n        )\n        if fc_cfg.additional_layer is None:\n            self.feature_dim = (\n                self.last_channel * (fc_cfg.output_size**2)\n                if isinstance(\n                    fc_cfg.output_size,\n                    int,\n                )\n                else self.last_channel * np.prod(fc_cfg.output_size)\n            )\n\n    elif isinstance(fc_cfg, SpatialSoftmaxConfig):\n        assert isinstance(self.feature_dim, int), \"feature_dim must be int when using SpatialSoftmaxConfig\"\n        self.fc = nn.Sequential(\n            SpatialSoftmax(fc_cfg),\n            nn.Flatten(),\n            LinearNormActivation(\n                self.last_channel * 2,\n                self.feature_dim,\n                fc_cfg.additional_layer,\n            )\n            if isinstance(\n                fc_cfg.additional_layer,\n                LinearConfig,\n            )\n            else MLPLayer(\n                self.last_channel * 2,\n                self.feature_dim,\n                fc_cfg.additional_layer,\n            )\n            if isinstance(\n                fc_cfg.additional_layer,\n                MLPConfig,\n            )\n            else nn.Identity(),\n        )\n        if fc_cfg.additional_layer is None:\n            self.feature_dim = self.last_channel * 2\n    else:\n        self.fc = nn.Identity()\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.Encoder-attributes","title":"Attributes","text":""},{"location":"api/vision/#ml_networks.torch.vision.Encoder.conved_shape","title":"conved_shape  <code>instance-attribute</code>","text":"<pre><code>conved_shape = cast('tuple[int, int]', conved_shape)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.Encoder.conved_size","title":"conved_size  <code>instance-attribute</code>","text":"<pre><code>conved_size = cast('int', conved_size)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.Encoder.encoder","title":"encoder  <code>instance-attribute</code>","text":"<pre><code>encoder\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.Encoder.fc","title":"fc  <code>instance-attribute</code>","text":"<pre><code>fc\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.Encoder.feature_dim","title":"feature_dim  <code>instance-attribute</code>","text":"<pre><code>feature_dim = feature_dim\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.Encoder.last_channel","title":"last_channel  <code>instance-attribute</code>","text":"<pre><code>last_channel = cast('int', last_channel)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.Encoder.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.Encoder-functions","title":"Functions","text":""},{"location":"api/vision/#ml_networks.torch.vision.Encoder.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor of shape (batch_size, *obs_shape)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor of shape (batch_size, *feature_dim)</p> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        input tensor of shape (batch_size, *obs_shape)\n\n    Returns\n    -------\n    torch.Tensor\n        output tensor of shape (batch_size, *feature_dim)\n    \"\"\"\n    batch_shape = x.shape[:-3]\n\n    x = x.reshape([-1, *self.obs_shape])\n    x = self.encoder(x)\n    x = x.view(-1, self.last_channel, *self.conved_shape)\n    x = self.fc(x)\n    return x.reshape([*batch_shape, *x.shape[1:]])\n</code></pre>"},{"location":"api/vision/#decoder","title":"Decoder","text":""},{"location":"api/vision/#ml_networks.torch.vision.Decoder","title":"Decoder","text":"<pre><code>Decoder(feature_dim, obs_shape, backbone_cfg, fc_cfg=None)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Decoder with various architectures.</p> <p>Parameters:</p> Name Type Description Default <code>feature_dim</code> <code>int | tuple[int, int, int]</code> <p>dimension of the feature tensor, if int, Decoder includes full connection layer to upsample the feature tensor. Otherwise, Decoder does not include full connection layer and directly process with backbone network.</p> required <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>shape of the output tensor</p> required <code>backbone_cfg</code> <code>ConvNetConfig | ViTConfig | ResNetConfig</code> <p>configuration of the network</p> required <code>fc_cfg</code> <code>MLPConfig | LinearConfig | None</code> <p>configuration of the full connection layer. If feature_dim is tuple, fc_cfg is ignored. If feature_dim is int, fc_cfg must be provided. Default is None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; feature_dim = 128\n&gt;&gt;&gt; obs_shape = (3, 64, 64)\n&gt;&gt;&gt; cfg = ConvNetConfig(\n...     channels=[64, 32, 16],\n...     conv_cfgs=[\n...         ConvConfig(kernel_size=4, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=4, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=4, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...     ]\n... )\n&gt;&gt;&gt; fc_cfg = MLPConfig(\n...     hidden_dim=256,\n...     n_layers=2,\n...     output_activation= \"ReLU\",\n...     linear_cfg= LinearConfig(\n...         activation= \"ReLU\",\n...         bias= True\n...     )\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; decoder = Decoder(feature_dim, obs_shape, cfg, fc_cfg)\n&gt;&gt;&gt; x = torch.randn(2, feature_dim)\n&gt;&gt;&gt; y = decoder(x)\n&gt;&gt;&gt; y.shape\ntorch.Size([2, 3, 64, 64])\n</code></pre> <pre><code>&gt;&gt;&gt; decoder\nDecoder(\n  (fc): MLPLayer(\n    (dense): Sequential(\n      (0): LinearNormActivation(\n        (linear): Linear(in_features=128, out_features=256, bias=True)\n        (norm): Identity()\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n      (1): LinearNormActivation(\n        (linear): Linear(in_features=256, out_features=256, bias=True)\n        (norm): Identity()\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n      (2): LinearNormActivation(\n        (linear): Linear(in_features=256, out_features=1024, bias=True)\n        (norm): Identity()\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n    )\n  )\n  (decoder): ConvTranspose(\n    (first_conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n    (conv): Sequential(\n      (0): ConvTransposeNormActivation(\n        (conv): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n        (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n      (1): ConvTransposeNormActivation(\n        (conv): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n        (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n      (2): ConvTransposeNormActivation(\n        (conv): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n        (norm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (activation): Activation(\n          (activation): ReLU()\n        )\n        (dropout): Identity()\n      )\n    )\n  )\n)\n</code></pre> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def __init__(\n    self,\n    feature_dim: int | tuple[int, int, int],\n    obs_shape: tuple[int, int, int],\n    backbone_cfg: ConvNetConfig | ViTConfig | ResNetConfig,\n    fc_cfg: MLPConfig | LinearConfig | None = None,\n) -&gt; None:\n    super().__init__()\n\n    self.obs_shape = obs_shape\n    self.feature_dim = feature_dim\n\n    self.input_shape: tuple[int, int, int]\n    if isinstance(backbone_cfg, ViTConfig):\n        self.input_shape = ViT.get_input_shape(obs_shape, backbone_cfg)\n    elif isinstance(backbone_cfg, ConvNetConfig):\n        self.input_shape = cast(\n            \"tuple[int, int, int]\",\n            ConvTranspose.get_input_shape(obs_shape, backbone_cfg),\n        )\n    elif isinstance(backbone_cfg, ResNetConfig):\n        self.input_shape = cast(\n            \"tuple[int, int, int]\",\n            ResNetPixShuffle.get_input_shape(obs_shape, backbone_cfg),\n        )\n    else:\n        msg = f\"{type(backbone_cfg)} is not implemented\"\n        raise NotImplementedError(msg)\n    if isinstance(feature_dim, int):\n        assert fc_cfg is not None, \"fc_cfg must be provided if feature_dim is provided\"\n        self.has_fc = True\n    else:\n        assert feature_dim == self.input_shape, f\"{feature_dim} != {self.input_shape}\"\n        self.has_fc = False\n\n    if isinstance(fc_cfg, MLPConfig):\n        assert isinstance(feature_dim, int), \"feature_dim must be int when using MLPConfig\"\n        self.fc: nn.Module = MLPLayer(feature_dim, int(np.prod(self.input_shape)), fc_cfg)\n    elif isinstance(fc_cfg, LinearConfig):\n        assert isinstance(feature_dim, int), \"feature_dim must be int when using LinearConfig\"\n        self.fc = LinearNormActivation(feature_dim, int(np.prod(self.input_shape)), fc_cfg)\n    else:\n        self.fc = nn.Identity()\n\n    if isinstance(backbone_cfg, ViTConfig):\n        self.decoder: nn.Module = ViT(in_shape=self.input_shape, obs_shape=obs_shape, cfg=backbone_cfg)\n    elif isinstance(backbone_cfg, ConvNetConfig):\n        self.decoder = ConvTranspose(in_shape=self.input_shape, obs_shape=obs_shape, cfg=backbone_cfg)\n    elif isinstance(backbone_cfg, ResNetConfig):\n        self.decoder = ResNetPixShuffle(in_shape=self.input_shape, obs_shape=obs_shape, cfg=backbone_cfg)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.Decoder-attributes","title":"Attributes","text":""},{"location":"api/vision/#ml_networks.torch.vision.Decoder.decoder","title":"decoder  <code>instance-attribute</code>","text":"<pre><code>decoder = ViT(in_shape=input_shape, obs_shape=obs_shape, cfg=backbone_cfg)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.Decoder.fc","title":"fc  <code>instance-attribute</code>","text":"<pre><code>fc = MLPLayer(feature_dim, int(prod(input_shape)), fc_cfg)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.Decoder.feature_dim","title":"feature_dim  <code>instance-attribute</code>","text":"<pre><code>feature_dim = feature_dim\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.Decoder.has_fc","title":"has_fc  <code>instance-attribute</code>","text":"<pre><code>has_fc = True\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.Decoder.input_shape","title":"input_shape  <code>instance-attribute</code>","text":"<pre><code>input_shape\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.Decoder.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.Decoder-functions","title":"Functions","text":""},{"location":"api/vision/#ml_networks.torch.vision.Decoder.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor of shape (batch_size, *feature_dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor of shape (batch_size, *obs_shape)</p> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        input tensor of shape (batch_size, *feature_dim)\n\n    Returns\n    -------\n    torch.Tensor\n        output tensor of shape (batch_size, *obs_shape)\n\n    \"\"\"\n    if self.has_fc:\n        batch_shape, data_shape = x.shape[:-1], x.shape[-1:]\n    else:\n        batch_shape, data_shape = x.shape[:-3], x.shape[-3:]\n    x = x.reshape([-1, *data_shape])\n    x = self.fc(x)\n    x = x.reshape([-1, *self.input_shape])\n    x = self.decoder(x)\n\n    return x.reshape([*batch_shape, *self.obs_shape])\n</code></pre>"},{"location":"api/vision/#convnet","title":"ConvNet","text":""},{"location":"api/vision/#ml_networks.torch.vision.ConvNet","title":"ConvNet","text":"<pre><code>ConvNet(obs_shape, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Convolutional Neural Network for Encoder.</p> <p>Parameters:</p> Name Type Description Default <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>shape of input tensor</p> required <code>cfg</code> <code>ConvNetConfig</code> <p>configuration of the network</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; obs_shape = (3, 64, 64)\n&gt;&gt;&gt; cfg = ConvNetConfig(\n...     channels=[16, 32, 64],\n...     conv_cfgs=[\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...     ]\n... )\n&gt;&gt;&gt; encoder = ConvNet(obs_shape, cfg)\n&gt;&gt;&gt; encoder\nConvNet(\n  (conv): Sequential(\n    (0): ConvNormActivation(\n      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (pixel_shuffle): Identity()\n      (activation): Activation(\n        (activation): ReLU()\n      )\n      (dropout): Identity()\n    )\n    (1): ConvNormActivation(\n      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (pixel_shuffle): Identity()\n      (activation): Activation(\n        (activation): ReLU()\n      )\n      (dropout): Identity()\n    )\n    (2): ConvNormActivation(\n      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (pixel_shuffle): Identity()\n      (activation): Activation(\n        (activation): ReLU()\n      )\n      (dropout): Identity()\n    )\n  )\n)\n&gt;&gt;&gt; x = torch.randn(2, *obs_shape)\n&gt;&gt;&gt; y = encoder(x)\n&gt;&gt;&gt; y.shape\ntorch.Size([2, 64, 8, 8])\n</code></pre> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def __init__(\n    self,\n    obs_shape: tuple[int, int, int],\n    cfg: ConvNetConfig,\n) -&gt; None:\n    super().__init__()\n\n    self.obs_shape = obs_shape\n    self.channels = [obs_shape[0], *cfg.channels]\n    self.cfg = cfg\n\n    self.conv = self._build_conv()\n\n    self.last_channel = self.channels[-1]\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvNet-attributes","title":"Attributes","text":""},{"location":"api/vision/#ml_networks.torch.vision.ConvNet.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvNet.channels","title":"channels  <code>instance-attribute</code>","text":"<pre><code>channels = [obs_shape[0], *(channels)]\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvNet.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = _build_conv()\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvNet.conved_shape","title":"conved_shape  <code>property</code>","text":"<pre><code>conved_shape\n</code></pre> <p>Get the shape of the output tensor after convolutional layers.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>shape of the output tensor</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; obs_shape = (3, 64, 64)\n&gt;&gt;&gt; cfg = ConvNetConfig(\n...     channels=[64, 32, 16],\n...     conv_cfgs=[\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...     ]\n... )\n&gt;&gt;&gt; encoder = ConvNet(obs_shape, cfg)\n&gt;&gt;&gt; encoder.conved_shape\n(8, 8)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvNet.conved_size","title":"conved_size  <code>property</code>","text":"<pre><code>conved_size\n</code></pre> <p>Get the size of the output tensor after convolutional layers.</p> <p>Returns:</p> Type Description <code>int</code> <p>size of the output tensor</p>"},{"location":"api/vision/#ml_networks.torch.vision.ConvNet.last_channel","title":"last_channel  <code>instance-attribute</code>","text":"<pre><code>last_channel = channels[-1]\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvNet.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvNet-functions","title":"Functions","text":""},{"location":"api/vision/#ml_networks.torch.vision.ConvNet.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor of shape (batch_size, *obs_shape)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor of shape (batch_size, self.last_channel, *self.conved_shape)</p> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        input tensor of shape (batch_size, *obs_shape)\n\n    Returns\n    -------\n    torch.Tensor\n        output tensor of shape (batch_size, self.last_channel, *self.conved_shape)\n\n    \"\"\"\n    return self.conv(x)\n</code></pre>"},{"location":"api/vision/#convtranspose","title":"ConvTranspose","text":""},{"location":"api/vision/#ml_networks.torch.vision.ConvTranspose","title":"ConvTranspose","text":"<pre><code>ConvTranspose(in_shape, obs_shape, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Convolutional Transpose Network for Decoder.</p> <p>Parameters:</p> Name Type Description Default <code>in_shape</code> <code>tuple[int, int, int]</code> <p>shape of input tensor</p> required <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>shape of output tensor</p> required <code>cfg</code> <code>ConvNetConfig</code> <p>configuration of the network</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; in_shape = (128, 8, 8)\n&gt;&gt;&gt; obs_shape = (3, 64, 64)\n&gt;&gt;&gt; cfg = ConvNetConfig(\n...     channels=[64, 32, 16],\n...     conv_cfgs=[\n...         ConvConfig(kernel_size=4, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=4, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=4, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...     ]\n... )\n&gt;&gt;&gt; decoder = ConvTranspose(in_shape, obs_shape, cfg)\n&gt;&gt;&gt; decoder\nConvTranspose(\n  (first_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n  (conv): Sequential(\n    (0): ConvTransposeNormActivation(\n      (conv): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (activation): Activation(\n        (activation): ReLU()\n      )\n      (dropout): Identity()\n    )\n    (1): ConvTransposeNormActivation(\n      (conv): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n      (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (activation): Activation(\n        (activation): ReLU()\n      )\n      (dropout): Identity()\n    )\n    (2): ConvTransposeNormActivation(\n      (conv): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n      (norm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (activation): Activation(\n        (activation): ReLU()\n      )\n      (dropout): Identity()\n    )\n  )\n)\n&gt;&gt;&gt; x = torch.randn(2, *in_shape)\n&gt;&gt;&gt; y = decoder(x)\n&gt;&gt;&gt; y.shape\ntorch.Size([2, 3, 64, 64])\n</code></pre> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def __init__(\n    self,\n    in_shape: tuple[int, int, int],\n    obs_shape: tuple[int, int, int],\n    cfg: ConvNetConfig,\n) -&gt; None:\n    super().__init__()\n    self.in_shape = in_shape\n    self.obs_shape = obs_shape\n    self.conv_out_shapes = []\n    self.cfg = cfg\n    self.channels = [*cfg.channels, obs_shape[0]]\n    assert len(cfg.channels) == len(cfg.conv_cfgs)\n    if self.in_shape[0] != cfg.channels[0]:\n        self.first_conv = nn.Conv2d(in_shape[0], cfg.channels[0], kernel_size=1, stride=1, padding=0)\n        self.init_channel = cfg.channels[0]\n        self.have_first_conv = True\n    else:\n        self.init_channel = in_shape[0]\n        self.have_first_conv = False\n\n    prev_shape: tuple[int, int] = tuple(in_shape[1:])  # type: ignore[assignment]\n    for conv_cfg in cfg.conv_cfgs:\n        padding, kernel, stride, dilation = (\n            conv_cfg.padding,\n            conv_cfg.kernel_size,\n            conv_cfg.stride,\n            conv_cfg.dilation,\n        )\n        prev_shape = tuple(conv_transpose_out_shape(prev_shape, padding, kernel, stride, dilation))  # type: ignore[assignment]\n        self.conv_out_shapes += [prev_shape]\n    assert self.conv_out_shapes[-1] == obs_shape[1:], f\"{self.conv_out_shapes[-1]} != {obs_shape[1:]}\"\n\n    self.conv = self._build_conv()\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvTranspose-attributes","title":"Attributes","text":""},{"location":"api/vision/#ml_networks.torch.vision.ConvTranspose.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvTranspose.channels","title":"channels  <code>instance-attribute</code>","text":"<pre><code>channels = [*(channels), obs_shape[0]]\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvTranspose.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = _build_conv()\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvTranspose.conv_out_shapes","title":"conv_out_shapes  <code>instance-attribute</code>","text":"<pre><code>conv_out_shapes = []\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvTranspose.first_conv","title":"first_conv  <code>instance-attribute</code>","text":"<pre><code>first_conv = Conv2d(in_shape[0], channels[0], kernel_size=1, stride=1, padding=0)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvTranspose.have_first_conv","title":"have_first_conv  <code>instance-attribute</code>","text":"<pre><code>have_first_conv = True\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvTranspose.in_shape","title":"in_shape  <code>instance-attribute</code>","text":"<pre><code>in_shape = in_shape\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvTranspose.init_channel","title":"init_channel  <code>instance-attribute</code>","text":"<pre><code>init_channel = channels[0]\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvTranspose.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvTranspose-functions","title":"Functions","text":""},{"location":"api/vision/#ml_networks.torch.vision.ConvTranspose.forward","title":"forward","text":"<pre><code>forward(z)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>Tensor</code> <p>input tensor of shape (batch_size, *in_shape)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor of shape (batch_size, *obs_shape)</p> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def forward(self, z: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    z: torch.Tensor\n        input tensor of shape (batch_size, *in_shape)\n\n    Returns\n    -------\n    torch.Tensor\n        output tensor of shape (batch_size, *obs_shape)\n\n\n    \"\"\"\n    if self.have_first_conv:\n        z = self.first_conv(z)\n    return self.conv(z)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ConvTranspose.get_input_shape","title":"get_input_shape  <code>staticmethod</code>","text":"<pre><code>get_input_shape(obs_shape, cfg)\n</code></pre> <p>Get input shape of the decoder.</p> <p>Parameters:</p> Name Type Description Default <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>shape of the output tensor</p> required <code>cfg</code> <code>ConvNetConfig</code> <p>configuration of the network</p> required <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>shape of the input tensor</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; obs_shape = (3, 64, 64)\n&gt;&gt;&gt; cfg = ConvNetConfig(\n...     channels=[64, 32, 16],\n...     conv_cfgs=[\n...         ConvConfig(kernel_size=4, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=4, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...         ConvConfig(kernel_size=4, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n...     ]\n... )\n&gt;&gt;&gt; ConvTranspose.get_input_shape(obs_shape, cfg)\n(16, 8, 8)\n</code></pre> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>@staticmethod\ndef get_input_shape(obs_shape: tuple[int, int, int], cfg: ConvNetConfig) -&gt; tuple[int, ...]:\n    \"\"\"\n    Get input shape of the decoder.\n\n    Parameters\n    ----------\n    obs_shape: tuple[int, int, int]\n        shape of the output tensor\n    cfg: ConvNetConfig\n        configuration of the network\n\n    Returns\n    -------\n    tuple[int, int, int]\n        shape of the input tensor\n\n    Examples\n    --------\n    &gt;&gt;&gt; obs_shape = (3, 64, 64)\n    &gt;&gt;&gt; cfg = ConvNetConfig(\n    ...     channels=[64, 32, 16],\n    ...     conv_cfgs=[\n    ...         ConvConfig(kernel_size=4, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n    ...         ConvConfig(kernel_size=4, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n    ...         ConvConfig(kernel_size=4, stride=2, padding=1, activation=\"ReLU\", norm=\"batch\", dropout=0.0),\n    ...     ]\n    ... )\n    &gt;&gt;&gt; ConvTranspose.get_input_shape(obs_shape, cfg)\n    (16, 8, 8)\n    \"\"\"\n    in_shape: tuple[int, int] = tuple(obs_shape[1:])  # type: ignore[assignment]\n    for conv_cfg in reversed(cfg.conv_cfgs):\n        padding, kernel, stride, dilation = (\n            conv_cfg.padding,\n            conv_cfg.kernel_size,\n            conv_cfg.stride,\n            conv_cfg.dilation,\n        )\n        in_shape = tuple(conv_transpose_in_shape(in_shape, padding, kernel, stride, dilation))  # type: ignore[assignment]\n    return (cfg.init_channel, *in_shape)\n</code></pre>"},{"location":"api/vision/#resnetpixunshuffle","title":"ResNetPixUnshuffle","text":""},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixUnshuffle","title":"ResNetPixUnshuffle","text":"<pre><code>ResNetPixUnshuffle(obs_shape, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>ResNet with PixelUnshuffle for Encoder.</p> <p>Parameters:</p> Name Type Description Default <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>shape of input tensor</p> required <code>cfg</code> <code>ResNetConfig</code> <p>configuration of the network</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; obs_shape = (3, 64, 64)\n&gt;&gt;&gt; cfg = ResNetConfig(\n...     conv_channel=64,\n...     conv_kernel=3,\n...     f_kernel=3,\n...     conv_activation=\"ReLU\",\n...     out_activation=\"ReLU\",\n...     n_res_blocks=2,\n...     scale_factor=2,\n...     n_scaling=3,\n...     norm=\"batch\",\n...     norm_cfg={},\n...     dropout=0.0\n... )\n&gt;&gt;&gt; encoder = ResNetPixUnshuffle(obs_shape, cfg)\n&gt;&gt;&gt; x = torch.randn(2, *obs_shape)\n&gt;&gt;&gt; y = encoder(x)\n&gt;&gt;&gt; y.shape\ntorch.Size([2, 64, 8, 8])\n</code></pre> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def __init__(\n    self,\n    obs_shape: tuple[int, int, int],\n    cfg: ResNetConfig,\n) -&gt; None:\n    super().__init__()\n\n    self.obs_shape = obs_shape\n    self.cfg = cfg\n\n    first_cfg = ConvConfig(\n        activation=cfg.conv_activation,\n        kernel_size=cfg.f_kernel,\n        stride=1,\n        padding=cfg.f_kernel // 2,\n        dilation=1,\n        groups=1,\n        bias=True,\n        dropout=cfg.dropout,\n        norm=cfg.norm,\n        norm_cfg=cfg.norm_cfg,\n        padding_mode=cfg.padding_mode,\n    )\n    # First layer\n    self.conv1 = ConvNormActivation(self.obs_shape[0], cfg.conv_channel, first_cfg)\n\n    # downsampling\n    downsample: list[nn.Module] = []\n    downsample_cfg = first_cfg\n    downsample_cfg.kernel_size = cfg.conv_kernel\n    downsample_cfg.padding = cfg.conv_kernel // 2\n    downsample_cfg.scale_factor = -cfg.scale_factor\n    for _ in range(cfg.n_scaling):\n        downsample += [\n            ConvNormActivation(cfg.conv_channel, cfg.conv_channel, downsample_cfg),\n        ]\n    self.downsample = nn.Sequential(*downsample)\n\n    # Residual blocks\n    res_blocks: list[nn.Module] = []\n    for _ in range(cfg.n_res_blocks):\n        res_blocks += [\n            ResidualBlock(\n                cfg.conv_channel,\n                cfg.conv_kernel,\n                cfg.conv_activation,\n                cfg.norm,\n                cfg.norm_cfg,\n                cfg.dropout,\n                cfg.padding_mode,\n            ),\n        ]\n        if cfg.attention is not None:\n            res_blocks += [Attention2d(cfg.conv_channel, nhead=None, attn_cfg=cfg.attention)]\n\n    self.res_blocks = nn.Sequential(*res_blocks)\n\n    cov2_cfg = first_cfg\n    cov2_cfg.kernel_size = cfg.conv_kernel\n    cov2_cfg.padding = cfg.conv_kernel // 2\n    cov2_cfg.scale_factor = 0\n\n    # Second conv layer post residual blocks\n    self.conv2 = ConvNormActivation(cfg.conv_channel, cfg.conv_channel, cov2_cfg)\n\n    # Final output layer\n    final_cfg = first_cfg\n    final_cfg.kernel_size = cfg.conv_kernel\n    final_cfg.padding = cfg.conv_kernel // 2\n\n    self.conv3 = ConvNormActivation(cfg.conv_channel, cfg.conv_channel, final_cfg)\n    self.last_channel = cfg.conv_channel\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixUnshuffle-attributes","title":"Attributes","text":""},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixUnshuffle.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixUnshuffle.conv1","title":"conv1  <code>instance-attribute</code>","text":"<pre><code>conv1 = ConvNormActivation(obs_shape[0], conv_channel, first_cfg)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixUnshuffle.conv2","title":"conv2  <code>instance-attribute</code>","text":"<pre><code>conv2 = ConvNormActivation(conv_channel, conv_channel, cov2_cfg)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixUnshuffle.conv3","title":"conv3  <code>instance-attribute</code>","text":"<pre><code>conv3 = ConvNormActivation(conv_channel, conv_channel, final_cfg)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixUnshuffle.conved_shape","title":"conved_shape  <code>property</code>","text":"<pre><code>conved_shape\n</code></pre> <p>Get the shape of the output tensor after convolutional layers.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>shape of the output tensor</p>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixUnshuffle.conved_size","title":"conved_size  <code>property</code>","text":"<pre><code>conved_size\n</code></pre> <p>Get the size of the output tensor after convolutional layers.</p> <p>Returns:</p> Type Description <code>int</code> <p>size of the output tensor</p>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixUnshuffle.downsample","title":"downsample  <code>instance-attribute</code>","text":"<pre><code>downsample = Sequential(*downsample)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixUnshuffle.last_channel","title":"last_channel  <code>instance-attribute</code>","text":"<pre><code>last_channel = conv_channel\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixUnshuffle.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixUnshuffle.res_blocks","title":"res_blocks  <code>instance-attribute</code>","text":"<pre><code>res_blocks = Sequential(*res_blocks)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixUnshuffle-functions","title":"Functions","text":""},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixUnshuffle.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor of shape (batch_size, *obs_shape)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor of shape (batch_size, self.last_channel, *self.conved_shape)</p> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        input tensor of shape (batch_size, *obs_shape)\n\n    Returns\n    -------\n    torch.Tensor\n        output tensor of shape (batch_size, self.last_channel, *self.conved_shape)\n\n    \"\"\"\n    out = self.conv1(x)\n    out1 = self.downsample(out)\n    out_res = self.res_blocks(out1)\n    out2 = self.conv2(out_res)\n    out = torch.add(out1, out2)\n    return self.conv3(out)\n</code></pre>"},{"location":"api/vision/#resnetpixshuffle","title":"ResNetPixShuffle","text":""},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle","title":"ResNetPixShuffle","text":"<pre><code>ResNetPixShuffle(in_shape, obs_shape, cfg)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>ResNet with PixelShuffle.</p> <p>Parameters:</p> Name Type Description Default <code>in_shape</code> <code>tuple[int, int, int]</code> <p>shape of input tensor</p> required <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>shape of output tensor</p> required <code>cfg</code> <code>ResNetConfig</code> <p>configuration of the network</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; in_shape = (128, 16, 16)\n&gt;&gt;&gt; obs_shape = (3, 64, 64)\n&gt;&gt;&gt; cfg = ResNetConfig(\n...     conv_channel=64,\n...     conv_kernel=3,\n...     f_kernel=3,\n...     conv_activation=\"ReLU\",\n...     out_activation=\"ReLU\",\n...     n_res_blocks=2,\n...     scale_factor=2,\n...     n_scaling=2,\n...     norm=\"batch\",\n...     norm_cfg={},\n...     dropout=0.0\n... )\n&gt;&gt;&gt; decoder = ResNetPixShuffle(in_shape, obs_shape, cfg)\n&gt;&gt;&gt; x = torch.randn(2, *in_shape)\n&gt;&gt;&gt; y = decoder(x)\n&gt;&gt;&gt; y.shape\ntorch.Size([2, 3, 64, 64])\n</code></pre> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def __init__(\n    self,\n    in_shape: tuple[int, int, int],\n    obs_shape: tuple[int, int, int],\n    cfg: ResNetConfig,\n) -&gt; None:\n    super().__init__()\n\n    self.in_shape = in_shape\n    self.obs_shape = obs_shape\n    self.conv_channel = cfg.conv_channel\n    self.conv_kernel = cfg.conv_kernel\n    self.final_kernel = cfg.f_kernel\n    self.conv_activation = cfg.conv_activation\n    self.out_activation = cfg.out_activation\n    self.n_res_blocks = cfg.n_res_blocks\n    self.upscale_factor = cfg.scale_factor\n    self.n_upsampling = cfg.n_scaling\n    self.norm = cfg.norm\n    self.norm_cfg = cfg.norm_cfg\n    self.dropout = cfg.dropout\n\n    self._scaling_factor = self.upscale_factor**self.n_upsampling\n\n    height = obs_shape[1]\n    width = obs_shape[2]\n\n    out_channels = obs_shape[0]\n    self.input_height, self.input_width = height // self._scaling_factor, width // self._scaling_factor\n    assert self.input_height == in_shape[1], f\"{self.input_height} != {in_shape[1]}\"\n    assert self.input_width == in_shape[2], f\"{self.input_width} != {in_shape[2]}\"\n\n    conv_cfg = ConvConfig(\n        activation=self.conv_activation,\n        kernel_size=self.conv_kernel,\n        stride=1,\n        padding=self.conv_kernel // 2,\n        dilation=1,\n        groups=1,\n        bias=True,\n        dropout=self.dropout,\n        norm=cfg.norm,\n        norm_cfg=cfg.norm_cfg,\n        padding_mode=cfg.padding_mode,\n    )\n\n    # First layer\n    self.conv1 = ConvNormActivation(in_shape[0], self.conv_channel, conv_cfg)\n\n    # Residual blocks\n\n    res_blocks: list[nn.Module] = []\n    for _ in range(self.n_res_blocks):\n        res_blocks += [\n            ResidualBlock(\n                self.conv_channel,\n                self.conv_kernel,\n                self.conv_activation,\n                self.norm,\n                self.norm_cfg,\n                self.dropout,\n                cfg.padding_mode,\n            ),\n        ]\n        if cfg.attention is not None:\n            res_blocks += [Attention2d(self.conv_channel, nhead=None, attn_cfg=cfg.attention)]\n    self.res_blocks = nn.Sequential(*res_blocks)\n\n    # Second conv layer post residual blocks\n    self.conv2 = ConvNormActivation(self.conv_channel, self.conv_channel, conv_cfg)\n\n    upscale_cfg = conv_cfg\n    upscale_cfg.scale_factor = self.upscale_factor\n\n    # Upsampling layers\n    upsampling: list[nn.Module] = []\n    for _ in range(self.n_upsampling):\n        upsampling += [\n            ConvNormActivation(self.conv_channel, self.conv_channel, upscale_cfg),\n        ]\n    self.upsampling = nn.Sequential(*upsampling)\n\n    final_cfg = conv_cfg\n    final_cfg.kernel_size = self.final_kernel\n    final_cfg.padding = self.final_kernel // 2\n    final_cfg.activation = self.out_activation\n    final_cfg.norm = \"none\"\n    final_cfg.norm_cfg = {}\n    final_cfg.dropout = 0.0\n    final_cfg.scale_factor = 0\n    # Final output layer\n    self.conv3 = ConvNormActivation(self.conv_channel, out_channels, final_cfg)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle-attributes","title":"Attributes","text":""},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.conv1","title":"conv1  <code>instance-attribute</code>","text":"<pre><code>conv1 = ConvNormActivation(in_shape[0], conv_channel, conv_cfg)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.conv2","title":"conv2  <code>instance-attribute</code>","text":"<pre><code>conv2 = ConvNormActivation(conv_channel, conv_channel, conv_cfg)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.conv3","title":"conv3  <code>instance-attribute</code>","text":"<pre><code>conv3 = ConvNormActivation(conv_channel, out_channels, final_cfg)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.conv_activation","title":"conv_activation  <code>instance-attribute</code>","text":"<pre><code>conv_activation = conv_activation\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.conv_channel","title":"conv_channel  <code>instance-attribute</code>","text":"<pre><code>conv_channel = conv_channel\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.conv_kernel","title":"conv_kernel  <code>instance-attribute</code>","text":"<pre><code>conv_kernel = conv_kernel\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = dropout\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.final_kernel","title":"final_kernel  <code>instance-attribute</code>","text":"<pre><code>final_kernel = f_kernel\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.in_shape","title":"in_shape  <code>instance-attribute</code>","text":"<pre><code>in_shape = in_shape\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.n_res_blocks","title":"n_res_blocks  <code>instance-attribute</code>","text":"<pre><code>n_res_blocks = n_res_blocks\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.n_upsampling","title":"n_upsampling  <code>instance-attribute</code>","text":"<pre><code>n_upsampling = n_scaling\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = norm\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.norm_cfg","title":"norm_cfg  <code>instance-attribute</code>","text":"<pre><code>norm_cfg = norm_cfg\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.out_activation","title":"out_activation  <code>instance-attribute</code>","text":"<pre><code>out_activation = out_activation\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.res_blocks","title":"res_blocks  <code>instance-attribute</code>","text":"<pre><code>res_blocks = Sequential(*res_blocks)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.upsampling","title":"upsampling  <code>instance-attribute</code>","text":"<pre><code>upsampling = Sequential(*upsampling)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.upscale_factor","title":"upscale_factor  <code>instance-attribute</code>","text":"<pre><code>upscale_factor = scale_factor\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle-functions","title":"Functions","text":""},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor of shape (batch_size, *in_shape)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor of shape (batch_size, *obs_shape)</p> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        input tensor of shape (batch_size, *in_shape)\n\n    Returns\n    -------\n    torch.Tensor\n        output tensor of shape (batch_size, *obs_shape)\n\n    \"\"\"\n    out1 = self.conv1(x)\n    out = self.res_blocks(out1)\n    out2 = self.conv2(out)\n    out = torch.add(out1, out2)\n    out = self.upsampling(out)\n    return self.conv3(out)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ResNetPixShuffle.get_input_shape","title":"get_input_shape  <code>staticmethod</code>","text":"<pre><code>get_input_shape(obs_shape, cfg)\n</code></pre> <p>Get input shape of the decoder.</p> <p>Parameters:</p> Name Type Description Default <code>obs_shape</code> <code>tuple[int, int, int]</code> <p>shape of the output tensor</p> required <code>cfg</code> <code>ResNetConfig</code> <p>configuration of the network</p> required <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>shape of the input tensor</p> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>@staticmethod\ndef get_input_shape(obs_shape: tuple[int, int, int], cfg: ResNetConfig) -&gt; tuple[int, int, int]:\n    \"\"\"\n    Get input shape of the decoder.\n\n    Parameters\n    ----------\n    obs_shape: tuple[int, int, int]\n        shape of the output tensor\n    cfg: ConvNetConfig\n        configuration of the network\n\n    Returns\n    -------\n    tuple[int, int, int]\n        shape of the input tensor\n\n    \"\"\"\n    return (\n        cfg.init_channel,\n        obs_shape[1] // (cfg.scale_factor**cfg.n_scaling),\n        obs_shape[2] // (cfg.scale_factor**cfg.n_scaling),\n    )\n</code></pre>"},{"location":"api/vision/#vit","title":"ViT","text":""},{"location":"api/vision/#ml_networks.torch.vision.ViT","title":"ViT","text":"<pre><code>ViT(in_shape, cfg, obs_shape=None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Vision Transformer for Encoder and Decoder.</p> <p>Parameters:</p> Name Type Description Default <code>in_shape</code> <code>tuple[int, int, int]</code> <p>shape of input tensor</p> required <code>cfg</code> <code>ViTConfig</code> <p>configuration of the network</p> required <code>obs_shape</code> <code>tuple[int, int, int] | None</code> <p>shape of output tensor. If None, it is considered as Encoder. Default is None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ml_networks.layers import TransformerConfig\n&gt;&gt;&gt; in_shape = (3, 64, 64)\n&gt;&gt;&gt; cfg = ViTConfig(\n...     patch_size=8,\n...     cls_token=True,\n...     transformer_cfg=TransformerConfig(\n...         d_model=64,\n...         nhead=8,\n...         dim_ff=256,\n...         n_layers=3,\n...         dropout=0.0,\n...         hidden_activation=\"ReLU\",\n...         output_activation=\"ReLU\"\n...     ),\n...     init_channel=3\n... )\n&gt;&gt;&gt; encoder = ViT(in_shape, cfg)\n&gt;&gt;&gt; x = torch.randn(2, *in_shape)\n&gt;&gt;&gt; y = encoder(x)\n&gt;&gt;&gt; y.shape\ntorch.Size([2, 1, 64, 64])\n</code></pre> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def __init__(\n    self,\n    in_shape: tuple[int, int, int],\n    cfg: ViTConfig,\n    obs_shape: tuple[int, int, int] | None = None,\n) -&gt; None:\n    super().__init__()\n\n    self.in_shape = in_shape\n    self.cfg = cfg\n    self.obs_shape = obs_shape if obs_shape is not None else in_shape\n    self.patch_size = cfg.patch_size\n\n    self.transformer_cfg = cfg.transformer_cfg\n    self.in_patch_dim = self.get_patch_dim(in_shape)\n    self.out_patch_dim = self.get_patch_dim(obs_shape) if obs_shape is not None else self.transformer_cfg.d_model\n    self.positional_embedding = PositionalEncoding(\n        self.in_patch_dim,\n        self.transformer_cfg.dropout,\n        max_len=self.get_n_patches(in_shape),\n    )\n    self.vit = TransformerLayer(\n        self.in_patch_dim,\n        self.out_patch_dim,\n        self.transformer_cfg,\n    )\n    self.is_encoder = obs_shape is None\n    if self.is_encoder:\n        self.n_patches = self.get_n_patches(in_shape)\n        self.patch_embed = PatchEmbed(\n            emb_dim=self.in_patch_dim,\n            patch_size=self.patch_size,\n            obs_shape=in_shape,\n        )\n    self.should_unpatchify = cfg.unpatchify\n    if cfg.cls_token:\n        self.cls_token = nn.Parameter(torch.randn(1, 1, self.in_patch_dim))\n    self.last_channel = self.get_n_patches(in_shape)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT-attributes","title":"Attributes","text":""},{"location":"api/vision/#ml_networks.torch.vision.ViT.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.cls_token","title":"cls_token  <code>instance-attribute</code>","text":"<pre><code>cls_token = Parameter(randn(1, 1, in_patch_dim))\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.conved_shape","title":"conved_shape  <code>property</code>","text":"<pre><code>conved_shape\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.conved_size","title":"conved_size  <code>property</code>","text":"<pre><code>conved_size\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.in_patch_dim","title":"in_patch_dim  <code>instance-attribute</code>","text":"<pre><code>in_patch_dim = get_patch_dim(in_shape)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.in_shape","title":"in_shape  <code>instance-attribute</code>","text":"<pre><code>in_shape = in_shape\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.is_encoder","title":"is_encoder  <code>instance-attribute</code>","text":"<pre><code>is_encoder = obs_shape is None\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.last_channel","title":"last_channel  <code>instance-attribute</code>","text":"<pre><code>last_channel = get_n_patches(in_shape)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.n_patches","title":"n_patches  <code>instance-attribute</code>","text":"<pre><code>n_patches = get_n_patches(in_shape)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.obs_shape","title":"obs_shape  <code>instance-attribute</code>","text":"<pre><code>obs_shape = obs_shape if obs_shape is not None else in_shape\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.out_patch_dim","title":"out_patch_dim  <code>instance-attribute</code>","text":"<pre><code>out_patch_dim = get_patch_dim(obs_shape) if obs_shape is not None else d_model\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.patch_embed","title":"patch_embed  <code>instance-attribute</code>","text":"<pre><code>patch_embed = PatchEmbed(emb_dim=in_patch_dim, patch_size=patch_size, obs_shape=in_shape)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.patch_size","title":"patch_size  <code>instance-attribute</code>","text":"<pre><code>patch_size = patch_size\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.positional_embedding","title":"positional_embedding  <code>instance-attribute</code>","text":"<pre><code>positional_embedding = PositionalEncoding(in_patch_dim, dropout, max_len=get_n_patches(in_shape))\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.should_unpatchify","title":"should_unpatchify  <code>instance-attribute</code>","text":"<pre><code>should_unpatchify = unpatchify\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.transformer_cfg","title":"transformer_cfg  <code>instance-attribute</code>","text":"<pre><code>transformer_cfg = transformer_cfg\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.vit","title":"vit  <code>instance-attribute</code>","text":"<pre><code>vit = TransformerLayer(in_patch_dim, out_patch_dim, transformer_cfg)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT-functions","title":"Functions","text":""},{"location":"api/vision/#ml_networks.torch.vision.ViT.forward","title":"forward","text":"<pre><code>forward(x, return_cls_token=False)\n</code></pre> <p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor of shape (batch_size, *in_shape)</p> required <code>return_cls_token</code> <code>bool</code> <p>whether to return cls_token. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor of shape (batch_size, *obs_shape)</p> <code>Tensor</code> <p>cls_token of shape (batch_size, self.out_patch_dim) if return_cls_token</p> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    return_cls_token: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        input tensor of shape (batch_size, *in_shape)\n    return_cls_token: bool\n        whether to return cls_token. Default is False.\n\n    Returns\n    -------\n    torch.Tensor\n        output tensor of shape (batch_size, *obs_shape)\n    torch.Tensor\n        cls_token of shape (batch_size, self.out_patch_dim) if return_cls_token\n\n    \"\"\"\n    x = self.patch_embed(x) if self.is_encoder else self.patchify(x)\n    x = self.positional_embedding(x)\n    if hasattr(self, \"cls_token\"):\n        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat([cls_token, x], dim=1)\n    x = self.vit(x)\n    if hasattr(self, \"cls_token\"):\n        cls_token = x[:, 0]\n        x = x[:, 1:]\n    if self.should_unpatchify:\n        x = self.unpatchify(x)\n    if return_cls_token and hasattr(self, \"cls_token\"):\n        return cls_token\n    return x\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.get_input_shape","title":"get_input_shape  <code>staticmethod</code>","text":"<pre><code>get_input_shape(obs_shape, cfg)\n</code></pre> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>@staticmethod\ndef get_input_shape(obs_shape: tuple[int, int, int], cfg: ViTConfig) -&gt; tuple[int, int, int]:\n    return (cfg.init_channel, obs_shape[1], obs_shape[2])\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.get_n_patches","title":"get_n_patches","text":"<pre><code>get_n_patches(obs_shape)\n</code></pre> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def get_n_patches(self, obs_shape: tuple[int, int, int]) -&gt; int:\n    return (obs_shape[1] // self.patch_size) * (obs_shape[2] // self.patch_size)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.get_patch_dim","title":"get_patch_dim","text":"<pre><code>get_patch_dim(obs_shape)\n</code></pre> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def get_patch_dim(self, obs_shape: tuple[int, int, int]) -&gt; int:\n    return self.patch_size**2 * obs_shape[0]\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.patchify","title":"patchify","text":"<pre><code>patchify(imgs)\n</code></pre> <p>\u753b\u50cf\u3092\u30d1\u30c3\u30c1\u306b\u5206\u5272\u3059\u308b.</p> <p>Parameters:</p> Name Type Description Default <code>imgs</code> <code>Tensor</code> <p>\u5165\u529b\u753b\u50cf. (N, C, H, W)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>\u30d1\u30c3\u30c1\u5316\u3057\u305f\u753b\u50cf. (N, L, patch_size**2 * D)</p> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def patchify(self, imgs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    \u753b\u50cf\u3092\u30d1\u30c3\u30c1\u306b\u5206\u5272\u3059\u308b.\n\n    Parameters\n    ----------\n    imgs: torch.Tensor\n        \u5165\u529b\u753b\u50cf. (N, C, H, W)\n\n    Returns\n    -------\n    torch.Tensor\n        \u30d1\u30c3\u30c1\u5316\u3057\u305f\u753b\u50cf. (N, L, patch_size**2 * D)\n    \"\"\"\n    p = self.patch_size\n    assert imgs.shape[-1] % p == 0\n    assert imgs.shape[-2] % p == 0\n    return rearrange(imgs, \"n c (h p1) (w p2) -&gt; n (h w) (p1 p2 c)\", p1=p, p2=p)\n</code></pre>"},{"location":"api/vision/#ml_networks.torch.vision.ViT.unpatchify","title":"unpatchify","text":"<pre><code>unpatchify(x)\n</code></pre> <p>\u30d1\u30c3\u30c1\u3092\u753b\u50cf\u306b\u623b\u3059.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>\u5165\u529b. (N, L, patch_size**2 * D)</p> required <p>Returns:</p> Type Description <code>    \u753b\u50cf. (N, C, H, W)</code> Source code in <code>src/ml_networks/torch/vision.py</code> <pre><code>def unpatchify(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    \u30d1\u30c3\u30c1\u3092\u753b\u50cf\u306b\u623b\u3059.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        \u5165\u529b. (N, L, patch_size**2 * D)\n\n    Returns\n    -------\n        \u753b\u50cf. (N, C, H, W)\n    \"\"\"\n    p = self.patch_size\n    h = self.obs_shape[1] // p\n    w = self.obs_shape[2] // p\n    assert h * w == x.shape[1], (\n        f\"{h * w} != {x.shape[1]}, please check the shape {x.shape} and obs_shape {self.obs_shape}\"\n    )\n    return rearrange(x, \"n (h w) (p1 p2 c) -&gt; n c (h p1) (w p2)\", h=h, w=w, p1=p, p2=p)\n</code></pre>"},{"location":"guides/advanced/","title":"\u9ad8\u5ea6\u306a\u6a5f\u80fd","text":"<p>HyperNetwork\u3001\u5bfe\u7167\u5b66\u7fd2\uff08Contrastive Learning\uff09\u3001Attention\u6a5f\u69cb\u306a\u3069\u3001\u9ad8\u5ea6\u306a\u6a5f\u80fd\u306e\u4f7f\u7528\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"guides/advanced/#hypernetwork","title":"HyperNetwork","text":"<p>HyperNetwork\u306f\u3001\u3042\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08HyperNet\uff09\u304c\u5225\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff08TargetNet\uff09\u306e\u91cd\u307f\u3092\u52d5\u7684\u306b\u751f\u6210\u3059\u308b\u30e1\u30bf\u5b66\u7fd2\u624b\u6cd5\u3067\u3059\u3002</p>"},{"location":"guides/advanced/#_2","title":"\u57fa\u672c\u7684\u306a\u4f7f\u7528\u65b9\u6cd5","text":"<pre><code>from ml_networks.torch import HyperNet\nfrom ml_networks import MLPConfig, LinearConfig\nimport torch\n\n# \u30bf\u30fc\u30b2\u30c3\u30c8\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b9a\u7fa9\ntarget_net = torch.nn.Sequential(\n    torch.nn.Linear(16, 32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32, 8),\n)\n\n# HyperNet\u306e\u8a2d\u5b9a\nmlp_cfg = MLPConfig(\n    hidden_dim=256,\n    n_layers=2,\n    output_activation=\"Identity\",\n    linear_cfg=LinearConfig(activation=\"ReLU\", bias=True)\n)\n\n# HyperNet\u306e\u4f5c\u6210\n# \u6761\u4ef6\u30d9\u30af\u30c8\u30eb\u304b\u3089\u30bf\u30fc\u30b2\u30c3\u30c8\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u91cd\u307f\u3092\u751f\u6210\nhypernet = HyperNet(\n    target_net=target_net,\n    input_dim=64,           # \u6761\u4ef6\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\n    mlp_cfg=mlp_cfg,\n)\n\n# \u6761\u4ef6\u30d9\u30af\u30c8\u30eb\ncondition = torch.randn(4, 64)\n\n# HyperNet\u3067\u751f\u6210\u3057\u305f\u91cd\u307f\u3067\u30bf\u30fc\u30b2\u30c3\u30c8\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b9f\u884c\nx = torch.randn(4, 16)\noutput = hypernet(x, condition)\n</code></pre>"},{"location":"guides/advanced/#_3","title":"\u5165\u529b\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0","text":"<p>HyperNet\u306f\u5165\u529b\u306e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30e2\u30fc\u30c9\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059\uff1a</p> <ul> <li><code>None</code>: \u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306a\u3057\uff08\u305d\u306e\u307e\u307e\u5165\u529b\uff09</li> <li><code>\"cos|sin\"</code>: cos/sin\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\uff08\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u985e\u4f3c\uff09</li> <li><code>\"z|1-z\"</code>: z, 1-z\u306e\u30da\u30a2\u306b\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0</li> </ul> <pre><code>hypernet = HyperNet(\n    target_net=target_net,\n    input_dim=64,\n    mlp_cfg=mlp_cfg,\n    input_mode=\"cos|sin\",   # cos/sin\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u4f7f\u7528\n)\n</code></pre>"},{"location":"guides/advanced/#contrastive-learning","title":"\u5bfe\u7167\u5b66\u7fd2\uff08Contrastive Learning\uff09","text":"<p>\u5bfe\u7167\u5b66\u7fd2\u306f\u3001\u985e\u4f3c\u3057\u305f\u30b5\u30f3\u30d7\u30eb\u306e\u8868\u73fe\u3092\u8fd1\u3065\u3051\u3001\u7570\u306a\u308b\u30b5\u30f3\u30d7\u30eb\u306e\u8868\u73fe\u3092\u9060\u3056\u3051\u308b\u305f\u3081\u306e\u624b\u6cd5\u3067\u3059\u3002</p>"},{"location":"guides/advanced/#_4","title":"\u57fa\u672c\u7684\u306a\u4f7f\u7528\u65b9\u6cd5","text":"<pre><code>from ml_networks.torch import ContrastiveLearningLoss\nfrom ml_networks import ContrastiveLearningConfig, MLPConfig, LinearConfig\nimport torch\n\n# \u5bfe\u7167\u5b66\u7fd2\u306e\u8a2d\u5b9a\ncfg = ContrastiveLearningConfig(\n    dim_feature=128,                    # \u7279\u5fb4\u91cf\u306e\u6b21\u5143\n    eval_func=MLPConfig(\n        hidden_dim=256,\n        n_layers=2,\n        output_activation=\"ReLU\",\n        linear_cfg=LinearConfig(\n            activation=\"ReLU\",\n            norm=\"layer\",\n            norm_cfg={\"eps\": 1e-5, \"elementwise_affine\": True, \"bias\": True},\n            dropout=0.1,\n            bias=True,\n        )\n    ),\n    cross_entropy_like=False,           # NCE\u640d\u5931\u3092\u4f7f\u7528\n)\n\n# \u5bfe\u7167\u5b66\u7fd2\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u4f5c\u6210\nmodel = ContrastiveLearningLoss(\n    dim_input1=256,   # \u5165\u529b1\u306e\u6b21\u5143\n    dim_input2=256,   # \u5165\u529b2\u306e\u6b21\u5143\n    cfg=cfg,\n)\n\n# 2\u3064\u306e\u30e2\u30c0\u30ea\u30c6\u30a3/\u30d3\u30e5\u30fc\u306e\u8868\u73fe\nx1 = torch.randn(32, 256)\nx2 = torch.randn(32, 256)\n\n# NCE\u640d\u5931\u306e\u8a08\u7b97\noutput = model.calc_nce(x1, x2)\nloss = output[\"nce\"]\n\n# \u57cb\u3081\u8fbc\u307f\u3082\u53d6\u5f97\u3059\u308b\u5834\u5408\noutput, embeddings = model.calc_nce(x1, x2, return_emb=True)\nemb1, emb2 = embeddings\nprint(emb1.shape)  # torch.Size([32, 128])\n</code></pre>"},{"location":"guides/advanced/#yaml","title":"YAML\u30d5\u30a1\u30a4\u30eb\u3067\u306e\u8a2d\u5b9a","text":"<pre><code>_target_: ml_networks.torch.contrastive.ContrastiveLearningLoss\ndim_input1: 256\ndim_input2: 256\ncfg:\n  _target_: ml_networks.config.ContrastiveLearningConfig\n  dim_feature: 128\n  eval_func:\n    _target_: ml_networks.config.MLPConfig\n    hidden_dim: 256\n    n_layers: 2\n    output_activation: ReLU\n    linear_cfg:\n      _target_: ml_networks.config.LinearConfig\n      activation: ReLU\n      norm: layer\n      dropout: 0.1\n      bias: true\n  cross_entropy_like: false\n</code></pre>"},{"location":"guides/advanced/#attention","title":"Attention\u6a5f\u69cb","text":""},{"location":"guides/advanced/#attention1d-attention2d","title":"Attention1d / Attention2d","text":"<p>1\u6b21\u5143\u30fb2\u6b21\u5143\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u30ec\u30a4\u30e4\u30fc\u3067\u3059\u3002</p> <pre><code>from ml_networks.torch import Attention1d, Attention2d\nimport torch\n\n# 1D\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\uff08\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u7528\uff09\nattn1d = Attention1d(in_channels=64, nhead=8)\nx = torch.randn(4, 64, 128)          # (batch, channels, length)\nout = attn1d(x)\nprint(out.shape)  # torch.Size([4, 64, 128])\n\n# 2D\u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\uff08\u753b\u50cf\u30c7\u30fc\u30bf\u7528\uff09\nattn2d = Attention2d(in_channels=64, nhead=8)\nx = torch.randn(4, 64, 32, 32)       # (batch, channels, height, width)\nout = attn2d(x)\nprint(out.shape)  # torch.Size([4, 64, 32, 32])\n</code></pre>"},{"location":"guides/advanced/#transformerlayer","title":"TransformerLayer","text":"<p>Transformer\u30a8\u30f3\u30b3\u30fc\u30c0\u30d6\u30ed\u30c3\u30af\u3092\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002</p> <pre><code>from ml_networks.torch import TransformerLayer\nfrom ml_networks import TransformerConfig\nimport torch\n\ncfg = TransformerConfig(\n    d_model=256,\n    nhead=8,\n    dim_ff=512,\n    n_layers=4,\n    dropout=0.1,\n    hidden_activation=\"GELU\",\n    output_activation=\"GELU\",\n)\n\n# TransformerLayer\u306f\u8a2d\u5b9a\u304b\u3089\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\n# \uff08\u901a\u5e38\u306fEncoder/Decoder\u306e\u4e00\u90e8\u3068\u3057\u3066\u4f7f\u7528\uff09\n</code></pre>"},{"location":"guides/advanced/#patchembed","title":"PatchEmbed","text":"<p>Vision Transformer\u7528\u306e\u30d1\u30c3\u30c1\u57cb\u3081\u8fbc\u307f\u30ec\u30a4\u30e4\u30fc\u3067\u3059\u3002</p> <pre><code>from ml_networks.torch import PatchEmbed\nimport torch\n\n# \u753b\u50cf\u3092\u30d1\u30c3\u30c1\u306b\u5206\u5272\u3057\u3066\u57cb\u3081\u8fbc\u307f\npatch_embed = PatchEmbed(\n    in_channels=3,\n    patch_size=16,\n    embed_dim=256,\n)\n\nx = torch.randn(4, 3, 64, 64)\npatches = patch_embed(x)\nprint(patches.shape)  # (4, 16, 256) = (batch, num_patches, embed_dim)\n</code></pre>"},{"location":"guides/advanced/#positionalencoding","title":"PositionalEncoding","text":"<p>Transformer\u306e\u4f4d\u7f6e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3067\u3059\u3002</p> <pre><code>from ml_networks.torch import PositionalEncoding\nimport torch\n\npos_enc = PositionalEncoding(d_model=256, max_len=512)\nx = torch.randn(4, 100, 256)         # (batch, seq_len, d_model)\nout = pos_enc(x)\nprint(out.shape)  # torch.Size([4, 100, 256])\n</code></pre>"},{"location":"guides/advanced/#residualblock","title":"ResidualBlock","text":"<p>\u6b8b\u5dee\u63a5\u7d9a\u3092\u6301\u3064\u30d6\u30ed\u30c3\u30af\u3067\u3059\u3002ConvNet\u5185\u90e8\u3067\u4f7f\u7528\u3055\u308c\u307e\u3059\u304c\u3001\u72ec\u7acb\u3057\u3066\u3082\u4f7f\u7528\u53ef\u80fd\u3067\u3059\u3002</p> <pre><code>from ml_networks.torch import ResidualBlock\nfrom ml_networks import ConvConfig\nimport torch\n\ncfg = ConvConfig(\n    kernel_size=3,\n    stride=1,\n    padding=1,\n    activation=\"ReLU\",\n    norm=\"batch\",\n)\n\nblock = ResidualBlock(in_channels=64, out_channels=64, conv_cfg=cfg)\nx = torch.randn(4, 64, 32, 32)\nout = block(x)\nprint(out.shape)  # torch.Size([4, 64, 32, 32])\n</code></pre>"},{"location":"guides/advanced/#bsqcodebook","title":"BSQCodebook\uff08\u30d9\u30af\u30c8\u30eb\u91cf\u5b50\u5316\uff09","text":"<p>BSQ\uff08Binary Spherical Quantization\uff09\u30b3\u30fc\u30c9\u30d6\u30c3\u30af\u306f\u3001\u30d9\u30af\u30c8\u30eb\u91cf\u5b50\u5316\u306b\u3088\u308b\u96e2\u6563\u8868\u73fe\u5b66\u7fd2\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002</p> <pre><code>from ml_networks.torch import BSQCodebook\nimport torch\n\ncodebook = BSQCodebook(\n    dim=64,           # \u5165\u529b\u7279\u5fb4\u91cf\u306e\u6b21\u5143\n    codebook_dim=8,   # \u30b3\u30fc\u30c9\u30d6\u30c3\u30af\u306e\u6b21\u5143\n)\n\n# \u9023\u7d9a\u7684\u306a\u7279\u5fb4\u91cf\u3092\u96e2\u6563\u7684\u306a\u30b3\u30fc\u30c9\u306b\u5909\u63db\nfeatures = torch.randn(32, 64)\nquantized = codebook(features)\n</code></pre>"},{"location":"guides/advanced/#l2norm","title":"L2Norm","text":"<p>L2\u6b63\u898f\u5316\u30ec\u30a4\u30e4\u30fc\u3067\u3059\u3002\u7279\u5fb4\u91cf\u3092\u5358\u4f4d\u8d85\u7403\u4e0a\u306b\u5c04\u5f71\u3057\u307e\u3059\u3002</p> <pre><code>from ml_networks.torch import Activation\nimport torch\n\n# L2Norm\u306f\u6d3b\u6027\u5316\u95a2\u6570\u3068\u3057\u3066\u4f7f\u7528\u53ef\u80fd\nl2_norm = Activation(\"L2Norm\")\nx = torch.randn(32, 64)\nnormalized = l2_norm(x)\n# normalized \u306e\u5404\u884c\u306eL2\u30ce\u30eb\u30e0\u306f1\u306b\u306a\u308b\n</code></pre>"},{"location":"guides/advanced/#basemodule","title":"BaseModule","text":"<p><code>BaseModule</code>\u306fPyTorch Lightning\u306e<code>LightningModule</code>\u3092\u62e1\u5f35\u3057\u305f\u57fa\u5e95\u30af\u30e9\u30b9\u3067\u3059\u3002</p> <pre><code>from ml_networks.torch import BaseModule\n\nclass MyModel(BaseModule):\n    def __init__(self):\n        super().__init__()\n        # \u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\n</code></pre>"},{"location":"guides/config-management/","title":"\u8a2d\u5b9a\u7ba1\u7406","text":"<p>\u3053\u306e\u30ac\u30a4\u30c9\u3067\u306f\u3001Hydra\u306e<code>hydra.utils.instantiate</code>\u3092\u4f7f\u7528\u3057\u3066YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u307f\u3001\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3059\u308b\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\u3002\u3053\u308c\u306f\u3001\u8a2d\u5b9a\u3092\u30d9\u30bf\u66f8\u304d\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u3001YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8aad\u307f\u8fbc\u3080\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9\u3067\u3059\u3002</p>"},{"location":"guides/config-management/#_2","title":"\u6982\u8981","text":"<p><code>ml-networks</code>\u3067\u306f\u3001\u8a2d\u5b9a\u3092Python\u30b3\u30fc\u30c9\u306b\u30d9\u30bf\u66f8\u304d\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u3001YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8aad\u307f\u8fbc\u3080\u3053\u3068\u3092\u63a8\u5968\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u4ee5\u4e0b\u306e\u30e1\u30ea\u30c3\u30c8\u304c\u3042\u308a\u307e\u3059\uff1a</p>"},{"location":"guides/config-management/#_3","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<p>Hydra\u3092\u4f7f\u7528\u3059\u308b\u306b\u306f\u3001<code>hydra-core</code>\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff1a</p> <pre><code>pip install hydra-core\n</code></pre> <p>\u307e\u305f\u306f\u3001<code>omegaconf</code>\u304c\u65e2\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u3001<code>hydra-core</code>\u3082\u4e00\u7dd2\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u307e\u3059\u3002</p> <ul> <li>\u8a2d\u5b9a\u306e\u5206\u96e2: \u30b3\u30fc\u30c9\u3068\u8a2d\u5b9a\u3092\u5206\u96e2\u3067\u304d\u308b</li> <li>\u518d\u5229\u7528\u6027: \u540c\u3058\u8a2d\u5b9a\u3092\u8907\u6570\u306e\u5b9f\u9a13\u3067\u518d\u5229\u7528\u3067\u304d\u308b</li> <li>\u53ef\u8aad\u6027: YAML\u30d5\u30a1\u30a4\u30eb\u306f\u69cb\u9020\u5316\u3055\u308c\u3066\u304a\u308a\u3001\u8aad\u307f\u3084\u3059\u3044</li> <li>\u67d4\u8edf\u6027: \u8a2d\u5b9a\u3092\u5909\u66f4\u3059\u308b\u969b\u306b\u30b3\u30fc\u30c9\u3092\u5909\u66f4\u3059\u308b\u5fc5\u8981\u304c\u306a\u3044</li> </ul>"},{"location":"guides/config-management/#_4","title":"\u57fa\u672c\u7684\u306a\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"guides/config-management/#1-yaml","title":"1. YAML\u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u6210","text":"<p>\u307e\u305a\u3001\u8a2d\u5b9a\u3092YAML\u30d5\u30a1\u30a4\u30eb\u306b\u8a18\u8ff0\u3057\u307e\u3059\u3002Hydra\u306e<code>instantiate</code>\u3092\u4f7f\u7528\u3059\u308b\u306b\u306f\u3001<code>_target_</code>\u30d5\u30a3\u30fc\u30eb\u30c9\u3067\u5bfe\u8c61\u30af\u30e9\u30b9\u306e\u5b8c\u5168\u306a\u30d1\u30b9\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002</p> <p>\u4f8b: <code>configs/mlp_config.yaml</code></p> <pre><code>_target_: ml_networks.layers.MLPLayer\ninput_dim: 16\noutput_dim: 8\nmlp_config:\n  _target_: ml_networks.config.MLPConfig\n  hidden_dim: 128\n  n_layers: 2\n  output_activation: Tanh\n  linear_cfg:\n    _target_: ml_networks.config.LinearConfig\n    activation: ReLU\n    bias: true\n    norm: none\n    dropout: 0.0\n</code></pre> <p>\u4f8b: <code>configs/encoder_config.yaml</code></p> <pre><code>_target_: ml_networks.vision.Encoder\nfeature_dim: 64\nobs_shape: [3, 64, 64]\nencoder_cfg:\n  _target_: ml_networks.config.ConvNetConfig\n  channels: [16, 32, 64]\n  conv_cfgs:\n    - _target_: ml_networks.config.ConvConfig\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: ReLU\n      bias: true\n      norm: none\n      dropout: 0.0\n    - _target_: ml_networks.config.ConvConfig\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: ReLU\n      bias: true\n      norm: none\n      dropout: 0.0\n    - _target_: ml_networks.config.ConvConfig\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: ReLU\n      bias: true\n      norm: none\n      dropout: 0.0\nfull_connection_cfg:\n  _target_: ml_networks.config.LinearConfig\n  activation: ReLU\n  bias: true\n</code></pre>"},{"location":"guides/config-management/#2-python","title":"2. Python\u30b3\u30fc\u30c9\u3067\u306e\u4f7f\u7528","text":"<p>YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u307f\u3001<code>hydra.utils.instantiate</code>\u3092\u4f7f\u7528\u3057\u3066\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3057\u307e\u3059\u3002</p> <pre><code>from hydra.utils import instantiate\nfrom omegaconf import OmegaConf\n\n# YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080\ncfg = OmegaConf.load(\"configs/mlp_config.yaml\")\n\n# instantiate\u3092\u4f7f\u7528\u3057\u3066\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\nmlp = instantiate(cfg)\n\n# \u4f7f\u7528\nimport torch\nx = torch.randn(32, 16)\ny = mlp(x)\nprint(y.shape)  # torch.Size([32, 8])\n</code></pre>"},{"location":"guides/config-management/#3","title":"3. \u30cd\u30b9\u30c8\u3055\u308c\u305f\u8a2d\u5b9a\u306e\u4f8b","text":"<p>\u8907\u96d1\u306a\u8a2d\u5b9a\u3067\u3082\u3001\u540c\u69d8\u306bYAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8aad\u307f\u8fbc\u3081\u307e\u3059\u3002</p> <p>\u4f8b: <code>configs/distribution_config.yaml</code></p> <pre><code>encoder:\n  _target_: ml_networks.vision.Encoder\n  feature_dim: 128  # \u6b63\u898f\u5206\u5e03\u306e\u5834\u5408\u3001\u7279\u5fb4\u91cf\u6b21\u5143\u306e2\u500d\u304c\u5fc5\u8981\n  obs_shape: [3, 64, 64]\n  encoder_cfg:\n    _target_: ml_networks.config.ConvNetConfig\n    channels: [16, 32, 64]\n    conv_cfgs:\n      - _target_: ml_networks.config.ConvConfig\n        kernel_size: 3\n        stride: 2\n        padding: 1\n        activation: ReLU\n  full_connection_cfg:\n    _target_: ml_networks.config.MLPConfig\n    hidden_dim: 128\n    n_layers: 2\n    output_activation: Identity\n    linear_cfg:\n      _target_: ml_networks.config.LinearConfig\n      activation: ReLU\n      bias: true\n\ndistribution:\n  _target_: ml_networks.distributions.Distribution\n  in_dim: 64\n  dist: normal\n  n_groups: 1\n  spherical: false\n</code></pre> <pre><code>from hydra.utils import instantiate\nfrom omegaconf import OmegaConf\nimport torch\n\n# \u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080\ncfg = OmegaConf.load(\"configs/distribution_config.yaml\")\n\n# \u30a8\u30f3\u30b3\u30fc\u30c0\u3068\u5206\u5e03\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\nencoder = instantiate(cfg.encoder)\ndist = instantiate(cfg.distribution)\n\n# \u4f7f\u7528\nobs = torch.randn(32, 3, 64, 64)\nz = encoder(obs)\ndist_z = dist(z)\nprint(dist_z)\n</code></pre>"},{"location":"guides/config-management/#_5","title":"\u8a2d\u5b9a\u306e\u90e8\u5206\u7684\u306a\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316","text":"<p>\u8a2d\u5b9a\u306e\u4e00\u90e8\u3060\u3051\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002</p> <pre><code>from hydra.utils import instantiate\nfrom omegaconf import OmegaConf\n\ncfg = OmegaConf.load(\"configs/encoder_config.yaml\")\n\n# encoder_cfg\u3060\u3051\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\nencoder_cfg = instantiate(cfg.encoder_cfg)\n\n# \u305d\u306e\u5f8c\u3001\u624b\u52d5\u3067Encoder\u3092\u4f5c\u6210\nfrom ml_networks.vision import Encoder\nencoder = Encoder(\n    feature_dim=cfg.feature_dim,\n    obs_shape=tuple(cfg.obs_shape),\n    encoder_cfg=encoder_cfg,\n    full_connection_cfg=instantiate(cfg.full_connection_cfg)\n)\n</code></pre>"},{"location":"guides/config-management/#_6","title":"\u8a2d\u5b9a\u306e\u691c\u8a3c","text":"<p>\u8a2d\u5b9a\u304c\u6b63\u3057\u3044\u304b\u3069\u3046\u304b\u3092\u691c\u8a3c\u3059\u308b\u306b\u306f\u3001<code>OmegaConf</code>\u306e\u6a5f\u80fd\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002</p> <pre><code>from omegaconf import OmegaConf\n\ncfg = OmegaConf.load(\"configs/mlp_config.yaml\")\n\n# \u8a2d\u5b9a\u306e\u69cb\u9020\u3092\u78ba\u8a8d\nprint(OmegaConf.to_yaml(cfg))\n\n# \u5fc5\u9808\u30d5\u30a3\u30fc\u30eb\u30c9\u306e\u30c1\u30a7\u30c3\u30af\nif \"input_dim\" not in cfg:\n    raise ValueError(\"input_dim is required\")\n</code></pre>"},{"location":"guides/config-management/#_7","title":"\u8a2d\u5b9a\u306e\u7d99\u627f\u3068\u5408\u6210","text":"<p>\u8907\u6570\u306e\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3092\u7d44\u307f\u5408\u308f\u305b\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002</p> <p>\u4f8b: <code>configs/base_encoder.yaml</code></p> <pre><code>encoder_cfg:\n  _target_: ml_networks.config.ConvNetConfig\n  channels: [16, 32, 64]\n  conv_cfgs:\n    - _target_: ml_networks.config.ConvConfig\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: ReLU\n</code></pre> <p>\u4f8b: <code>configs/custom_encoder.yaml</code></p> <pre><code>defaults:\n  - base_encoder\n\nfeature_dim: 128\nobs_shape: [3, 128, 128]\n</code></pre>"},{"location":"guides/config-management/#_8","title":"\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9","text":"<ol> <li>\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306e\u547d\u540d\u898f\u5247: <code>{component}_config.yaml</code>\u306e\u3088\u3046\u306a\u547d\u540d\u898f\u5247\u3092\u4f7f\u7528\u3059\u308b</li> <li>\u8a2d\u5b9a\u306e\u968e\u5c64\u5316: \u5171\u901a\u306e\u8a2d\u5b9a\u306f\u5225\u30d5\u30a1\u30a4\u30eb\u306b\u5206\u96e2\u3057\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u7d99\u627f\u3059\u308b</li> <li>\u8a2d\u5b9a\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u7ba1\u7406: \u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u3082Git\u3067\u7ba1\u7406\u3057\u3001\u5b9f\u9a13\u306e\u518d\u73fe\u6027\u3092\u78ba\u4fdd\u3059\u308b</li> <li>\u8a2d\u5b9a\u306e\u691c\u8a3c: \u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3093\u3060\u5f8c\u3001\u5fc5\u9808\u30d5\u30a3\u30fc\u30eb\u30c9\u306e\u5b58\u5728\u3092\u78ba\u8a8d\u3059\u308b</li> <li>\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u5316: \u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306b\u30b3\u30e1\u30f3\u30c8\u3092\u8ffd\u52a0\u3057\u3066\u3001\u5404\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u610f\u5473\u3092\u8aac\u660e\u3059\u308b</li> </ol>"},{"location":"guides/config-management/#_9","title":"\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb\u306e\u4f8b","text":""},{"location":"guides/config-management/#mlp","title":"MLP\u8a2d\u5b9a","text":"<pre><code># configs/mlp_simple.yaml\n_target_: ml_networks.layers.MLPLayer\ninput_dim: 16\noutput_dim: 8\nmlp_config:\n  _target_: ml_networks.config.MLPConfig\n  hidden_dim: 128\n  n_layers: 2\n  output_activation: Tanh\n  linear_cfg:\n    _target_: ml_networks.config.LinearConfig\n    activation: ReLU\n    bias: true\n</code></pre>"},{"location":"guides/config-management/#encoderresnet-pixelunshuffle","title":"Encoder\u8a2d\u5b9a\uff08ResNet + PixelUnShuffle\uff09","text":"<pre><code># configs/encoder_resnet.yaml\n_target_: ml_networks.vision.Encoder\nfeature_dim: 64\nobs_shape: [3, 64, 64]\nencoder_cfg:\n  _target_: ml_networks.config.ResNetConfig\n  conv_channel: 64\n  conv_kernel: 3\n  f_kernel: 3\n  conv_activation: ReLU\n  out_activation: ReLU\n  n_res_blocks: 3\n  scale_factor: 2\n  n_scaling: 3\n  norm: batch\n  norm_cfg:\n    affine: true\n  dropout: 0.0\nfull_connection_cfg:\n  _target_: ml_networks.config.LinearConfig\n  activation: ReLU\n  bias: true\n</code></pre>"},{"location":"guides/config-management/#decoder","title":"Decoder\u8a2d\u5b9a","text":"<pre><code># configs/decoder_config.yaml\n_target_: ml_networks.vision.Decoder\nfeature_dim: 64\nobs_shape: [3, 64, 64]\ndecoder_cfg:\n  _target_: ml_networks.config.ConvNetConfig\n  channels: [64, 32, 16]\n  conv_cfgs:\n    - _target_: ml_networks.config.ConvConfig\n      output_padding: 0\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: ReLU\n    - _target_: ml_networks.config.ConvConfig\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: ReLU\n    - _target_: ml_networks.config.ConvConfig\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: Tanh\nfull_connection_cfg:\n  _target_: ml_networks.config.LinearConfig\n  activation: ReLU\n  bias: true\n</code></pre>"},{"location":"guides/config-management/#_10","title":"\u30c8\u30e9\u30d6\u30eb\u30b7\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0","text":""},{"location":"guides/config-management/#_11","title":"\u3088\u304f\u3042\u308b\u30a8\u30e9\u30fc","text":"<ol> <li><code>_target_</code>\u304c\u898b\u3064\u304b\u3089\u306a\u3044: \u30af\u30e9\u30b9\u306e\u5b8c\u5168\u306a\u30d1\u30b9\u304c\u6b63\u3057\u3044\u304b\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</li> <li>\u578b\u306e\u4e0d\u4e00\u81f4: YAML\u306e\u5024\u304c\u671f\u5f85\u3055\u308c\u308b\u578b\u3068\u4e00\u81f4\u3057\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\uff08\u4f8b: <code>true</code> vs <code>True</code>\uff09</li> <li>\u5fc5\u9808\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6b20\u5982: \u8a2d\u5b9a\u306b\u5fc5\u9808\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u542b\u307e\u308c\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044</li> </ol>"},{"location":"guides/config-management/#_12","title":"\u30c7\u30d0\u30c3\u30b0\u306e\u30d2\u30f3\u30c8","text":"<pre><code>from omegaconf import OmegaConf\n\ncfg = OmegaConf.load(\"configs/mlp_config.yaml\")\n\n# \u8a2d\u5b9a\u306e\u69cb\u9020\u3092\u78ba\u8a8d\nprint(OmegaConf.to_yaml(cfg))\n\n# \u7279\u5b9a\u306e\u5024\u3092\u78ba\u8a8d\nprint(cfg.mlp_config.hidden_dim)\n\n# \u8a2d\u5b9a\u3092\u691c\u8a3c\nOmegaConf.set_struct(cfg, True)  # \u69cb\u9020\u3092\u56fa\u5b9a\u3057\u3066\u691c\u8a3c\n</code></pre>"},{"location":"guides/data-io/","title":"\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u3068\u8aad\u307f\u8fbc\u307f","text":"<p>\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u3068\u8aad\u307f\u8fbc\u307f\u6a5f\u80fd\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"guides/data-io/#blosc2","title":"blosc2\u5f62\u5f0f\u3067\u306e\u4fdd\u5b58\u30fb\u8aad\u307f\u8fbc\u307f","text":"<p><code>ml-networks</code>\u3067\u306f\u3001blosc2\u5f62\u5f0f\u3067\u306e\u30c7\u30fc\u30bf\u4fdd\u5b58\u30fb\u8aad\u307f\u8fbc\u307f\u3092\u63a8\u5968\u3057\u3066\u3044\u307e\u3059\u3002\u5727\u7e2e\u7387\u304c\u9ad8\u304f\u3001\u4fdd\u5b58\u3082\u9ad8\u901f\u3067\u3059\u3002</p>"},{"location":"guides/data-io/#_2","title":"\u57fa\u672c\u7684\u306a\u4f7f\u7528\u65b9\u6cd5","text":"<pre><code>from ml_networks import save_blosc2, load_blosc2\nimport torch\nimport numpy as np\n\n# numpy\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\ndata = torch.randn(32, 3, 64, 64).detach().cpu().numpy()\n\n# \u4fdd\u5b58\nsave_blosc2(data, \"dataset/image.blosc2\")\n\n# \u8aad\u307f\u8fbc\u307f\nloaded_data = load_blosc2(\"dataset/image.blosc2\")\n</code></pre>"},{"location":"guides/data-io/#_3","title":"\u5206\u5e03\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58","text":"<p>\u5206\u5e03\u30c7\u30fc\u30bf\u3082\u4fdd\u5b58\u3067\u304d\u307e\u3059\uff1a</p> <pre><code>from ml_networks import Distribution\n\ndist = Distribution(\n    in_dim=feature_dim,\n    dist=\"normal\",\n    n_groups=1,\n)\n\nz = encoder(obs)\ndist_z = dist(z)\n\n# \u5206\u5e03\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\ndist_z.save(\"reports\")\n# reports\u306e\u4e0b\u306bmean.blosc2, std.blosc2, stoch.blosc2\u304c\u4fdd\u5b58\u3055\u308c\u308b\n# \u4ed6\u306e\u5206\u5e03\u30c7\u30fc\u30bf\u3082\u540c\u69d8\u306b\u4fdd\u5b58\u3055\u308c\u308b\n</code></pre>"},{"location":"guides/data-io/#_4","title":"\u6ce8\u610f\u4e8b\u9805","text":"<ul> <li>blosc2\u5f62\u5f0f\u306f\u5727\u7e2e\u7387\u304c\u9ad8\u304f\u3001\u4fdd\u5b58\u3082\u9ad8\u901f\u3067\u3059</li> <li>\u5927\u304d\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u6271\u3046\u5834\u5408\u306b\u7279\u306b\u6709\u52b9\u3067\u3059</li> <li>\u5206\u5e03\u30c7\u30fc\u30bf\u3092\u4fdd\u5b58\u3059\u308b\u5834\u5408\u306f\u3001\u5404\u30d1\u30e9\u30e1\u30fc\u30bf\uff08mean\u3001std\u3001stoch\u306a\u3069\uff09\u304c\u500b\u5225\u306e\u30d5\u30a1\u30a4\u30eb\u3068\u3057\u3066\u4fdd\u5b58\u3055\u308c\u307e\u3059</li> </ul>"},{"location":"guides/decoder/","title":"Decoder\u30ac\u30a4\u30c9","text":"<p>Decoder\u306e\u4f7f\u7528\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p> <p>\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9</p> <p>YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080\u65b9\u6cd5\u306b\u3064\u3044\u3066\u306f\u3001\u8a2d\u5b9a\u7ba1\u7406\u30ac\u30a4\u30c9\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"guides/decoder/#_1","title":"\u6982\u8981","text":"<p>Decoder\u306f\u7279\u5fb4\u91cf\u304b\u3089\u753b\u50cf\u306a\u3069\u3092\u518d\u69cb\u6210\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3059\u3002Encoder\u3068\u5bfe\u306b\u306a\u308b\u69cb\u9020\u3092\u6301\u3061\u307e\u3059\u3002</p>"},{"location":"guides/decoder/#_2","title":"\u57fa\u672c\u7684\u306a\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"guides/decoder/#1-yaml","title":"\u65b9\u6cd51: YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8aad\u307f\u8fbc\u3080\uff08\u63a8\u5968\uff09","text":"<p>\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb <code>configs/decoder_config.yaml</code> \u3092\u4f5c\u6210\u3057\u307e\u3059\uff1a</p> <pre><code>_target_: ml_networks.vision.Decoder\nfeature_dim: 64\nobs_shape: [3, 64, 64]\ndecoder_cfg:\n  _target_: ml_networks.config.ConvNetConfig\n  channels: [64, 32, 16]\n  conv_cfgs:\n    - _target_: ml_networks.config.ConvConfig\n      output_padding: 0\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: ReLU\n    - _target_: ml_networks.config.ConvConfig\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: ReLU\n    - _target_: ml_networks.config.ConvConfig\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: Tanh\nfull_connection_cfg:\n  _target_: ml_networks.config.LinearConfig\n  activation: ReLU\n  bias: true\n</code></pre> <p>Python\u30b3\u30fc\u30c9\uff1a</p> <pre><code>from hydra.utils import instantiate\nfrom omegaconf import OmegaConf\nimport torch\n\n# YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080\ncfg = OmegaConf.load(\"configs/decoder_config.yaml\")\n\n# instantiate\u3092\u4f7f\u7528\u3057\u3066\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\ndecoder = instantiate(cfg)\n\n# \u63a8\u8ad6\nz = torch.randn(32, 64)\npredicted_obs = decoder(z)\nprint(predicted_obs.shape)  # torch.Size([32, 3, 64, 64])\n</code></pre>"},{"location":"guides/decoder/#2-python","title":"\u65b9\u6cd52: Python\u30b3\u30fc\u30c9\u3067\u76f4\u63a5\u8a2d\u5b9a\u3059\u308b","text":"<pre><code>from ml_networks import Decoder, ConvNetConfig, ConvConfig, LinearConfig\nimport torch\n\n# Decoder\u306e\u8a2d\u5b9a\ndecoder_cfg = ConvNetConfig(\n    channels=[64, 32, 16],  # \u5404\u5c64\u306echannel\u6570\n    conv_cfgs=[\n        ConvConfig(\n            output_padding=0,  # ConvTranspose2d\u306e\u307f\u306b\u5229\u7528\u3055\u308c\u308b\n            kernel_size=3,\n            stride=2,\n            padding=1,\n        ),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"Tanh\"),  # \u6700\u5f8c\u306e\u5c64\n    ]\n)\n\n# \u5168\u7d50\u5408\u5c64\u306e\u8a2d\u5b9a\nfull_connection_cfg = LinearConfig(\n    activation=\"ReLU\",\n    bias=True,\n)\n\n# Decoder\u306e\u4f5c\u6210\nobs_shape = (3, 64, 64)\nfeature_dim = 64\ndecoder = Decoder(feature_dim, obs_shape, decoder_cfg, full_connection_cfg)\n\n# \u63a8\u8ad6\nz = torch.randn(32, feature_dim)\npredicted_obs = decoder(z)\nprint(predicted_obs.shape)  # torch.Size([32, 3, 64, 64])\n</code></pre>"},{"location":"guides/decoder/#_3","title":"\u30d0\u30c3\u30af\u30dc\u30fc\u30f3\u306e\u7a2e\u985e","text":""},{"location":"guides/decoder/#convtranspose","title":"ConvTranspose","text":"<p>\u591a\u5c64ConvTranspose\u306e\u30c7\u30b3\u30fc\u30c0\uff1a</p> <pre><code>decoder_cfg = ConvNetConfig(\n    channels=[64, 32, 16],\n    conv_cfgs=[\n        ConvConfig(\n            output_padding=0,  # \u51fa\u529b\u30d1\u30c7\u30a3\u30f3\u30b0\n            kernel_size=3,\n            stride=2,\n            padding=1,\n        ),\n        # ... \u4ed6\u306e\u5c64\n    ]\n)\n</code></pre>"},{"location":"guides/decoder/#resnet-pixelshuffle","title":"ResNet + PixelShuffle","text":"<p>ResNet\u3068PixelShuffle\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u30c7\u30b3\u30fc\u30c0\uff1a</p> <p>YAML\u30d5\u30a1\u30a4\u30eb (<code>configs/decoder_resnet.yaml</code>):</p> <pre><code>_target_: ml_networks.vision.Decoder\nfeature_dim: 64\nobs_shape: [3, 64, 64]\ndecoder_cfg:\n  _target_: ml_networks.config.ResNetConfig\n  conv_channel: 64\n  conv_kernel: 3\n  f_kernel: 3\n  conv_activation: ReLU\n  output_activation: Tanh  # \u51fa\u529b\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\n  n_res_blocks: 3\n  scale_factor: 2  # PixelShuffle\u306e\u30b9\u30b1\u30fc\u30eb\u30d5\u30a1\u30af\u30bf\n  n_scaling: 3     # PixelShuffle\u306e\u6570\n  norm: batch\n  norm_cfg:\n    affine: true\n  dropout: 0.0\nfull_connection_cfg:\n  _target_: ml_networks.config.LinearConfig\n  activation: ReLU\n  bias: true\n</code></pre> <p>Python\u30b3\u30fc\u30c9:</p> <pre><code>from hydra.utils import instantiate\nfrom omegaconf import OmegaConf\n\ncfg = OmegaConf.load(\"configs/decoder_resnet.yaml\")\ndecoder = instantiate(cfg)\n</code></pre>"},{"location":"guides/decoder/#_4","title":"\u5168\u7d50\u5408\u5c64\u306e\u8a2d\u5b9a","text":"<p>Encoder\u3068\u540c\u69d8\u306b\u3001\u4ee5\u4e0b\u306e\u8a2d\u5b9a\u304c\u53ef\u80fd\u3067\u3059\uff1a</p> <ul> <li><code>LinearConfig</code>: \u5358\u4e00\u306e\u7dda\u5f62\u5c64</li> <li><code>MLPConfig</code>: \u591a\u5c64MLP</li> <li><code>None</code>: \u7279\u5fb4\u30de\u30c3\u30d7\u3092\u305d\u306e\u307e\u307e\u5165\u529b</li> </ul>"},{"location":"guides/decoder/#_5","title":"\u6ce8\u610f\u4e8b\u9805","text":"<ul> <li><code>output_padding</code>\u306f<code>ConvTranspose2d</code>\u306e\u307f\u306b\u5229\u7528\u3055\u308c\u307e\u3059</li> <li>\u6700\u5f8c\u306e\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\u306f\u51fa\u529b\u306b\u9069\u3057\u305f\u3082\u306e\u3092\u9078\u629e\u3057\u3066\u304f\u3060\u3055\u3044\uff08\u4f8b: \"Tanh\"\uff09</li> <li><code>feature_dim</code>\u306fbackbone\u306e\u5165\u529b\u7279\u5fb4\u30de\u30c3\u30d7\u6b21\u5143\u3068\u4e00\u81f4\u3055\u305b\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059</li> </ul>"},{"location":"guides/distributions/","title":"\u5206\u5e03\u30ac\u30a4\u30c9","text":"<p>\u5206\u5e03\u306e\u4f7f\u7528\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p> <p>\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9</p> <p>YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080\u65b9\u6cd5\u306b\u3064\u3044\u3066\u306f\u3001\u8a2d\u5b9a\u7ba1\u7406\u30ac\u30a4\u30c9\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"guides/distributions/#_2","title":"\u6982\u8981","text":"<p><code>ml-networks</code>\u306f\u3001\u7279\u5fb4\u91cf\u3092\u5206\u5e03\u306b\u5909\u63db\u3059\u308b\u6a5f\u80fd\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002\u6b63\u898f\u5206\u5e03\u3001\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5206\u5e03\u3001\u30d9\u30eb\u30cc\u30fc\u30a4\u5206\u5e03\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"guides/distributions/#_3","title":"\u6b63\u898f\u5206\u5e03","text":""},{"location":"guides/distributions/#_4","title":"\u57fa\u672c\u7684\u306a\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"guides/distributions/#1-yaml","title":"\u65b9\u6cd51: YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8aad\u307f\u8fbc\u3080\uff08\u63a8\u5968\uff09","text":"<p>\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb <code>configs/distribution_config.yaml</code> \u3092\u4f5c\u6210\u3057\u307e\u3059\uff1a</p> <pre><code>encoder:\n  _target_: ml_networks.vision.Encoder\n  feature_dim: 128  # \u6b63\u898f\u5206\u5e03\u306e\u5834\u5408\u3001\u7279\u5fb4\u91cf\u6b21\u5143\u306e2\u500d\u304c\u5fc5\u8981\n  obs_shape: [3, 64, 64]\n  encoder_cfg:\n    _target_: ml_networks.config.ConvNetConfig\n    channels: [16, 32, 64]\n    conv_cfgs:\n      - _target_: ml_networks.config.ConvConfig\n        kernel_size: 3\n        stride: 2\n        padding: 1\n        activation: ReLU\n      - _target_: ml_networks.config.ConvConfig\n        kernel_size: 3\n        stride: 2\n        padding: 1\n        activation: ReLU\n      - _target_: ml_networks.config.ConvConfig\n        kernel_size: 3\n        stride: 2\n        padding: 1\n        activation: ReLU\n  full_connection_cfg:\n    _target_: ml_networks.config.MLPConfig\n    hidden_dim: 128\n    n_layers: 2\n    output_activation: Identity\n    linear_cfg:\n      _target_: ml_networks.config.LinearConfig\n      activation: ReLU\n      bias: true\n\ndistribution:\n  _target_: ml_networks.distributions.Distribution\n  in_dim: 64\n  dist: normal\n  n_groups: 1\n  spherical: false\n</code></pre> <p>Python\u30b3\u30fc\u30c9\uff1a</p> <pre><code>from hydra.utils import instantiate\nfrom omegaconf import OmegaConf\nimport torch\nimport torch.distributions as D\n\n# \u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080\ncfg = OmegaConf.load(\"configs/distribution_config.yaml\")\n\n# \u30a8\u30f3\u30b3\u30fc\u30c0\u3068\u5206\u5e03\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\nencoder = instantiate(cfg.encoder)\ndist = instantiate(cfg.distribution)\n\n# \u4f7f\u7528\nobs = torch.randn(32, 3, 64, 64)\nz = encoder(obs)\n\n# \u81ea\u52d5\u7684\u306b\u5206\u5e03\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3078\u306e\u5909\u63db\u30fb\u518d\u30d1\u30e9\u30e1\u30fc\u30bf\u5316\u30c8\u30ea\u30c3\u30af\u304c\u9069\u7528\u3055\u308c\u308b\ndist_z = dist(z)\nprint(dist_z)\n# NormalStoch(mean: torch.Size([32, 64]), std: torch.Size([32, 64]), stoch: torch.Size([32, 64]))\n\n# torch.distributions.Distribution\u306b\u5909\u63db\ntorch_dist_z = dist_z.get_distribution(independent=1)\n\n# KLD\u306e\u8a08\u7b97\nnormal = D.Normal(0, 1)\nkld = D.kl_divergence(torch_dist_z, normal).mean()\n</code></pre>"},{"location":"guides/distributions/#2-python","title":"\u65b9\u6cd52: Python\u30b3\u30fc\u30c9\u3067\u76f4\u63a5\u8a2d\u5b9a\u3059\u308b","text":"<pre><code>from ml_networks import Distribution, Encoder, ConvNetConfig, ConvConfig, MLPConfig, LinearConfig\nimport torch\nimport torch.distributions as D\n\nfeature_dim = 64\nobs_shape = (3, 64, 64)\n\n# \u30ac\u30a6\u30b9\u5206\u5e03\u3092\u4f7f\u3046\u5834\u5408\u306f\u5e73\u5747\u3068\u6a19\u6e96\u504f\u5dee\u3067\u7279\u5fb4\u91cf\u6b21\u5143\u306e2\u500d\u306e\u6b21\u5143\u304c\u5fc5\u8981\nencoder_cfg = ConvNetConfig(\n    channels=[16, 32, 64],\n    conv_cfgs=[\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n    ]\n)\n\nfull_connection_cfg = MLPConfig(\n    hidden_dim=128,\n    n_layers=2,\n    output_activation=\"Identity\",  # \u5206\u5e03\u306b\u5909\u63db\u3059\u308b\u5834\u5408\u4f55\u3082\u304b\u3051\u306a\u3044\u306e\u304c\u3044\u3044\n    linear_cfg=LinearConfig(\n        activation=\"ReLU\",\n        bias=True,\n    )\n)\n\nencoder = Encoder(feature_dim * 2, obs_shape, encoder_cfg, full_connection_cfg)\n\ndist = Distribution(\n    in_dim=feature_dim,  # \u5206\u5e03\u306e\u6b21\u5143\uff08\u5e73\u5747\u30fb\u6a19\u6e96\u504f\u5dee\u306e\u6b21\u5143\uff09\n    dist=\"normal\",        # \u5206\u5e03\u306e\u7a2e\u985e\n    n_groups=1,          # \u5206\u5e03\u306e\u30b0\u30eb\u30fc\u30d7\u6570\uff08\u30ac\u30a6\u30b9\u5206\u5e03\u306e\u5834\u5408\u306f\u610f\u5473\u306a\u3044\uff09\n)\n\nobs = torch.randn(32, 3, 64, 64)\nz = encoder(obs)\n\n# \u81ea\u52d5\u7684\u306b\u5206\u5e03\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3078\u306e\u5909\u63db\u30fb\u518d\u30d1\u30e9\u30e1\u30fc\u30bf\u5316\u30c8\u30ea\u30c3\u30af\u304c\u9069\u7528\u3055\u308c\u308b\ndist_z = dist(z)\nprint(dist_z)\n# NormalStoch(mean: torch.Size([32, 64]), std: torch.Size([32, 64]), stoch: torch.Size([32, 64]))\n\n# torch.distributions.Distribution\u306b\u5909\u63db\ntorch_dist_z = dist_z.get_distribution(independent=1)\n\n# KLD\u306e\u8a08\u7b97\nnormal = D.Normal(0, 1)\nkld = D.kl_divergence(torch_dist_z, normal).mean()\n</code></pre>"},{"location":"guides/distributions/#_5","title":"\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u5206\u5e03","text":"<pre><code>encoder = Encoder(feature_dim, obs_shape, encoder_cfg, full_connection_cfg)\n\ndist = Distribution(\n    in_dim=feature_dim,\n    dist=\"categorical\",\n    n_groups=8,  # feature_dim\u304cn_groups\u306e\u500d\u6570\u3067\u306a\u3044\u3068\u30a8\u30e9\u30fc\u304c\u51fa\u308b\n)\n\nz = encoder(obs)\ndist_z = dist(z)\nprint(dist_z)\n# CategoricalStoch(logits: torch.Size([32, 8, 8]), probs: torch.Size([32, 8, 8]), stoch: torch.Size([32, 8, 8]))\n\nflat_dist = D.OneHotCategorical(probs=torch.ones_like(dist_z.probs)/dist_z.probs.shape[-1])\nkld = D.kl_divergence(dist_z.get_distribution(), flat_dist).mean()\n</code></pre>"},{"location":"guides/distributions/#_6","title":"\u30d9\u30eb\u30cc\u30fc\u30a4\u5206\u5e03","text":"<pre><code>dist = Distribution(\n    in_dim=feature_dim,\n    dist=\"bernoulli\",\n    n_groups=2,      # \u8d85\u7403\u306e\u6570\n    spherical=False,  # \u8d85\u7403\u306b\u3059\u308b\u304b\u3069\u3046\u304b\n)\n</code></pre>"},{"location":"guides/distributions/#_7","title":"\u5206\u5e03\u30c7\u30fc\u30bf\u306e\u64cd\u4f5c","text":""},{"location":"guides/distributions/#stack","title":"stack","text":"<pre><code>from ml_networks import stack_dist\n\ndist_list = []\nfor batch in dataloader:\n    obs = batch[\"obs\"]\n    z = encoder(obs)\n    dist_z = dist(z)\n    dist_list.append(dist_z)\n\n# \u5206\u5e03\u30c7\u30fc\u30bf\u3092stack\nstacked_dist = stack_dist(dist_list, dim=0)\nprint(stacked_dist.shape)\n# NormalShape(mean: torch.Size([100, 32, 64]), std: torch.Size([100, 32, 64]), stoch: torch.Size([100, 32, 64]))\n</code></pre>"},{"location":"guides/distributions/#concatenate","title":"concatenate","text":"<pre><code>from ml_networks import cat_dist\n\n# \u5206\u5e03\u30c7\u30fc\u30bf\u3092concatenate\nconcatenated_dist = cat_dist(dist_list, dim=-1)\nprint(concatenated_dist.shape)\n# NormalShape(mean: torch.Size([32, 6400]), std: torch.Size([32, 6400]), stoch: torch.Size([32, 6400]))\n</code></pre>"},{"location":"guides/distributions/#_8","title":"\u5206\u5e03\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58","text":"<pre><code>dist = Distribution(\n    in_dim=feature_dim,\n    dist=\"normal\",\n    n_groups=1,\n)\n\nz = encoder(obs)\ndist_z = dist(z)\n\n# \u5206\u5e03\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\ndist_z.save(\"reports\")\n# reports\u306e\u4e0b\u306bmean.blosc2, std.blosc2, stoch.blosc2\u304c\u4fdd\u5b58\u3055\u308c\u308b\n# \u4ed6\u306e\u5206\u5e03\u30c7\u30fc\u30bf\u3082\u540c\u69d8\u306b\u4fdd\u5b58\u3055\u308c\u308b\n</code></pre>"},{"location":"guides/encoder/","title":"Encoder\u30ac\u30a4\u30c9","text":"<p>Encoder\u306e\u4f7f\u7528\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p> <p>\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9</p> <p>YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080\u65b9\u6cd5\u306b\u3064\u3044\u3066\u306f\u3001\u8a2d\u5b9a\u7ba1\u7406\u30ac\u30a4\u30c9\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"guides/encoder/#_1","title":"\u6982\u8981","text":"<p>Encoder\u306f\u753b\u50cf\u306a\u3069\u306e\u5165\u529b\u3092\u7279\u5fb4\u91cf\u306b\u5909\u63db\u3059\u308b\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3059\u3002\u69d8\u3005\u306a\u30d0\u30c3\u30af\u30dc\u30fc\u30f3\uff08ConvNet\u3001ResNet\u3001ViT\u306a\u3069\uff09\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059\u3002</p>"},{"location":"guides/encoder/#_2","title":"\u57fa\u672c\u7684\u306a\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"guides/encoder/#1-yaml","title":"\u65b9\u6cd51: YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8aad\u307f\u8fbc\u3080\uff08\u63a8\u5968\uff09","text":"<p>\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb <code>configs/encoder_config.yaml</code> \u3092\u4f5c\u6210\u3057\u307e\u3059\uff1a</p> <pre><code>_target_: ml_networks.vision.Encoder\nfeature_dim: 64\nobs_shape: [3, 64, 64]\nencoder_cfg:\n  _target_: ml_networks.config.ConvNetConfig\n  channels: [16, 32, 64]\n  conv_cfgs:\n    - _target_: ml_networks.config.ConvConfig\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: ReLU\n    - _target_: ml_networks.config.ConvConfig\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: ReLU\n    - _target_: ml_networks.config.ConvConfig\n      kernel_size: 3\n      stride: 2\n      padding: 1\n      activation: ReLU\nfull_connection_cfg:\n  _target_: ml_networks.config.LinearConfig\n  activation: ReLU\n  bias: true\n</code></pre> <p>Python\u30b3\u30fc\u30c9\uff1a</p> <pre><code>from hydra.utils import instantiate\nfrom omegaconf import OmegaConf\nimport torch\n\n# YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080\ncfg = OmegaConf.load(\"configs/encoder_config.yaml\")\n\n# instantiate\u3092\u4f7f\u7528\u3057\u3066\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\nencoder = instantiate(cfg)\n\n# \u63a8\u8ad6\nobs = torch.randn(32, 3, 64, 64)\nz = encoder(obs)\nprint(z.shape)  # torch.Size([32, 64])\n</code></pre>"},{"location":"guides/encoder/#2-python","title":"\u65b9\u6cd52: Python\u30b3\u30fc\u30c9\u3067\u76f4\u63a5\u8a2d\u5b9a\u3059\u308b","text":"<pre><code>from ml_networks import Encoder, ConvNetConfig, ConvConfig, LinearConfig\nimport torch\n\n# Encoder\u306e\u8a2d\u5b9a\nencoder_cfg = ConvNetConfig(\n    channels=[16, 32, 64],  # \u5404\u5c64\u306echannel\u6570\n    conv_cfgs=[\n        ConvConfig(\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            activation=\"ReLU\",\n        ),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n    ]\n)\n\n# \u5168\u7d50\u5408\u5c64\u306e\u8a2d\u5b9a\nfull_connection_cfg = LinearConfig(\n    activation=\"ReLU\",\n    bias=True,\n)\n\n# Encoder\u306e\u4f5c\u6210\nobs_shape = (3, 64, 64)  # (C, H, W)\nfeature_dim = 64\nencoder = Encoder(feature_dim, obs_shape, encoder_cfg, full_connection_cfg)\n\n# \u63a8\u8ad6\nobs = torch.randn(32, 3, 64, 64)\nz = encoder(obs)\nprint(z.shape)  # torch.Size([32, 64])\n</code></pre>"},{"location":"guides/encoder/#_3","title":"\u30d0\u30c3\u30af\u30dc\u30fc\u30f3\u306e\u7a2e\u985e","text":""},{"location":"guides/encoder/#convnet","title":"ConvNet","text":"<p>\u591a\u5c64CNN\u306e\u30a8\u30f3\u30b3\u30fc\u30c0\uff1a</p> <pre><code>encoder_cfg = ConvNetConfig(\n    channels=[16, 32, 64],\n    conv_cfgs=[\n        ConvConfig(\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            dilation=1,\n            activation=\"ReLU\",\n            groups=1,\n            bias=True,\n            norm=\"none\",\n            norm_cfg={},\n            dropout=0.0,\n            scale_factor=0,\n        ),\n        # ... \u4ed6\u306e\u5c64\n    ]\n)\n</code></pre>"},{"location":"guides/encoder/#resnet-pixelunshuffle","title":"ResNet + PixelUnShuffle","text":"<p>ResNet\u3068PixelUnShuffle\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u30a8\u30f3\u30b3\u30fc\u30c0\uff1a</p> <p>YAML\u30d5\u30a1\u30a4\u30eb (<code>configs/encoder_resnet.yaml</code>):</p> <pre><code>_target_: ml_networks.vision.Encoder\nfeature_dim: 64\nobs_shape: [3, 64, 64]\nencoder_cfg:\n  _target_: ml_networks.config.ResNetConfig\n  conv_channel: 64\n  conv_kernel: 3\n  f_kernel: 3\n  conv_activation: ReLU\n  out_activation: ReLU\n  n_res_blocks: 3\n  scale_factor: 2  # PixelUnShuffle\u306e\u30b9\u30b1\u30fc\u30eb\u30d5\u30a1\u30af\u30bf\n  n_scaling: 3     # PixelUnShuffle\u306e\u6570\n  norm: batch\n  norm_cfg:\n    affine: true\n  dropout: 0.0\nfull_connection_cfg:\n  _target_: ml_networks.config.LinearConfig\n  activation: ReLU\n  bias: true\n</code></pre> <p>Python\u30b3\u30fc\u30c9:</p> <pre><code>from hydra.utils import instantiate\nfrom omegaconf import OmegaConf\n\ncfg = OmegaConf.load(\"configs/encoder_resnet.yaml\")\nencoder = instantiate(cfg)\n</code></pre>"},{"location":"guides/encoder/#vit","title":"ViT\uff08\u975e\u63a8\u5968\uff09","text":"<p>Vision Transformer\u3082\u4f7f\u7528\u53ef\u80fd\u3067\u3059\u304c\u3001\u5b9f\u88c5\u304c\u4e0d\u5b89\u5b9a\u306a\u305f\u3081\u975e\u63a8\u5968\u3067\u3059\u3002</p>"},{"location":"guides/encoder/#_4","title":"\u5168\u7d50\u5408\u5c64\u306e\u8a2d\u5b9a","text":""},{"location":"guides/encoder/#_5","title":"\u5358\u4e00\u306e\u7dda\u5f62\u5c64","text":"<pre><code>full_connection_cfg = LinearConfig(\n    activation=\"ReLU\",\n    bias=True,\n)\n</code></pre>"},{"location":"guides/encoder/#mlp","title":"\u591a\u5c64MLP","text":"<pre><code>from ml_networks import MLPConfig\n\nfull_connection_cfg = MLPConfig(\n    hidden_dim=128,\n    n_layers=2,\n    output_activation=\"Tanh\",\n    linear_cfg=LinearConfig(\n        activation=\"ReLU\",\n        bias=True,\n    )\n)\n</code></pre>"},{"location":"guides/encoder/#spatialsoftmax","title":"SpatialSoftmax","text":"<pre><code>from ml_networks import SpatialSoftmaxConfig\n\nfull_connection_cfg = SpatialSoftmaxConfig(\n    temperature=1.0,\n    eps=1e-6,\n    is_argmax=False,\n)\n</code></pre>"},{"location":"guides/encoder/#adaptiveaveragepooling","title":"AdaptiveAveragePooling","text":"<pre><code>from ml_networks import AdaptiveAveragePoolingConfig\n\nfull_connection_cfg = AdaptiveAveragePoolingConfig()\n</code></pre>"},{"location":"guides/encoder/#_6","title":"\u7279\u5fb4\u30de\u30c3\u30d7\u3092\u305d\u306e\u307e\u307e\u51fa\u529b","text":"<pre><code>full_connection_cfg = None\n# feature_dim\u306fbackbone\u306e\u51fa\u529b\u7279\u5fb4\u30de\u30c3\u30d7\u6b21\u5143\u3068\u4e00\u81f4\u3055\u305b\u308b\u5fc5\u8981\u304c\u3042\u308b\n</code></pre>"},{"location":"guides/encoder/#spatialsoftmax_1","title":"\u72ec\u7acb\u3057\u305fSpatialSoftmax\u306e\u4f7f\u7528","text":"<pre><code>from ml_networks import SpatialSoftmaxConfig, SpatialSoftmax\n\ndata = torch.randn(32, 3, 64, 64)\ncfg = SpatialSoftmaxConfig(\n    temperature=1.0,\n    eps=1e-6,\n    is_argmax=False,\n)\nspatial_softmax = SpatialSoftmax(cfg)\nz = spatial_softmax(data)\n</code></pre>"},{"location":"guides/jax/","title":"JAX\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u30ac\u30a4\u30c9","text":"<p>JAX\uff08Flax NNX\uff09\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u306e\u4f7f\u7528\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"guides/jax/#_1","title":"\u6982\u8981","text":"<p><code>ml-networks</code>\u306fPyTorch\u306b\u52a0\u3048\u3066\u3001JAX\uff08Flax NNX\uff09\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3092\u63d0\u4f9b\u3057\u3066\u3044\u307e\u3059\u3002PyTorch\u3068\u540c\u4e00\u306e<code>Config</code>\u4f53\u7cfb\u3092\u4f7f\u7528\u3059\u308b\u305f\u3081\u3001\u8a2d\u5b9a\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u306a\u304f\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u5207\u308a\u66ff\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002</p> <p>JAX\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u306e\u7279\u5fb4</p> <ul> <li>PyTorch\u3068\u540c\u4e00\u306e<code>Config</code>\u30af\u30e9\u30b9\u3092\u4f7f\u7528</li> <li>Flax NNX\uff08<code>flax.nnx</code>\uff09\u30d9\u30fc\u30b9\u306e\u5b9f\u88c5</li> <li>optax\u306b\u3088\u308b\u6700\u9069\u5316</li> <li>distrax\u306b\u3088\u308b\u78ba\u7387\u5206\u5e03</li> </ul>"},{"location":"guides/jax/#_2","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<p>JAX\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3092\u4f7f\u7528\u3059\u308b\u306b\u306f\u3001\u8ffd\u52a0\u306e\u4f9d\u5b58\u95a2\u4fc2\u304c\u5fc5\u8981\u3067\u3059\uff1a</p> <pre><code>pip install jax flax optax distrax\n</code></pre>"},{"location":"guides/jax/#_3","title":"\u8981\u4ef6","text":"<ul> <li>JAX &gt;= 0.4.30</li> <li>Flax &gt;= 0.12.0\uff08NNX\u30e2\u30b8\u30e5\u30fc\u30eb\uff09</li> <li>optax &gt;= 0.2.0</li> <li>distrax &gt;= 0.1.5</li> </ul>"},{"location":"guides/jax/#_4","title":"\u30a4\u30f3\u30dd\u30fc\u30c8","text":"<p>PyTorch\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3068JAX\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u306f\u3001\u305d\u308c\u305e\u308c\u5225\u306e\u30b5\u30d6\u30e2\u30b8\u30e5\u30fc\u30eb\u304b\u3089\u30a4\u30f3\u30dd\u30fc\u30c8\u3057\u307e\u3059\uff1a</p> PyTorchJAX <pre><code>from ml_networks.torch import (\n    MLPLayer, Encoder, Decoder,\n    Distribution, ConditionalUnet2d,\n    focal_loss, charbonnier,\n    get_optimizer, torch_fix_seed,\n)\n</code></pre> <pre><code>from ml_networks.jax import (\n    MLPLayer, Encoder, Decoder,\n    Distribution, ConditionalUnet2d,\n    focal_loss, charbonnier,\n    get_optimizer, jax_fix_seed,\n)\n</code></pre> <p>\u8a2d\u5b9a\u30af\u30e9\u30b9\u306f\u5171\u901a\u3067\u3059\uff1a</p> <pre><code>from ml_networks import (\n    MLPConfig, LinearConfig, ConvConfig, ConvNetConfig,\n    ResNetConfig, UNetConfig, ViTConfig,\n)\n</code></pre>"},{"location":"guides/jax/#_5","title":"\u57fa\u672c\u7684\u306a\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"guides/jax/#mlp","title":"MLP","text":"<pre><code>from ml_networks.jax import MLPLayer\nfrom ml_networks import MLPConfig, LinearConfig\nimport jax\nimport jax.numpy as jnp\n\n# \u8a2d\u5b9a\uff08PyTorch\u3068\u540c\u3058\uff09\nmlp_config = MLPConfig(\n    hidden_dim=128,\n    n_layers=2,\n    output_activation=\"Tanh\",\n    linear_cfg=LinearConfig(activation=\"ReLU\", bias=True)\n)\n\n# MLP\u306e\u4f5c\u6210\uff08rngs\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u5fc5\u8981\uff09\nrngs = jax.random.PRNGKey(0)\nmlp = MLPLayer(input_dim=16, output_dim=8, mlp_config=mlp_config, rngs=rngs)\n\n# \u63a8\u8ad6\nx = jnp.ones((32, 16))\ny = mlp(x)\nprint(y.shape)  # (32, 8)\n</code></pre> <p>PyTorch\u3068\u306e\u9055\u3044: <code>rngs</code>\u30d1\u30e9\u30e1\u30fc\u30bf</p> <p>JAX\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3067\u306f\u3001\u30e2\u30c7\u30eb\u306e\u521d\u671f\u5316\u6642\u306b\u4e71\u6570\u30ad\u30fc\uff08<code>rngs</code>\uff09\u3092\u6e21\u3059\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u308c\u306fJAX\u306e\u95a2\u6570\u578b\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u30e2\u30c7\u30eb\u306b\u57fa\u3065\u304f\u3082\u306e\u3067\u3001\u518d\u73fe\u6027\u3092\u4fdd\u8a3c\u3057\u307e\u3059\u3002</p>"},{"location":"guides/jax/#encoder","title":"Encoder","text":"<pre><code>from ml_networks.jax import Encoder\nfrom ml_networks import ConvNetConfig, ConvConfig, LinearConfig\nimport jax\nimport jax.numpy as jnp\n\n# \u30d0\u30c3\u30af\u30dc\u30fc\u30f3\u306e\u8a2d\u5b9a\nencoder_cfg = ConvNetConfig(\n    channels=[16, 32, 64],\n    conv_cfgs=[\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n    ]\n)\n\n# \u5168\u7d50\u5408\u5c64\u306e\u8a2d\u5b9a\nfull_connection_cfg = LinearConfig(activation=\"ReLU\", bias=True)\n\n# Encoder\u306e\u4f5c\u6210\nobs_shape = (3, 64, 64)\nfeature_dim = 64\nrngs = jax.random.PRNGKey(0)\nencoder = Encoder(feature_dim, obs_shape, encoder_cfg, full_connection_cfg, rngs=rngs)\n\n# \u63a8\u8ad6\nobs = jnp.ones((32, 3, 64, 64))\nz = encoder(obs)\nprint(z.shape)  # (32, 64)\n</code></pre>"},{"location":"guides/jax/#decoder","title":"Decoder","text":"<pre><code>from ml_networks.jax import Decoder\nfrom ml_networks import ConvNetConfig, ConvConfig, LinearConfig\nimport jax\nimport jax.numpy as jnp\n\n# \u30c7\u30b3\u30fc\u30c0\u306e\u8a2d\u5b9a\ndecoder_cfg = ConvNetConfig(\n    channels=[64, 32, 16],\n    conv_cfgs=[\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"Tanh\"),\n    ]\n)\n\nfull_connection_cfg = LinearConfig(activation=\"ReLU\", bias=True)\n\nobs_shape = (3, 64, 64)\nfeature_dim = 64\nrngs = jax.random.PRNGKey(0)\ndecoder = Decoder(feature_dim, obs_shape, decoder_cfg, full_connection_cfg, rngs=rngs)\n\n# \u63a8\u8ad6\nz = jnp.ones((32, feature_dim))\npredicted_obs = decoder(z)\nprint(predicted_obs.shape)  # (32, 3, 64, 64)\n</code></pre>"},{"location":"guides/jax/#_6","title":"\u5206\u5e03","text":"<pre><code>from ml_networks.jax import Distribution, Encoder\nfrom ml_networks import ConvNetConfig, ConvConfig, MLPConfig, LinearConfig\nimport jax\nimport jax.numpy as jnp\n\nfeature_dim = 64\nobs_shape = (3, 64, 64)\n\nencoder_cfg = ConvNetConfig(\n    channels=[16, 32, 64],\n    conv_cfgs=[\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n        ConvConfig(kernel_size=3, stride=2, padding=1, activation=\"ReLU\"),\n    ]\n)\n\nfull_connection_cfg = MLPConfig(\n    hidden_dim=128,\n    n_layers=2,\n    output_activation=\"Identity\",\n    linear_cfg=LinearConfig(activation=\"ReLU\", bias=True)\n)\n\nrngs = jax.random.PRNGKey(0)\n\n# \u6b63\u898f\u5206\u5e03\u306e\u5834\u5408\u3001\u5e73\u5747\u3068\u6a19\u6e96\u504f\u5dee\u3067\u7279\u5fb4\u91cf\u6b21\u5143\u306e2\u500d\u304c\u5fc5\u8981\nencoder = Encoder(feature_dim * 2, obs_shape, encoder_cfg, full_connection_cfg, rngs=rngs)\n\ndist = Distribution(\n    in_dim=feature_dim,\n    dist=\"normal\",\n    n_groups=1,\n    rngs=rngs,\n)\n\nobs = jnp.ones((32, 3, 64, 64))\nz = encoder(obs)\ndist_z = dist(z)\n</code></pre>"},{"location":"guides/jax/#_7","title":"\u6700\u9069\u5316","text":"<p>JAX\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u3067\u306f\u3001optax\u30d9\u30fc\u30b9\u306e\u6700\u9069\u5316\u3092\u4f7f\u7528\u3057\u307e\u3059\uff1a</p> <pre><code>from ml_networks.jax import get_optimizer\n\n# PyTorch\u30b9\u30bf\u30a4\u30eb\u306e\u540d\u524d\u3067optax\u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u3092\u53d6\u5f97\noptimizer = get_optimizer(\"Adam\", learning_rate=1e-3)\n\n# PyTorch\u306e\u540d\u524d\u304c\u81ea\u52d5\u7684\u306boptax\u306e\u540d\u524d\u306b\u30de\u30c3\u30d4\u30f3\u30b0\u3055\u308c\u308b\n# \"Adam\" -&gt; \"adam\", \"SGD\" -&gt; \"sgd\", \"AdamW\" -&gt; \"adamw\" \u306a\u3069\n</code></pre>"},{"location":"guides/jax/#seed","title":"Seed\u56fa\u5b9a","text":"<pre><code>from ml_networks.jax import jax_fix_seed\n\n# JAX, numpy, random\u306eseed\u3092\u56fa\u5b9a\njax_fix_seed(42)\n</code></pre>"},{"location":"guides/jax/#_8","title":"\u640d\u5931\u95a2\u6570","text":"<pre><code>from ml_networks.jax import focal_loss, binary_focal_loss, charbonnier\nimport jax.numpy as jnp\n\n# Focal Loss\nlogits = jnp.ones((32, 10))\nlabels = jnp.zeros((32,), dtype=jnp.int32)\nloss = focal_loss(logits, labels, gamma=2.0)\n\n# Charbonnier Loss\npredicted = jnp.ones((32, 3, 64, 64))\ntarget = jnp.zeros((32, 3, 64, 64))\nloss = charbonnier(predicted, target)\n</code></pre>"},{"location":"guides/jax/#pytorchjax","title":"PyTorch\u3068JAX\u306e\u5bfe\u5fdc\u8868","text":"\u6a5f\u80fd PyTorch (<code>ml_networks.torch</code>) JAX (<code>ml_networks.jax</code>) MLP <code>MLPLayer</code> <code>MLPLayer</code> Encoder <code>Encoder</code> <code>Encoder</code> Decoder <code>Decoder</code> <code>Decoder</code> ConvNet <code>ConvNet</code> <code>ConvNet</code> ResNet <code>ResNetPixShuffle</code> / <code>ResNetPixUnshuffle</code> <code>ResNetPixShuffle</code> / <code>ResNetPixUnshuffle</code> ViT <code>ViT</code> <code>ViT</code> UNet <code>ConditionalUnet1d</code> / <code>ConditionalUnet2d</code> <code>ConditionalUnet1d</code> / <code>ConditionalUnet2d</code> \u5206\u5e03 <code>Distribution</code> <code>Distribution</code> \u6d3b\u6027\u5316\u95a2\u6570 <code>Activation</code> <code>Activation</code> \u640d\u5931\u95a2\u6570 <code>focal_loss</code>, <code>charbonnier</code> \u306a\u3069 <code>focal_loss</code>, <code>charbonnier</code> \u306a\u3069 \u6700\u9069\u5316 <code>get_optimizer</code> (PyTorch/pytorch_optimizer) <code>get_optimizer</code> (optax) Seed\u56fa\u5b9a <code>torch_fix_seed</code> <code>jax_fix_seed</code> HyperNet <code>HyperNet</code> <code>HyperNet</code> \u5bfe\u7167\u5b66\u7fd2 <code>ContrastiveLearningLoss</code> <code>ContrastiveLearningLoss</code>"},{"location":"guides/jax/#_9","title":"\u6ce8\u610f\u4e8b\u9805","text":"<ul> <li>JAX\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u306f\u6bd4\u8f03\u7684\u65b0\u3057\u3044\u5b9f\u88c5\u3067\u3042\u308a\u3001PyTorch\u30d0\u30c3\u30af\u30a8\u30f3\u30c9\u306b\u6bd4\u3079\u3066\u30c6\u30b9\u30c8\u304c\u9650\u3089\u308c\u3066\u3044\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059</li> <li><code>rngs</code>\u30d1\u30e9\u30e1\u30fc\u30bf\u306fFlax NNX\u306e\u4ed5\u69d8\u3067\u3042\u308a\u3001\u30e2\u30c7\u30eb\u306e\u521d\u671f\u5316\u6642\u306b\u5fc5\u305a\u6e21\u3059\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059</li> <li>JAX\u306e\u5236\u7d04\u306b\u3088\u308a\u3001\u52d5\u7684\u306a\u5f62\u72b6\u5909\u66f4\uff08\u4f8b: <code>torch.view</code>\u306e\u4e00\u90e8\u7528\u6cd5\uff09\u306f\u5236\u9650\u3055\u308c\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059</li> <li>GPU\u306e\u4f7f\u7528\u306b\u306fJAX\u306eGPU\u7248\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff08<code>jax[cuda12]</code>\uff09</li> </ul>"},{"location":"guides/loss-functions/","title":"\u640d\u5931\u95a2\u6570\u30ac\u30a4\u30c9","text":"<p>\u640d\u5931\u95a2\u6570\u306e\u4f7f\u7528\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"guides/loss-functions/#focal-loss","title":"Focal Loss","text":"<p>\u5206\u985e\u306e\u5b66\u7fd2\u306b\u9069\u3057\u305f\u640d\u5931\u95a2\u6570\u3067\u3059\u3002</p>"},{"location":"guides/loss-functions/#_2","title":"\u591a\u30af\u30e9\u30b9\u5206\u985e\u306e\u5834\u5408","text":"<pre><code>from ml_networks import focal_loss\nimport torch\n\nlogits = torch.randn(32, 10)\nlabels = torch.randint(0, 10, (32,))\nloss = focal_loss(\n    logits,\n    labels,\n    gamma=2.0,   # Focal Loss\u306e\u91cd\u307f\u306e\u8abf\u6574\n    sum_dim=-1   # \u3069\u306e\u6b21\u5143\u3067sum\u3059\u308b\u304b\n)\n</code></pre>"},{"location":"guides/loss-functions/#_3","title":"\u4e8c\u5024\u5206\u985e\u306e\u5834\u5408","text":"<pre><code>from ml_networks import binary_focal_loss\n\nlogits = torch.randn(32)\nlabels = torch.randint(0, 2, (32,))\nloss = binary_focal_loss(\n    logits,\n    labels,\n    gamma=2.0,\n    sum_dim=-1\n)\n</code></pre>"},{"location":"guides/loss-functions/#charbonnier-loss","title":"Charbonnier Loss","text":"<p>\u753b\u50cf\u518d\u69cb\u6210\u306e\u640d\u5931\u95a2\u6570\u3067\u3059\u3002\u640d\u5931\u306e\u52fe\u914d\u304c\u5b89\u5b9a\u3057\u307e\u3059\u3002</p> <pre><code>from ml_networks import charbonnier\n\nloss = charbonnier(\n    predicted_obs,\n    obs,\n    epsilon=1e-3,  # charbonnier loss\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\n    alpha=1,       # charbonnier loss\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\n    sum_dim=[-1, -2, -3]  # \u3069\u306e\u6b21\u5143\u3067sum\u3059\u308b\u304b\n)\n</code></pre>"},{"location":"guides/loss-functions/#focal-frequency-loss","title":"Focal Frequency Loss","text":"<p>\u753b\u50cf\u81ea\u4f53\u3067\u306a\u304f\u3001\u753b\u50cf\u306e\u5468\u6ce2\u6570\u6210\u5206\u306b\u7126\u70b9\u3092\u5f53\u3066\u305f\u640d\u5931\u95a2\u6570\u3067\u3059\u3002Focal Loss\u3092\u753b\u50cf\u306b\u9069\u7528\u3057\u305f\u3082\u306e\u3068\u3044\u3046\u4f4d\u7f6e\u4ed8\u3051\u3067\u3059\u3002</p> <pre><code>from ml_networks import FocalFrequencyLoss\n\nloss_fn = FocalFrequencyLoss(\n    loss_weight=1.0,      # Focal Frequency Loss\u306e\u91cd\u307f\n    alpha=1.0,            # spectrum weight\u306escaling factor\n    patch_factor=1,       # \u30d1\u30c3\u30c1\u30d9\u30fc\u30b9\u306efocal frequency loss\u7528\u306e\u30af\u30ed\u30c3\u30d7\u30d5\u30a1\u30af\u30bf\n    ave_spectrum=False,   # \u30df\u30cb\u30d0\u30c3\u30c1\u5e73\u5747\u30b9\u30da\u30af\u30c8\u30e9\u30e0\u3092\u4f7f\u3046\u304b\u3069\u3046\u304b\n    log_matrix=False,     # \u30b9\u30da\u30af\u30c8\u30e9\u30e0\u91cd\u307f\u884c\u5217\u3092\u5bfe\u6570\u3067\u8abf\u6574\u3059\u308b\u304b\u3069\u3046\u304b\n    batch_matrix=False    # \u30d0\u30c3\u30c1\u30d9\u30fc\u30b9\u306e\u7d71\u8a08\u3067\u30b9\u30da\u30af\u30c8\u30e9\u30e0\u91cd\u307f\u884c\u5217\u3092\u8a08\u7b97\u3059\u308b\u304b\u3069\u3046\u304b\n)\n\nloss = loss_fn(predicted_obs, obs)\n</code></pre>"},{"location":"guides/loss-functions/#kl-divergence","title":"KL Divergence","text":"<p>\u5206\u5e03\u9593\u306eKL\u30c0\u30a4\u30d0\u30fc\u30b8\u30a7\u30f3\u30b9\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002</p> <pre><code>from ml_networks import kl_divergence\nimport torch.distributions as D\n\n# \u4f8b: \u6b63\u898f\u5206\u5e03\u9593\u306eKL\u30c0\u30a4\u30d0\u30fc\u30b8\u30a7\u30f3\u30b9\ndist1 = D.Normal(0, 1)\ndist2 = D.Normal(1, 2)\nkld = kl_divergence(dist1, dist2)\n</code></pre>"},{"location":"guides/loss-functions/#kl-balancing","title":"KL Balancing","text":"<p>\u8907\u6570\u306eKL\u30c0\u30a4\u30d0\u30fc\u30b8\u30a7\u30f3\u30b9\u3092\u30d0\u30e9\u30f3\u30b9\u3059\u308b\u305f\u3081\u306e\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\u3067\u3059\u3002</p> <pre><code>from ml_networks import kl_balancing\n\n# \u8907\u6570\u306eKL\u30c0\u30a4\u30d0\u30fc\u30b8\u30a7\u30f3\u30b9\u3092\u30d0\u30e9\u30f3\u30b9\nkld_list = [kld1, kld2, kld3]\nbalanced_kld = kl_balancing(kld_list, alpha=0.5)\n</code></pre>"},{"location":"guides/mlp/","title":"MLP\u30ac\u30a4\u30c9","text":"<p>MLP\uff08\u591a\u5c64\u30d1\u30fc\u30bb\u30d7\u30c8\u30ed\u30f3\uff09\u306e\u4f7f\u7528\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p> <p>\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9</p> <p>YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080\u65b9\u6cd5\u306b\u3064\u3044\u3066\u306f\u3001\u8a2d\u5b9a\u7ba1\u7406\u30ac\u30a4\u30c9\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"guides/mlp/#_1","title":"\u57fa\u672c\u7684\u306a\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"guides/mlp/#1-yaml","title":"\u65b9\u6cd51: YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8aad\u307f\u8fbc\u3080\uff08\u63a8\u5968\uff09","text":"<p>\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb <code>configs/mlp_config.yaml</code> \u3092\u4f5c\u6210\u3057\u307e\u3059\uff1a</p> <pre><code>_target_: ml_networks.layers.MLPLayer\ninput_dim: 16\noutput_dim: 8\nmlp_config:\n  _target_: ml_networks.config.MLPConfig\n  hidden_dim: 128\n  n_layers: 2\n  output_activation: Tanh\n  linear_cfg:\n    _target_: ml_networks.config.LinearConfig\n    activation: ReLU\n    bias: true\n    norm: none\n    norm_cfg: {}\n    norm_first: false\n    dropout: 0.0\n</code></pre> <p>Python\u30b3\u30fc\u30c9\uff1a</p> <pre><code>from hydra.utils import instantiate\nfrom omegaconf import OmegaConf\nimport torch\n\n# YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080\ncfg = OmegaConf.load(\"configs/mlp_config.yaml\")\n\n# instantiate\u3092\u4f7f\u7528\u3057\u3066\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\nmlp = instantiate(cfg)\n\n# \u63a8\u8ad6\nx = torch.randn(32, 16)\ny = mlp(x)\nprint(y.shape)  # torch.Size([32, 8])\n</code></pre>"},{"location":"guides/mlp/#2-python","title":"\u65b9\u6cd52: Python\u30b3\u30fc\u30c9\u3067\u76f4\u63a5\u8a2d\u5b9a\u3059\u308b","text":"<pre><code>from ml_networks import MLPLayer, MLPConfig, LinearConfig\nimport torch\n\nmlp_config = MLPConfig(\n    hidden_dim=128,  # \u96a0\u308c\u5c64\u306e\u6b21\u5143\n    n_layers=2,      # \u96a0\u308c\u5c64\u306e\u6570\n    output_activation=\"Tanh\",  # \u51fa\u529b\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570\n    linear_cfg=LinearConfig(\n        activation=\"ReLU\",  # \u6d3b\u6027\u5316\u95a2\u6570\n        bias=True,         # \u30d0\u30a4\u30a2\u30b9\u3092\u4f7f\u3046\u304b\u3069\u3046\u304b\n        norm=\"none\",       # \u6b63\u898f\u5316\u3092\u884c\u3046\u304b\u3069\u3046\u304b\n                         # \"none\"\u3067\u6b63\u898f\u5316\u306a\u3057\n                         # \"layer\"\u3067LayerNorm\n                         # \"rms\"\u3067RMSNorm\u304c\u4f7f\u3048\u308b\n        norm_cfg={},      # \u6b63\u898f\u5316\u306e\u8a2d\u5b9a\n        norm_first=False, # \u6b63\u898f\u5316\u3092nn.Linear\u306e\u524d\u306b\u884c\u3046\u304b\u3069\u3046\u304b\n        dropout=0.0,      # \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387\n    )\n)\n\ninput_dim = 16\noutput_dim = 8\n\nmlp = MLPLayer(input_dim, output_dim, mlp_config)\n\nx = torch.randn(32, input_dim)\ny = mlp(x)\nprint(y.shape)  # torch.Size([32, 8])\n</code></pre>"},{"location":"guides/mlp/#_2","title":"\u8a2d\u5b9a\u30aa\u30d7\u30b7\u30e7\u30f3","text":""},{"location":"guides/mlp/#linearconfig","title":"LinearConfig","text":"<p><code>LinearConfig</code>\u306f\u5404\u7dda\u5f62\u5c64\u306e\u8a2d\u5b9a\u3092\u5236\u5fa1\u3057\u307e\u3059\uff1a</p> <ul> <li><code>activation</code>: \u6d3b\u6027\u5316\u95a2\u6570\uff08\"ReLU\", \"Tanh\", \"Sigmoid\"\u306a\u3069\uff09</li> <li><code>bias</code>: \u30d0\u30a4\u30a2\u30b9\u3092\u4f7f\u3046\u304b\u3069\u3046\u304b\uff08\u30c7\u30d5\u30a9\u30eb\u30c8: <code>True</code>\uff09</li> <li><code>norm</code>: \u6b63\u898f\u5316\u306e\u7a2e\u985e\uff08\"none\", \"layer\", \"rms\"\uff09</li> <li><code>norm_cfg</code>: \u6b63\u898f\u5316\u306e\u8a2d\u5b9a\uff08\u8f9e\u66f8\u5f62\u5f0f\uff09</li> <li><code>norm_first</code>: \u6b63\u898f\u5316\u3092\u7dda\u5f62\u5c64\u306e\u524d\u306b\u884c\u3046\u304b\u3069\u3046\u304b\uff08\u30c7\u30d5\u30a9\u30eb\u30c8: <code>False</code>\uff09</li> <li><code>dropout</code>: \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u7387\uff08\u30c7\u30d5\u30a9\u30eb\u30c8: <code>0.0</code>\uff09</li> </ul>"},{"location":"guides/mlp/#mlpconfig","title":"MLPConfig","text":"<p><code>MLPConfig</code>\u306fMLP\u5168\u4f53\u306e\u8a2d\u5b9a\u3092\u5236\u5fa1\u3057\u307e\u3059\uff1a</p> <ul> <li><code>hidden_dim</code>: \u96a0\u308c\u5c64\u306e\u6b21\u5143</li> <li><code>n_layers</code>: \u96a0\u308c\u5c64\u306e\u6570</li> <li><code>output_activation</code>: \u51fa\u529b\u5c64\u306e\u6d3b\u6027\u5316\u95a2\u6570</li> <li><code>linear_cfg</code>: <code>LinearConfig</code>\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9</li> </ul>"},{"location":"guides/mlp/#_3","title":"\u4f7f\u7528\u4f8b","text":""},{"location":"guides/mlp/#mlp_1","title":"\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u4ed8\u304dMLP","text":"<p>YAML\u30d5\u30a1\u30a4\u30eb (<code>configs/mlp_dropout.yaml</code>):</p> <pre><code>_target_: ml_networks.layers.MLPLayer\ninput_dim: 16\noutput_dim: 8\nmlp_config:\n  _target_: ml_networks.config.MLPConfig\n  hidden_dim: 128\n  n_layers: 3\n  output_activation: Tanh\n  linear_cfg:\n    _target_: ml_networks.config.LinearConfig\n    activation: ReLU\n    bias: true\n    norm: layer\n    dropout: 0.2  # 20%\u306e\u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\n</code></pre> <p>Python\u30b3\u30fc\u30c9:</p> <pre><code>from hydra.utils import instantiate\nfrom omegaconf import OmegaConf\n\ncfg = OmegaConf.load(\"configs/mlp_dropout.yaml\")\nmlp = instantiate(cfg)\n</code></pre>"},{"location":"guides/mlp/#layernormmlp","title":"LayerNorm\u4ed8\u304dMLP","text":"<p>YAML\u30d5\u30a1\u30a4\u30eb (<code>configs/mlp_layernorm.yaml</code>):</p> <pre><code>_target_: ml_networks.layers.MLPLayer\ninput_dim: 16\noutput_dim: 8\nmlp_config:\n  _target_: ml_networks.config.MLPConfig\n  hidden_dim: 256\n  n_layers: 4\n  output_activation: Identity\n  linear_cfg:\n    _target_: ml_networks.config.LinearConfig\n    activation: ReLU\n    bias: true\n    norm: layer\n    norm_cfg:\n      eps: 1e-5\n    norm_first: true  # \u6b63\u898f\u5316\u3092\u5148\u306b\u5b9f\u884c\n</code></pre> <p>Python\u30b3\u30fc\u30c9:</p> <pre><code>from hydra.utils import instantiate\nfrom omegaconf import OmegaConf\n\ncfg = OmegaConf.load(\"configs/mlp_layernorm.yaml\")\nmlp = instantiate(cfg)\n</code></pre>"},{"location":"guides/unet/","title":"UNet\u30ac\u30a4\u30c9","text":"<p>\u6761\u4ef6\u4ed8\u304dUNet\uff08Conditional UNet\uff09\u306e\u4f7f\u7528\u65b9\u6cd5\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p> <p>\u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9</p> <p>YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8a2d\u5b9a\u3092\u8aad\u307f\u8fbc\u3080\u65b9\u6cd5\u306b\u3064\u3044\u3066\u306f\u3001\u8a2d\u5b9a\u7ba1\u7406\u30ac\u30a4\u30c9\u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"guides/unet/#_1","title":"\u6982\u8981","text":"<p><code>ml-networks</code>\u3067\u306f\u3001Diffusion Model\u306a\u3069\u3067\u5e83\u304f\u4f7f\u308f\u308c\u308b\u6761\u4ef6\u4ed8\u304dUNet\u3092\u63d0\u4f9b\u3057\u3066\u3044\u307e\u3059\u30022D\u30d0\u30fc\u30b8\u30e7\u30f3\uff08\u753b\u50cf\u7528\uff09\u30681D\u30d0\u30fc\u30b8\u30e7\u30f3\uff08\u6642\u7cfb\u5217\u7528\uff09\u306e\u4e21\u65b9\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u4e3b\u306a\u7279\u5fb4\uff1a</p> <ul> <li>\u6761\u4ef6\u4ed8\u304d\u751f\u6210: \u7279\u5fb4\u91cf\u30d9\u30af\u30c8\u30eb\u3092\u6761\u4ef6\u3068\u3057\u3066\u753b\u50cf/\u6642\u7cfb\u5217\u3092\u751f\u6210</li> <li>Attention\u6a5f\u69cb: \u30bb\u30eb\u30d5\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u306b\u3088\u308b\u9577\u8ddd\u96e2\u4f9d\u5b58\u6027\u306e\u30e2\u30c7\u30ea\u30f3\u30b0</li> <li>\u6b8b\u5dee\u63a5\u7d9a: \u30b9\u30ad\u30c3\u30d7\u63a5\u7d9a\u306b\u3088\u308b\u60c5\u5831\u306e\u4fdd\u6301</li> <li>HyperNetwork\u5bfe\u5fdc: \u91cd\u307f\u306e\u52d5\u7684\u751f\u6210\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09</li> </ul>"},{"location":"guides/unet/#conditionalunet2d2d","title":"ConditionalUnet2d\uff082D\u753b\u50cf\u7528\uff09","text":""},{"location":"guides/unet/#_2","title":"\u57fa\u672c\u7684\u306a\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"guides/unet/#1-yaml","title":"\u65b9\u6cd51: YAML\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u8aad\u307f\u8fbc\u3080\uff08\u63a8\u5968\uff09","text":"<p>\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb <code>configs/unet2d_config.yaml</code> \u3092\u4f5c\u6210\u3057\u307e\u3059\uff1a</p> <pre><code>_target_: ml_networks.torch.unet.ConditionalUnet2d\nfeature_dim: 32\nobs_shape: [3, 64, 64]\ncfg:\n  _target_: ml_networks.config.UNetConfig\n  channels: [64, 128, 256]\n  conv_cfg:\n    _target_: ml_networks.config.ConvConfig\n    kernel_size: 3\n    padding: 1\n    stride: 1\n    groups: 1\n    activation: ReLU\n    dropout: 0.0\n  has_attn: true\n  nhead: 8\n  cond_pred_scale: true\n</code></pre> <p>Python\u30b3\u30fc\u30c9\uff1a</p> <pre><code>from hydra.utils import instantiate\nfrom omegaconf import OmegaConf\nimport torch\n\ncfg = OmegaConf.load(\"configs/unet2d_config.yaml\")\nnet = instantiate(cfg)\n\n# \u5165\u529b: \u30ce\u30a4\u30ba\u753b\u50cf + \u6761\u4ef6\u30d9\u30af\u30c8\u30eb\nx = torch.randn(2, 3, 64, 64)      # (batch, channels, height, width)\ncond = torch.randn(2, 32)            # (batch, feature_dim)\nout = net(x, cond)\nprint(out.shape)  # torch.Size([2, 3, 64, 64])\n</code></pre>"},{"location":"guides/unet/#2-python","title":"\u65b9\u6cd52: Python\u30b3\u30fc\u30c9\u3067\u76f4\u63a5\u8a2d\u5b9a\u3059\u308b","text":"<pre><code>from ml_networks.torch import ConditionalUnet2d\nfrom ml_networks import UNetConfig, ConvConfig\nimport torch\n\ncfg = UNetConfig(\n    channels=[64, 128, 256],       # \u5404\u89e3\u50cf\u5ea6\u30ec\u30d9\u30eb\u306e\u30c1\u30e3\u30f3\u30cd\u30eb\u6570\n    conv_cfg=ConvConfig(\n        kernel_size=3,\n        padding=1,\n        stride=1,\n        groups=1,\n        activation=\"ReLU\",\n        dropout=0.0,\n    ),\n    has_attn=True,                 # Attention\u6a5f\u69cb\u3092\u4f7f\u7528\n    nhead=8,                       # Attention\u306e\u30d8\u30c3\u30c9\u6570\n    cond_pred_scale=True,          # \u6761\u4ef6\u4ed8\u304d\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\n)\n\nnet = ConditionalUnet2d(\n    feature_dim=32,                # \u6761\u4ef6\u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143\n    obs_shape=(3, 64, 64),         # \u753b\u50cf\u306e\u5f62\u72b6 (C, H, W)\n    cfg=cfg,\n)\n\n# \u63a8\u8ad6\nx = torch.randn(2, 3, 64, 64)\ncond = torch.randn(2, 32)\nout = net(x, cond)\nprint(out.shape)  # torch.Size([2, 3, 64, 64])\n</code></pre>"},{"location":"guides/unet/#conditionalunet1d1d","title":"ConditionalUnet1d\uff081D\u6642\u7cfb\u5217\u7528\uff09","text":""},{"location":"guides/unet/#_3","title":"\u57fa\u672c\u7684\u306a\u4f7f\u7528\u65b9\u6cd5","text":"<pre><code>from ml_networks.torch import ConditionalUnet1d\nfrom ml_networks import UNetConfig, ConvConfig\nimport torch\n\ncfg = UNetConfig(\n    channels=[64, 128, 256],\n    conv_cfg=ConvConfig(\n        kernel_size=3,\n        padding=1,\n        stride=1,\n        groups=1,\n        activation=\"ReLU\",\n        dropout=0.0,\n    ),\n    has_attn=False,\n    cond_pred_scale=True,\n)\n\nnet = ConditionalUnet1d(\n    feature_dim=32,\n    obs_shape=(8, 128),            # (\u30c1\u30e3\u30f3\u30cd\u30eb\u6570, \u30b7\u30fc\u30b1\u30f3\u30b9\u9577)\n    cfg=cfg,\n)\n\n# \u63a8\u8ad6\nx = torch.randn(2, 8, 128)        # (batch, channels, length)\ncond = torch.randn(2, 32)\nout = net(x, cond)\nprint(out.shape)  # torch.Size([2, 8, 128])\n</code></pre>"},{"location":"guides/unet/#unetconfig","title":"UNetConfig \u306e\u8a2d\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf","text":"\u30d1\u30e9\u30e1\u30fc\u30bf \u578b \u30c7\u30d5\u30a9\u30eb\u30c8 \u8aac\u660e <code>channels</code> <code>tuple[int, ...]</code> - \u5404\u89e3\u50cf\u5ea6\u30ec\u30d9\u30eb\u306e\u30c1\u30e3\u30f3\u30cd\u30eb\u6570 <code>conv_cfg</code> <code>ConvConfig</code> - \u7573\u307f\u8fbc\u307f\u5c64\u306e\u8a2d\u5b9a <code>cond_pred_scale</code> <code>bool</code> <code>False</code> \u6761\u4ef6\u4ed8\u304d\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u3092\u4f7f\u7528\u3059\u308b\u304b <code>nhead</code> <code>int \\| None</code> <code>None</code> Attention\u306e\u30d8\u30c3\u30c9\u6570 <code>has_attn</code> <code>bool</code> <code>False</code> Attention\u6a5f\u69cb\u3092\u4f7f\u7528\u3059\u308b\u304b <code>use_shuffle</code> <code>bool</code> <code>False</code> PixelShuffle/Unshuffle\u3092\u4f7f\u7528\u3059\u308b\u304b <code>use_hypernet</code> <code>bool</code> <code>False</code> HyperNetwork\u3092\u4f7f\u7528\u3059\u308b\u304b <code>hyper_mlp_cfg</code> <code>MLPConfig \\| None</code> <code>None</code> HyperNetwork\u306eMLP\u8a2d\u5b9a <p><code>has_attn</code>\u3092<code>True</code>\u306b\u3059\u308b\u5834\u5408</p> <p><code>has_attn=True</code>\u306e\u5834\u5408\u3001<code>nhead</code>\u3092\u5fc5\u305a\u6307\u5b9a\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u6307\u5b9a\u3057\u306a\u3044\u3068\u30a2\u30b5\u30fc\u30b7\u30e7\u30f3\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3059\u3002</p>"},{"location":"guides/unet/#diffusion-model","title":"Diffusion Model\u3067\u306e\u4f7f\u7528\u4f8b","text":"<p>\u6761\u4ef6\u4ed8\u304dUNet\u306f\u3001Diffusion Model\uff08DDPM\u3001DDIM\u306a\u3069\uff09\u306e\u30ce\u30a4\u30ba\u4e88\u6e2c\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3068\u3057\u3066\u5178\u578b\u7684\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\uff1a</p> <pre><code>from ml_networks.torch import ConditionalUnet2d\nfrom ml_networks import UNetConfig, ConvConfig\nimport torch\n\n# UNet\u306e\u8a2d\u5b9a\ncfg = UNetConfig(\n    channels=[64, 128, 256, 512],\n    conv_cfg=ConvConfig(\n        kernel_size=3,\n        padding=1,\n        stride=1,\n        activation=\"ReLU\",\n    ),\n    has_attn=True,\n    nhead=8,\n    cond_pred_scale=True,\n)\n\nnet = ConditionalUnet2d(\n    feature_dim=256,\n    obs_shape=(3, 64, 64),\n    cfg=cfg,\n)\n\n# Diffusion\u306e1\u30b9\u30c6\u30c3\u30d7\nnoisy_image = torch.randn(4, 3, 64, 64)   # \u30ce\u30a4\u30ba\u304c\u52a0\u3048\u3089\u308c\u305f\u753b\u50cf\ncondition = torch.randn(4, 256)             # \u6761\u4ef6\uff08\u30c6\u30ad\u30b9\u30c8\u57cb\u3081\u8fbc\u307f\u306a\u3069\uff09\n\n# \u30ce\u30a4\u30ba\u4e88\u6e2c\npredicted_noise = net(noisy_image, condition)\nprint(predicted_noise.shape)  # torch.Size([4, 3, 64, 64])\n</code></pre>"},{"location":"guides/unet/#pixelshuffleunshuffle","title":"PixelShuffle/Unshuffle\u306e\u4f7f\u7528","text":"<p>\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0/\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306bPixelShuffle/Unshuffle\u3092\u4f7f\u7528\u3067\u304d\u307e\u3059\uff1a</p> <pre><code>cfg:\n  _target_: ml_networks.config.UNetConfig\n  channels: [64, 128, 256]\n  conv_cfg:\n    _target_: ml_networks.config.ConvConfig\n    kernel_size: 3\n    padding: 1\n    stride: 1\n    activation: ReLU\n  use_shuffle: true       # PixelShuffle/Unshuffle\u3092\u6709\u52b9\u5316\n  has_attn: false\n</code></pre>"},{"location":"guides/unet/#hypernetwork","title":"HyperNetwork\u3068\u306e\u7d44\u307f\u5408\u308f\u305b","text":"<p>UNet\u306e\u91cd\u307f\u3092HyperNetwork\u3067\u52d5\u7684\u306b\u751f\u6210\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\uff1a</p> <pre><code>cfg:\n  _target_: ml_networks.config.UNetConfig\n  channels: [64, 128]\n  conv_cfg:\n    _target_: ml_networks.config.ConvConfig\n    kernel_size: 3\n    padding: 1\n    stride: 1\n    activation: ReLU\n  use_hypernet: true\n  hyper_mlp_cfg:\n    _target_: ml_networks.config.MLPConfig\n    hidden_dim: 256\n    n_layers: 2\n    output_activation: Identity\n    linear_cfg:\n      _target_: ml_networks.config.LinearConfig\n      activation: ReLU\n      bias: true\n</code></pre>"},{"location":"guides/utilities/","title":"\u305d\u306e\u4ed6\u306e\u4fbf\u5229\u306a\u6a5f\u80fd","text":"<p>\u305d\u306e\u4ed6\u306e\u4fbf\u5229\u306a\u6a5f\u80fd\u3092\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"guides/utilities/#_2","title":"\u6d3b\u6027\u5316\u95a2\u6570","text":"<p>string\u3067\u6d3b\u6027\u5316\u95a2\u6570\u3092\u6307\u5b9a\u3067\u304d\u307e\u3059\u3002PyTorch\u306b\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\u6d3b\u6027\u5316\u95a2\u6570\u306b\u52a0\u3048\u3066\u3001\u4ee5\u4e0b\u306e\u6d3b\u6027\u5316\u95a2\u6570\u304c\u4f7f\u3048\u307e\u3059\uff1a</p> <ul> <li>\"REReLU\": Reparametrized ReLU - \u9006\u4f1d\u64ad\u304cGELU\u7b49\u306b\u306a\u308bReLU</li> <li>\"SiGLU\": SiLU + GLU - SiLU(Swish)\u3068GLU\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u6d3b\u6027\u5316\u95a2\u6570</li> <li>\"CRReLU\": Correction Regularized ReLU - \u6b63\u5247\u5316\u3055\u308c\u305fReLU</li> <li>\"TanhExp\": Mish\u306e\u6539\u5584\u7248\u3068\u3044\u3046\u4f4d\u7f6e\u4ed8\u3051</li> </ul> <pre><code>from ml_networks import Activation\n\nact = Activation(\"ReLU\")\nx = torch.randn(32, 64)\ny = act(x)\n</code></pre>"},{"location":"guides/utilities/#_3","title":"\u6700\u9069\u5316\u624b\u6cd5","text":"<p>string\u3067\u6700\u9069\u5316\u624b\u6cd5\u3092\u6307\u5b9a\u3067\u304d\u307e\u3059\u3002PyTorch\u306b\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\u6700\u9069\u5316\u624b\u6cd5\u306b\u52a0\u3048\u3066\u3001pytorch_optimizer\u306b\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\u6700\u9069\u5316\u624b\u6cd5\u304c\u4f7f\u3048\u307e\u3059\u3002</p> <pre><code>from ml_networks import get_optimizer\nimport torch.nn as nn\n\nmodel = nn.Linear(16, 8)\n\n# **kwargs\u3067optimizer\u3078\u306e\u69d8\u3005\u306a\u8a2d\u5b9a\u306b\u95a2\u3059\u308b\u5f15\u6570\u3092\u6e21\u3059\u3053\u3068\u304c\u3067\u304d\u308b\noptimizer = get_optimizer(model.parameters(), \"Adam\", lr=1e-3, weight_decay=1e-4)\n</code></pre>"},{"location":"guides/utilities/#seed","title":"seed\u56fa\u5b9a","text":"<p>\u518d\u73fe\u6027\u3092\u62c5\u4fdd\u3059\u308b\u305f\u3081\u306bseed\u3092\u56fa\u5b9a\u3067\u304d\u307e\u3059\u3002</p> <pre><code>from ml_networks import torch_fix_seed, determine_loader\n\n# random, np, torch\u306eseed\u3092\u56fa\u5b9a\u3059\u308b\n# \u3055\u3089\u306bGPU\u95a2\u9023\u306e\u518d\u73fe\u6027\u3082\uff08\u3042\u308b\u7a0b\u5ea6\uff09\u62c5\u4fdd\ntorch_fix_seed(42)\n</code></pre>"},{"location":"guides/utilities/#dataloader","title":"DataLoader\u306e\u518d\u73fe\u6027","text":"<p>DataLoader\u306e\u518d\u73fe\u6027\u3092\u62c5\u4fdd\u3059\u308b\u305f\u3081\u306b<code>determine_loader</code>\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002</p> <pre><code>from torch.utils.data import Dataset\n\ndataset = Dataset(any_data)\n\n# DataLoader\u306e\u518d\u73fe\u6027\u3092\u62c5\u4fdd\u3059\u308b\n# \u901a\u5e38\u306edataloader\u306e\u547c\u3073\u51fa\u3057\u3067\u306f\u3001\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u51fa\u3057\u306b\u95a2\u3059\u308b\u518d\u73fe\u6027\u306f\u62c5\u4fdd\u3055\u308c\u306a\u3044\nloader = determine_loader(\n    dataset,\n    seed=42,        # \u4e71\u6570\u306eseed\n    batch_size=32,  # \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\n    shuffle=True,   # \u6bce\u30a8\u30dd\u30c3\u30af\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u8eab\u3092\u5165\u308c\u66ff\u3048\u308b\u304b\n    collate_fn=None,  # \u7279\u5b9a\u306e\u30df\u30cb\u30d0\u30c3\u30c1\u4f5c\u6210\u51e6\u7406\u304c\u3042\u308b\u5834\u5408\u306f\u6307\u5b9a\u3059\u308b\n)\n</code></pre>"},{"location":"guides/utilities/#gumbel-softmax","title":"Gumbel Softmax","text":"<p>Gumbel Softmax\u3092\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002</p> <pre><code>from ml_networks import gumbel_softmax\n\nlogits = torch.randn(32, 10)\nsamples = gumbel_softmax(logits, temperature=1.0, hard=True)\n</code></pre>"},{"location":"guides/utilities/#softmax","title":"Softmax","text":"<p>\u30ab\u30b9\u30bf\u30e0Softmax\u3092\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002</p> <pre><code>from ml_networks import softmax\n\nlogits = torch.randn(32, 10)\nprobs = softmax(logits, dim=-1)\n</code></pre>"},{"location":"guides/utilities/#minmaxnormalize","title":"MinMaxNormalize","text":"<p>Min-Max\u6b63\u898f\u5316\u3092\u5b9f\u884c\u3067\u304d\u307e\u3059\u3002</p> <pre><code>from ml_networks import MinMaxNormalize\n\nnormalize = MinMaxNormalize(min=0.0, max=1.0)\ndata = torch.randn(32, 3, 64, 64)\nnormalized_data = normalize(data)\n</code></pre>"},{"location":"guides/utilities/#softmaxtransformation","title":"SoftmaxTransformation","text":"<p>Softmax\u5909\u63db\u3092\u5b9f\u884c\u3067\u304d\u307e\u3059\u3002</p> <pre><code>from ml_networks import SoftmaxTransformation\n\ntransform = SoftmaxTransformation(temperature=1.0)\ndata = torch.randn(32, 10)\ntransformed_data = transform(data)\n</code></pre>"}]}